<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Lieven Clement" />


<title>3. Prediction with High Dimensional Predictors</title>

<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<script src="site_libs/navigation-1.1/sourceembed.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<link href="site_libs/font-awesome-6.4.2/css/all.min.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>



<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #204a87; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #204a87; font-weight: bold; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #ce5c00; font-weight: bold; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */

.sourceCode .row {
  width: 100%;
}
.sourceCode {
  overflow-x: auto;
}
.code-folding-btn {
  margin-right: -30px;
}
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>


<style type="text/css">
#rmd-source-code {
  display: none;
}
</style>


<link rel="stylesheet" href="style.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">HDDA</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-chalkboard-teacher"></span>
     
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="intro.html">1. Introduction</a>
    </li>
    <li>
      <a href="svd.html">2. Singular Value Decomposition</a>
    </li>
    <li>
      <a href="prediction.html">3. Prediction with High Dimensional Predictors</a>
    </li>
    <li>
      <a href="sparseSvd.html">4. Sparse Singular Value Decomposition</a>
    </li>
    <li>
      <a href="lda.html">5. Linear Discriminant Analysis</a>
    </li>
    <li>
      <a href="hclust.html">6.1. Introduction to Clustering</a>
    </li>
    <li>
      <a href="https://sites.stat.washington.edu/people/raftery/Research/PDF/fraley1998.pdf">6.2. Paper Model-based Clustering</a>
    </li>
    <li>
      <a href="em.html">6.3. EM algorithm</a>
    </li>
    <li>
      <a href="lsi.html">7. Large Scale Inference</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-laptop"></span>
     
    Labs
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Lab1-Intro-SVD.html">Lab 1: Intro &amp; SVD</a>
    </li>
    <li>
      <a href="Lab2-PCA.html">Lab 2: SVD - PCA</a>
    </li>
    <li>
      <a href="Lab3-Penalized-Regression.html">Lab 3: Prediction</a>
    </li>
    <li>
      <a href="Lab4-Sparse-PCA-LDA.html">Lab 4: Sparse PCA &amp; LDA</a>
    </li>
    <li>
      <a href="Lab5-Clustering.html">Lab 5: Clustering</a>
    </li>
    <li>
      <a href="Lab6-Large-Scale-Inference.html">Lab 6: LSI</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="https://github.com/statOmics/HDDA">
    <span class="fab fa-github"></span>
     
  </a>
</li>
<li>
  <a href="http://statomics.github.io/">statOmics</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
<li role="separator" class="divider"></li>
<li><a id="rmd-download-source" href="#">Download Rmd</a></li>
</ul>
</div>



<h1 class="title toc-ignore">3. Prediction with High Dimensional Predictors</h1>
<h4 class="author">Lieven Clement</h4>
<h4 class="date">statOmics, Ghent University (<a href="https://statomics.github.io" class="uri">https://statomics.github.io</a>)</h4>

</div>


<div id="introduction" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<div id="prediction-with-high-dimensional-predictors" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Prediction with High Dimensional Predictors</h2>
<p>General setting:</p>
<ul>
<li><p>Aim: build a <strong>prediction model</strong> that gives a prediction of an outcome for a given set of predictors.</p></li>
<li><p>We use <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> to refer to the predictors and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math> to refer to the outcome.</p></li>
<li><p>A <strong>training data set</strong> is available, say <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐗</mi><mo>,</mo><mi>𝐘</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\mathbf{X},\mathbf{Y})</annotation></semantics></math>. It contains <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> observations on outcomes and on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> predictors.</p></li>
<li><p>Using the training data, a prediction model is build, say <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{m}(\mathbf{X})</annotation></semantics></math>. This typically involves <strong>model building (feature selection)</strong> and parameter estimation.</p></li>
<li><p>During the model building, potential <strong>models need to be evaluated</strong> in terms of their prediction quality.</p></li>
</ul>
</div>
<div id="example-toxicogenomics-in-early-drug-development" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Example: Toxicogenomics in early drug development</h2>
<div id="background" class="section level3" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Background</h3>
<ul>
<li><p>Effect of compound on gene expression.</p></li>
<li><p>Insight in action and toxicity of drug in early phase</p></li>
<li><p>Determine activity with bio-assay: e.g. binding affinity of compound to cell wall receptor (target, IC50).</p></li>
<li><p>Early phase: 20 to 50 compounds</p></li>
<li><p>Based on in vitro results one aims to get insight in how to build better compound (higher on-target activity less toxicity.</p></li>
<li><p>Small variations in molecular structure lead to variations in BA and gene expression.</p></li>
<li><p>Aim: Build model to predict bio-activity based on gene expression in liver cell line.</p></li>
</ul>
</div>
<div id="data" class="section level3" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Data</h3>
<ul>
<li><p>30 chemical compounds have been screened for toxicity</p></li>
<li><p>Bioassay data on toxicity screening</p></li>
<li><p>Gene expressions in a liver cell line are profiled for each compound (4000 genes)</p></li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>toxData <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>  <span class="st">&quot;https://raw.githubusercontent.com/statOmics/HDA2020/data/toxDataCentered.csv&quot;</span>,</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>  <span class="at">col_types =</span> <span class="fu">cols</span>()</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>)</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>svdX <span class="ot">&lt;-</span> <span class="fu">svd</span>(toxData[,<span class="sc">-</span><span class="dv">1</span>])</span></code></pre></div>
<p>Data is already centered:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>toxData <span class="sc">%&gt;%</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>  colMeans <span class="sc">%&gt;%</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>  range</span></code></pre></div>
<pre><code>#&gt; [1] -2.405483e-17  5.921189e-17</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a> toxData <span class="sc">%&gt;%</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>  names <span class="sc">%&gt;%</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>  head</span></code></pre></div>
<pre><code>#&gt; [1] &quot;BA&quot; &quot;X1&quot; &quot;X2&quot; &quot;X3&quot; &quot;X4&quot; &quot;X5&quot;</code></pre>
<ul>
<li>First column contains data on Bioassay.</li>
<li>The higher the score on Bioassay the more toxic the compound</li>
<li>Other columns contain data on gene expression X1, … , X4000</li>
</ul>
</div>
<div id="data-exploration" class="section level3" number="1.2.3">
<h3><span class="header-section-number">1.2.3</span> Data exploration</h3>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>toxData <span class="sc">%&gt;%</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span><span class="st">&quot;&quot;</span>,<span class="at">y=</span>BA)) <span class="sc">+</span></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>(<span class="at">outlier.shape=</span><span class="cn">NA</span>) <span class="sc">+</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">position=</span><span class="st">&quot;jitter&quot;</span>)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>svdX <span class="ot">&lt;-</span> toxData[,<span class="sc">-</span><span class="dv">1</span>] <span class="sc">%&gt;%</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>  svd</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>Vk <span class="ot">&lt;-</span> svdX<span class="sc">$</span>v[,<span class="dv">1</span><span class="sc">:</span>k]</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>Uk <span class="ot">&lt;-</span> svdX<span class="sc">$</span>u[,<span class="dv">1</span><span class="sc">:</span>k]</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>Dk <span class="ot">&lt;-</span> <span class="fu">diag</span>(svdX<span class="sc">$</span>d[<span class="dv">1</span><span class="sc">:</span>k])</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>Zk <span class="ot">&lt;-</span> Uk<span class="sc">%*%</span>Dk</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a><span class="fu">colnames</span>(Zk) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;Z&quot;</span>,<span class="dv">1</span><span class="sc">:</span>k)</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a><span class="fu">colnames</span>(Vk) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;V&quot;</span>,<span class="dv">1</span><span class="sc">:</span>k)</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>Zk <span class="sc">%&gt;%</span></span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>  as.data.frame <span class="sc">%&gt;%</span></span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">BA =</span> toxData <span class="sc">%&gt;%</span> <span class="fu">pull</span>(BA)) <span class="sc">%&gt;%</span></span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span> Z1, <span class="at">y =</span> Z2, <span class="at">color =</span> BA)) <span class="sc">+</span></span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a>  <span class="fu">scale_colour_gradient2</span>(<span class="at">low =</span> <span class="st">&quot;blue&quot;</span>,<span class="at">mid=</span><span class="st">&quot;white&quot;</span>,<span class="at">high=</span><span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>, <span class="at">pch =</span> <span class="dv">21</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<ul>
<li>Scores on the first two principal components (or MDS plot).</li>
<li>Each point corresponds to a compound.</li>
<li>Color code refers to the toxicity score (higher score more toxic).</li>
<li>Clear separation between compounds according to toxicity.</li>
</ul>
<hr />
<ul>
<li>Next logic step in a PCA is to interpret the principal components.</li>
<li>We thus have to assess the loadings.</li>
<li>We can add a vector for each gene to get a biplot, but this would require plotting 4000 vectors, which would render the plot unreadable.</li>
</ul>
<p>Alternative graph to look at the many loadings of the first two PCs.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="fu">grid.arrange</span>(</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>  Vk <span class="sc">%&gt;%</span></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>    as.data.frame <span class="sc">%&gt;%</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">geneID =</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(Vk)) <span class="sc">%&gt;%</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> geneID, <span class="at">y =</span> V1)) <span class="sc">+</span></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">pch=</span><span class="dv">21</span>) <span class="sc">+</span></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>    <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">2</span>)<span class="sc">*</span><span class="fu">sd</span>(Vk[,<span class="dv">1</span>]), <span class="at">col =</span> <span class="st">&quot;red&quot;</span>) ,</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>  Vk <span class="sc">%&gt;%</span></span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>    as.data.frame <span class="sc">%&gt;%</span></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">geneID =</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(Vk)) <span class="sc">%&gt;%</span></span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> geneID, <span class="at">y =</span> V2)) <span class="sc">+</span></span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">pch=</span><span class="dv">21</span>) <span class="sc">+</span></span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>    <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">2</span>)<span class="sc">*</span><span class="fu">sd</span>(Vk[,<span class="dv">2</span>]), <span class="at">col =</span> <span class="st">&quot;red&quot;</span>),</span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>  <span class="at">ncol=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<ul>
<li><p>It is almost impossible to interpret the PCs because there are 4000 genes contributing to each PC.</p></li>
<li><p>In an attempt to find the most important genes (in the sense that they drive the interpretation of the PCs), the plots show horizontal reference lines: the average of the loadings, and the average ± twice the standard deviation of the loadings. In between the lines we expects about 95% of the loadings (if they were normally distributed).</p></li>
<li><p>The points outside the band come from the genes that have rather large loadings (in absolute value) and hence are important for the interpretation of the PCs.</p></li>
<li><p>Note, that particularly for the first PC, only a few genes show a markedly large loadings that are negative. This means that an upregulation of these genes will lead to low scores on PC1.</p></li>
<li><p>These genes will very likely play an important role in the toxicity mechanism.</p></li>
<li><p>Indeed, low scores on PC1 are in the direction of more toxicity.</p></li>
<li><p>In the next chapter we will introduce a method to obtain sparse PCs.</p></li>
</ul>
</div>
<div id="prediction-model" class="section level3" number="1.2.4">
<h3><span class="header-section-number">1.2.4</span> Prediction model</h3>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(BA <span class="sc">~</span> <span class="sc">-</span><span class="dv">1</span> <span class="sc">+</span> ., toxData)</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>m1 <span class="sc">%&gt;%</span></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>  coef <span class="sc">%&gt;%</span></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>  <span class="fu">head</span>(<span class="dv">40</span>)</span></code></pre></div>
<pre><code>#&gt;          X1          X2          X3          X4          X5          X6 
#&gt;  -7.4569940   0.3571348  11.2492315  10.8354021 -13.7433891   5.6833874 
#&gt;          X7          X8          X9         X10         X11         X12 
#&gt;  65.5387777   4.3404555   7.9103924  37.0296057 -54.8368698 -55.5547845 
#&gt;         X13         X14         X15         X16         X17         X18 
#&gt;   5.7924667  23.1428002  -6.9610365 -28.5250571 -22.5509025 -97.9623731 
#&gt;         X19         X20         X21         X22         X23         X24 
#&gt; -30.4171782 -32.6991673 -14.2808834 -16.1431266 -22.7498681  73.1635178 
#&gt;         X25         X26         X27         X28         X29         X30 
#&gt;  -5.7065827  37.4745379 -20.1999102  14.9906821  99.6080955          NA 
#&gt;         X31         X32         X33         X34         X35         X36 
#&gt;          NA          NA          NA          NA          NA          NA 
#&gt;         X37         X38         X39         X40 
#&gt;          NA          NA          NA          NA</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>m1 <span class="sc">%&gt;%</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>  coef <span class="sc">%&gt;%</span></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>  is.na <span class="sc">%&gt;%</span></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>  sum</span></code></pre></div>
<pre><code>#&gt; [1] 3971</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="fu">summary</span>(m1)<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>#&gt; [1] 1</code></pre>
<p>Problem??</p>
</div>
</div>
<div id="brain-example" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Brain example</h2>
<ul>
<li>Courtesy to Solomon Kurz. Statistical rethinking with brms, ggplot2, and the tidyverse version 1.2.0.</li>
</ul>
<p><a href="https://bookdown.org/content/3890/" class="uri">https://bookdown.org/content/3890/</a>
<a href="https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse" class="uri">https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse</a></p>
<ul>
<li>Data with brain size and body size for seven species</li>
</ul>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>brain <span class="ot">&lt;-</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">species =</span> <span class="fu">c</span>(<span class="st">&quot;afarensis&quot;</span>, <span class="st">&quot;africanus&quot;</span>, <span class="st">&quot;habilis&quot;</span>, <span class="st">&quot;boisei&quot;</span>, <span class="st">&quot;rudolfensis&quot;</span>, <span class="st">&quot;ergaster&quot;</span>, <span class="st">&quot;sapiens&quot;</span>),</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>       <span class="at">brain   =</span> <span class="fu">c</span>(<span class="dv">438</span>, <span class="dv">452</span>, <span class="dv">612</span>, <span class="dv">521</span>, <span class="dv">752</span>, <span class="dv">871</span>, <span class="dv">1350</span>),</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>       <span class="at">mass    =</span> <span class="fu">c</span>(<span class="fl">37.0</span>, <span class="fl">35.5</span>, <span class="fl">34.5</span>, <span class="fl">41.5</span>, <span class="fl">55.5</span>, <span class="fl">61.0</span>, <span class="fl">53.5</span>))</span></code></pre></div>
<div id="data-exploration-1" class="section level3" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> Data exploration</h3>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>brain</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["species"],"name":[1],"type":["chr"],"align":["left"]},{"label":["brain"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["mass"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"afarensis","2":"438","3":"37.0"},{"1":"africanus","2":"452","3":"35.5"},{"1":"habilis","2":"612","3":"34.5"},{"1":"boisei","2":"521","3":"41.5"},{"1":"rudolfensis","2":"752","3":"55.5"},{"1":"ergaster","2":"871","3":"61.0"},{"1":"sapiens","2":"1350","3":"53.5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>p <span class="ot">&lt;-</span> brain <span class="sc">%&gt;%</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span>  mass, <span class="at">y =</span> brain, <span class="at">label =</span> species)) <span class="sc">+</span></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>p <span class="sc">+</span> <span class="fu">geom_text</span>(<span class="at">nudge_y =</span> <span class="dv">40</span>)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
<div id="models" class="section level3" number="1.3.2">
<h3><span class="header-section-number">1.3.2</span> Models</h3>
<p>Six models range in complexity from the simple univariate model</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mtext mathvariant="normal">brain</mtext><mi>i</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>∼</mo><mo>Normal</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>μ</mi><mi>i</mi></msub><mo>,</mo><mi>σ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>μ</mi><mi>i</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><msub><mi>β</mi><mn>1</mn></msub><msub><mtext mathvariant="normal">mass</mtext><mi>i</mi></msub><mo>,</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
\text{brain}_i &amp; \sim \operatorname{Normal} (\mu_i, \sigma) \\
\mu_i &amp; = \beta_0 + \beta_1 \text{mass}_i,
\end{align*}</annotation></semantics></math></p>
<p>to the dizzying sixth-degree polynomial model</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mtext mathvariant="normal">brain</mtext><mi>i</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>∼</mo><mo>Normal</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>μ</mi><mi>i</mi></msub><mo>,</mo><mi>σ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>μ</mi><mi>i</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><msub><mi>β</mi><mn>1</mn></msub><msub><mtext mathvariant="normal">mass</mtext><mi>i</mi></msub><mo>+</mo><msub><mi>β</mi><mn>2</mn></msub><msubsup><mtext mathvariant="normal">mass</mtext><mi>i</mi><mn>2</mn></msubsup><mo>+</mo><msub><mi>β</mi><mn>3</mn></msub><msubsup><mtext mathvariant="normal">mass</mtext><mi>i</mi><mn>3</mn></msubsup><mo>+</mo><msub><mi>β</mi><mn>4</mn></msub><msubsup><mtext mathvariant="normal">mass</mtext><mi>i</mi><mn>4</mn></msubsup><mo>+</mo><msub><mi>β</mi><mn>5</mn></msub><msubsup><mtext mathvariant="normal">mass</mtext><mi>i</mi><mn>5</mn></msubsup><mo>+</mo><msub><mi>β</mi><mn>6</mn></msub><msubsup><mtext mathvariant="normal">mass</mtext><mi>i</mi><mn>6</mn></msubsup><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
\text{brain}_i &amp; \sim \operatorname{Normal} (\mu_i, \sigma) \\
\mu_i &amp; = \beta_0 + \beta_1 \text{mass}_i + \beta_2 \text{mass}_i^2 + \beta_3 \text{mass}_i^3 + \beta_4 \text{mass}_i^4 + \beta_5 \text{mass}_i^5 + \beta_6 \text{mass}_i^6.
\end{align*}</annotation></semantics></math></p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>formulas <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="cf">function</span>(i)</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>  <span class="fu">return</span>(</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>     <span class="fu">paste0</span>(<span class="st">&quot;I(mass^&quot;</span>,<span class="dv">1</span><span class="sc">:</span>i,<span class="st">&quot;)&quot;</span>) <span class="sc">%&gt;%</span> <span class="fu">paste</span>(<span class="at">collapse=</span><span class="st">&quot; + &quot;</span>)</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>    )</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a>)</span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>formulas <span class="ot">&lt;-</span> <span class="fu">sapply</span>(</span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a>  <span class="fu">paste0</span>(<span class="st">&quot;brain ~ &quot;</span>, formulas),</span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a>  as.formula)</span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" tabindex="-1"></a>models <span class="ot">&lt;-</span> <span class="fu">lapply</span>(formulas, lm , <span class="at">data =</span> brain)</span></code></pre></div>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="fu">data.frame</span>(</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>  <span class="at">formula=</span>formulas <span class="sc">%&gt;%</span></span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a>    as.character,</span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a>  <span class="at">r2 =</span> <span class="fu">sapply</span>(</span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a>    models,</span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a>    <span class="cf">function</span>(mod) <span class="fu">summary</span>(mod)<span class="sc">$</span>r.squared)</span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a>  )  <span class="sc">%&gt;%</span></span>
<span id="cb19-8"><a href="#cb19-8" tabindex="-1"></a>  <span class="fu">ggplot</span>(</span>
<span id="cb19-9"><a href="#cb19-9" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">x =</span> r2,</span>
<span id="cb19-10"><a href="#cb19-10" tabindex="-1"></a>      <span class="at">y =</span> formula,</span>
<span id="cb19-11"><a href="#cb19-11" tabindex="-1"></a>      <span class="at">label =</span> r2 <span class="sc">%&gt;%</span></span>
<span id="cb19-12"><a href="#cb19-12" tabindex="-1"></a>        <span class="fu">round</span>(<span class="dv">2</span>) <span class="sc">%&gt;%</span></span>
<span id="cb19-13"><a href="#cb19-13" tabindex="-1"></a>        as.character)</span>
<span id="cb19-14"><a href="#cb19-14" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb19-15"><a href="#cb19-15" tabindex="-1"></a>  <span class="fu">geom_text</span>()</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>We plot the fit for each model individually and them arrange them together in one plot.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>plots <span class="ot">&lt;-</span> <span class="fu">lapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="cf">function</span>(i)</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>{</span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a>  p <span class="sc">+</span></span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">formula =</span> y <span class="sc">~</span> <span class="fu">poly</span>(x,i)) <span class="sc">+</span></span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a>  <span class="fu">ggtitle</span>(</span>
<span id="cb20-6"><a href="#cb20-6" tabindex="-1"></a>    <span class="fu">paste0</span>(</span>
<span id="cb20-7"><a href="#cb20-7" tabindex="-1"></a>      <span class="st">&quot;r2 = &quot;</span>,</span>
<span id="cb20-8"><a href="#cb20-8" tabindex="-1"></a>      <span class="fu">round</span>(<span class="fu">summary</span>(models[[i]])<span class="sc">$</span>r.squared<span class="sc">*</span><span class="dv">100</span>,<span class="dv">1</span>),</span>
<span id="cb20-9"><a href="#cb20-9" tabindex="-1"></a>      <span class="st">&quot;%&quot;</span>)</span>
<span id="cb20-10"><a href="#cb20-10" tabindex="-1"></a>    )</span>
<span id="cb20-11"><a href="#cb20-11" tabindex="-1"></a>})</span>
<span id="cb20-12"><a href="#cb20-12" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" tabindex="-1"></a><span class="fu">do.call</span>(<span class="st">&quot;grid.arrange&quot;</span>,<span class="fu">c</span>(plots, <span class="at">ncol =</span> <span class="dv">3</span>))</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<ul>
<li><p>We clearly see that increasing the model complexity always produces a fit with a smaller SSE.</p></li>
<li><p>The problem of overfitting is very obvious. The more complex polynomial models will not generalise well for prediction!</p></li>
<li><p>We even have a model that fits the data perfectly, but that will make very absurd preditions!</p></li>
<li><p>Too few parameters hurts, too. Fit the underfit intercept-only model.</p></li>
</ul>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a>m0 <span class="ot">&lt;-</span> <span class="fu">lm</span>(brain <span class="sc">~</span> <span class="dv">1</span>, brain)</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a><span class="fu">summary</span>(m0)</span></code></pre></div>
<pre><code>#&gt; 
#&gt; Call:
#&gt; lm(formula = brain ~ 1, data = brain)
#&gt; 
#&gt; Residuals:
#&gt;     Min      1Q  Median      3Q     Max 
#&gt; -275.71 -227.21 -101.71   97.79  636.29 
#&gt; 
#&gt; Coefficients:
#&gt;             Estimate Std. Error t value Pr(&gt;|t|)   
#&gt; (Intercept)    713.7      121.8    5.86  0.00109 **
#&gt; ---
#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
#&gt; 
#&gt; Residual standard error: 322.2 on 6 degrees of freedom</code></pre>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a>p <span class="sc">+</span></span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>  <span class="fu">stat_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">formula =</span> y <span class="sc">~</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a>  <span class="fu">ggtitle</span>(</span>
<span id="cb23-4"><a href="#cb23-4" tabindex="-1"></a>    <span class="fu">paste0</span>(</span>
<span id="cb23-5"><a href="#cb23-5" tabindex="-1"></a>      <span class="st">&quot;r2 = &quot;</span>,</span>
<span id="cb23-6"><a href="#cb23-6" tabindex="-1"></a>      <span class="fu">round</span>(<span class="fu">summary</span>(m0)<span class="sc">$</span>r.squared<span class="sc">*</span><span class="dv">100</span>,<span class="dv">1</span>),</span>
<span id="cb23-7"><a href="#cb23-7" tabindex="-1"></a>      <span class="st">&quot;%&quot;</span>)</span>
<span id="cb23-8"><a href="#cb23-8" tabindex="-1"></a>    )</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>The underfit model did not learn anything about the relation between mass and brain. It would also do a very poor job for predicting new data.</p>
</div>
</div>
<div id="overview" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Overview</h2>
<p>We will make a distinction between continuous and discrete outcomes. In this course we focus on</p>
<ul>
<li><p>Linear regression models for continous outcomes</p>
<ul>
<li>Penalised regression: Lasso and ridge</li>
<li>Principal component regression (PCR)</li>
</ul></li>
<li><p>Logistic regression models for binary outcomes</p>
<ul>
<li>Penalised regression: Lasso and ridge</li>
</ul></li>
</ul>
<p>For all types of model, we will discuss feature selection methods.</p>
</div>
</div>
<div id="linear-regression-for-high-dimensional-data" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Linear Regression for High Dimensional Data</h1>
<p>Consider linear regression model (for double centered data)
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo>=</mo><msub><mi>β</mi><mn>1</mn></msub><msub><mi>X</mi><mrow><mi>i</mi><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>β</mi><mn>2</mn></msub><msub><mi>X</mi><mrow><mi>i</mi><mn>2</mn></mrow></msub><mo>+</mo><mi>⋯</mi><mo>+</mo><msub><mi>β</mi><mi>p</mi></msub><msub><mi>X</mi><mrow><mi>i</mi><mi>p</mi></mrow></msub><mo>+</mo><msub><mi>ϵ</mi><mi>i</mi></msub><mo>,</mo></mrow><annotation encoding="application/x-tex">
  Y_i = \beta_1X_{i1} + \beta_2 X_{i2} + \cdots + \beta_pX_{ip} + \epsilon_i ,
</annotation></semantics></math>
with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>ϵ</mi><mo>∣</mo><mi>𝐗</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\text{E}\left[\epsilon \mid \mathbf{X}\right]=0</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">var</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>ϵ</mi><mo>∣</mo><mi>𝐗</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\text{var}\left[\epsilon \mid \mathbf{X}\right]=\sigma^2</annotation></semantics></math>.</p>
<p>In matrix notation the model becomes
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐘</mi><mo>=</mo><mi>𝐗</mi><mi>𝛃</mi><mo>+</mo><mi>𝛜</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">
  \mathbf{Y} = \mathbf{X}\mathbf\beta + \mathbf\epsilon.
</annotation></semantics></math>
The least squares estimator of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\mathbf\beta</annotation></semantics></math> is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>T</mi></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><msup><mi>𝐗</mi><mi>T</mi></msup><mi>𝐘</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">
  \hat{\mathbf\beta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} ,
</annotation></semantics></math>
and the variance of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\mathbf\beta}</annotation></semantics></math> equals
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">var</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>T</mi></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><msup><mi>σ</mi><mn>2</mn></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">
  \text{var}\left[\hat{\mathbf\beta}\right] = (\mathbf{X}^T\mathbf{X})^{-1}\sigma^2.
</annotation></semantics></math>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>→</mo><annotation encoding="application/x-tex">\longrightarrow</annotation></semantics></math> the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">p \times p</annotation></semantics></math> matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>T</mi></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">(\mathbf{X}^T\mathbf{X})^{-1}</annotation></semantics></math> is crucial</p>
<p>Note, that</p>
<ul>
<li><p>with double centered data it is meant that both the responses are centered (mean of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐘</mi><annotation encoding="application/x-tex">\mathbf{Y}</annotation></semantics></math> is zero) and that all predictors are centered (columns of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math> have zero mean). With double centered data the intercept in a linear regression model is always exactly equal to zero and hence the intercept must not be included in the model.</p></li>
<li><p>we do not assume that the residuals are normally distributed. For prediction purposes this is often not required (normality is particularly important for statistical inference in small samples).</p></li>
</ul>
<div id="linear-regression-for-multivariate-data-vs-high-dimensional-data" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Linear Regression for multivariate data vs High Dimensional Data</h2>
<ul>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><annotation encoding="application/x-tex">\mathbf{X^TX}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">(\mathbf{X^TX})^{-1}</annotation></semantics></math> are <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">p \times p</annotation></semantics></math> matrices</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><annotation encoding="application/x-tex">\mathbf{X^TX}</annotation></semantics></math> can only be inverted if it has rank <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math></p></li>
<li><p>Rank of a matrix of form <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><annotation encoding="application/x-tex">\mathbf{X^TX}</annotation></semantics></math>, with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">n\times p</annotation></semantics></math> matrix, can never be larger than <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>min</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>,</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\min(n,p)</annotation></semantics></math>.</p></li>
<li><p>in most regression problems <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>&gt;</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">n&gt;p</annotation></semantics></math> and rank of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\mathbf{X^TX})</annotation></semantics></math> equals <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math></p></li>
<li><p>in high dimensional regression problems <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&gt;</mo><mo>&gt;</mo><mo>&gt;</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">p &gt;&gt;&gt; n</annotation></semantics></math> and rank of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\mathbf{X^TX})</annotation></semantics></math> equals <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>&lt;</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">n&lt;p</annotation></semantics></math></p></li>
<li><p>in the toxicogenomics example <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>30</mn><mo>&lt;</mo><mi>p</mi><mo>=</mo><mn>4000</mn></mrow><annotation encoding="application/x-tex">n=30&lt;p=4000</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">rank</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>≤</mo><mi>n</mi><mo>=</mo><mn>30</mn></mrow><annotation encoding="application/x-tex">\text{rank}(\mathbf{X^TX})\leq n=30</annotation></semantics></math>.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>→</mo><annotation encoding="application/x-tex">\longrightarrow</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">(\mathbf{X^TX})^{-1}</annotation></semantics></math> does not exist, and neither does <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\boldsymbol{\beta}}</annotation></semantics></math>.</p></li>
</ul>
</div>
<div id="can-svd-help" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Can SVD help?</h2>
<ul>
<li><p>Since the columns of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math> are centered, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo>∝</mo><mtext mathvariant="normal">var</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>𝐗</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{X^TX} \propto \text{var}\left[\mathbf{X}\right]</annotation></semantics></math>.</p></li>
<li><p>if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">rank</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>n</mi><mo>=</mo><mn>30</mn></mrow><annotation encoding="application/x-tex">\text{rank}(\mathbf{X^TX})=n=30</annotation></semantics></math>, the PCA will give 30 components, each being a linear combination of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>4000</mn></mrow><annotation encoding="application/x-tex">p=4000</annotation></semantics></math> variables. These 30 PCs contain all information present in the original <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math> data.</p></li>
<li><p>if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">rank</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>n</mi><mo>=</mo><mn>30</mn></mrow><annotation encoding="application/x-tex">\text{rank}(\mathbf{X})=n=30</annotation></semantics></math>, the SVD of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math> is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐗</mi><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>δ</mi><mi>i</mi></msub><msub><mi>𝐮</mi><mi>i</mi></msub><msubsup><mi>𝐯</mi><mi>i</mi><mi>T</mi></msubsup><mo>=</mo><mi>𝐔</mi><mi>𝚫</mi><msup><mi>𝐕</mi><mi>T</mi></msup><mo>=</mo><msup><mrow><mi>𝐙</mi><mi>𝐕</mi></mrow><mi>T</mi></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">
   \mathbf{X} = \sum_{i=1}^n \delta_i \mathbf{u}_i \mathbf{v}_i^T = \mathbf{U} \boldsymbol{\Delta} \mathbf{V}^T = \mathbf{ZV}^T,
  </annotation></semantics></math>
with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐙</mi><annotation encoding="application/x-tex">\mathbf{Z}</annotation></semantics></math> the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n\times n</annotation></semantics></math> matrix with the scores on the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> PCs.</p></li>
<li><p>Still problematic because if we use all PCs <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">n=p</annotation></semantics></math>.</p></li>
</ul>
</div>
</div>
<div id="principal-component-regression" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Principal Component Regression</h1>
<p>A principal component regression (PCR) consists of</p>
<ol style="list-style-type: decimal">
<li><p>transforming <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>4000</mn></mrow><annotation encoding="application/x-tex">p=4000</annotation></semantics></math> dimensional <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>-variable to the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>30</mn></mrow><annotation encoding="application/x-tex">n=30</annotation></semantics></math> dimensional <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Z</mi><annotation encoding="application/x-tex">Z</annotation></semantics></math>-variable (PC scores). The <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> PCs are mutually uncorrelated.</p></li>
<li><p>using the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> PC-variables as regressors in a linear regression model</p></li>
<li><p>performing feature selection to select the most important regressors (PC).</p></li>
</ol>
<p>Feature selection is key, because we don’t want to have as many regressors as there are observations in the data. This would result in zero residual degrees of freedom. (see later)</p>
<hr />
<p>To keep the exposition general so that we allow for a feature selection to have taken place, I use the notation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐔</mi><mi>S</mi></msub><annotation encoding="application/x-tex">\mathbf{U}_S</annotation></semantics></math> to denote a matrix with left-singular column vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐮</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\mathbf{u}_i</annotation></semantics></math>, with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>∈</mo><mi>𝒮</mi></mrow><annotation encoding="application/x-tex">i \in {\mathcal{S}}</annotation></semantics></math> (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝒮</mi><annotation encoding="application/x-tex">{\mathcal{S}}</annotation></semantics></math> an index set referring to the PCs to be included in the regression model).</p>
<p>For example, suppose that a feature selection method has resulted in the selection of PCs 1, 3 and 12 for inclusion in the prediction model, then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝒮</mi><mo>=</mo><mo stretchy="false" form="prefix">{</mo><mn>1</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>12</mn><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">{\mathcal{S}}=\{1,3,12\}</annotation></semantics></math> and
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐔</mi><mi>S</mi></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>𝐮</mi><mn>1</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>𝐮</mi><mn>3</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>𝐮</mi><mn>12</mn></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
 \mathbf{U}_S = \begin{pmatrix}
  \mathbf{u}_1 &amp; \mathbf{u}_3 &amp; \mathbf{u}_{12}
 \end{pmatrix}.
</annotation></semantics></math></p>
<hr />
<div id="example-model-based-on-first-4-pcs" class="section level3" number="3.0.1">
<h3><span class="header-section-number">3.0.1</span> Example model based on first 4 PCs</h3>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">30</span></span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a>Uk <span class="ot">&lt;-</span> svdX<span class="sc">$</span>u[,<span class="dv">1</span><span class="sc">:</span>k]</span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a>Dk <span class="ot">&lt;-</span> <span class="fu">diag</span>(svdX<span class="sc">$</span>d[<span class="dv">1</span><span class="sc">:</span>k])</span>
<span id="cb24-4"><a href="#cb24-4" tabindex="-1"></a>Zk <span class="ot">&lt;-</span> Uk<span class="sc">%*%</span>Dk</span>
<span id="cb24-5"><a href="#cb24-5" tabindex="-1"></a>Y <span class="ot">&lt;-</span> toxData <span class="sc">%&gt;%</span></span>
<span id="cb24-6"><a href="#cb24-6" tabindex="-1"></a>  <span class="fu">pull</span>(BA)</span>
<span id="cb24-7"><a href="#cb24-7" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" tabindex="-1"></a>m4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y<span class="sc">~</span>Zk[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>])</span>
<span id="cb24-9"><a href="#cb24-9" tabindex="-1"></a><span class="fu">summary</span>(m4)</span></code></pre></div>
<pre><code>#&gt; 
#&gt; Call:
#&gt; lm(formula = Y ~ Zk[, 1:4])
#&gt; 
#&gt; Residuals:
#&gt;     Min      1Q  Median      3Q     Max 
#&gt; -2.1438 -0.7033 -0.1222  0.7255  2.2997 
#&gt; 
#&gt; Coefficients:
#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept)  7.961e-16  2.081e-01   0.000   1.0000    
#&gt; Zk[, 1:4]1  -5.275e-01  7.725e-02  -6.828 3.72e-07 ***
#&gt; Zk[, 1:4]2  -1.231e-02  8.262e-02  -0.149   0.8828    
#&gt; Zk[, 1:4]3  -1.759e-01  8.384e-02  -2.098   0.0461 *  
#&gt; Zk[, 1:4]4  -3.491e-02  8.396e-02  -0.416   0.6811    
#&gt; ---
#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
#&gt; 
#&gt; Residual standard error: 1.14 on 25 degrees of freedom
#&gt; Multiple R-squared:  0.672,  Adjusted R-squared:  0.6195 
#&gt; F-statistic:  12.8 on 4 and 25 DF,  p-value: 8.352e-06</code></pre>
<p>Note:</p>
<ul>
<li>the intercept is estimated as zero. (Why?) The model could have been fitted as</li>
</ul>
<pre><code>m4 &lt;- lm(Y~-1+Zk[,1:4])</code></pre>
<ul>
<li><p>the PC-predictors are uncorrelated (by construction)</p></li>
<li><p>first PC-predictors are not necessarily the most important predictors</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-values are not very meaningful when prediction is the objective</p></li>
</ul>
<p>Methods for feature selection will be discussed later.</p>
</div>
</div>
<div id="ridge-regression" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Ridge Regression</h1>
<div id="penalty" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Penalty</h2>
<p>The ridge parameter estimator is defined as the parameter <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\mathbf\beta</annotation></semantics></math> that minimises the <strong>penalised least squares criterion</strong></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">SSE</mtext><mtext mathvariant="normal">pen</mtext></msub><mo>=</mo><mo stretchy="false" form="postfix">‖</mo><mi>𝐘</mi><mo>−</mo><mrow><mi>𝐗</mi><mi>𝛃</mi></mrow><msubsup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><mo stretchy="false" form="postfix">‖</mo><mi>𝛃</mi><msubsup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">
 \text{SSE}_\text{pen}=\Vert\mathbf{Y} - \mathbf{X\beta}\Vert_2^2 + \lambda \Vert \boldsymbol{\beta} \Vert_2^2
</annotation></semantics></math></p>
<ul>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">‖</mo><mi>𝛃</mi><msubsup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn><mn>2</mn></msubsup><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></msubsup><msubsup><mi>β</mi><mi>j</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\Vert \boldsymbol{\beta} \Vert_2^2=\sum_{j=1}^p \beta_j^2</annotation></semantics></math> is the <strong><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding="application/x-tex">L_2</annotation></semantics></math> penalty term</strong></p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda&gt;0</annotation></semantics></math> is the penalty parameter (to be chosen by the user).</p></li>
</ul>
<p>Note, that that is equivalent to minimizing
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">‖</mo><mi>𝐘</mi><mo>−</mo><mrow><mi>𝐗</mi><mi>𝛃</mi></mrow><msubsup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn><mn>2</mn></msubsup><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> subject to </mtext><mspace width="0.333em"></mspace></mrow><mo stretchy="false" form="postfix">‖</mo><mi>𝛃</mi><msubsup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn><mn>2</mn></msubsup><mo>≤</mo><mi>s</mi></mrow><annotation encoding="application/x-tex">
\Vert\mathbf{Y} - \mathbf{X\beta}\Vert_2^2 \text{ subject to } \Vert \boldsymbol{\beta}\Vert^2_2\leq s
</annotation></semantics></math></p>
<p>Note, that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math> has a one-to-one correspondence with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></p>
</div>
<div id="graphical-interpretation" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Graphical interpretation</h2>
<p><img src="prediction_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
<div id="solution" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Solution</h2>
<p>The solution is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐘</mi></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
  \hat{\boldsymbol{\beta}} = (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} \mathbf{X^T Y}.
</annotation></semantics></math>
It can be shown that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\mathbf{X^TX}+\lambda \mathbf{I})</annotation></semantics></math> is always of rank <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda&gt;0</annotation></semantics></math>.</p>
<p>Hence, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\mathbf{X^TX}+\lambda \mathbf{I})</annotation></semantics></math> is invertible and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\boldsymbol{\beta}}</annotation></semantics></math> exists even if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&gt;</mo><mo>&gt;</mo><mo>&gt;</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">p&gt;&gt;&gt;n</annotation></semantics></math>.</p>
<p>We also find
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">var</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><msup><mi>𝐗</mi><mi>T</mi></msup><mi>𝐗</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">
  \text{var}\left[\hat{\mathbf\beta}\right] = (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} \mathbf{X}^T\mathbf{X} (\mathbf{X^TX}+\lambda \mathbf{I})^{-1}\sigma^2
</annotation></semantics></math></p>
<p>However, it can be shown that improved intervals that also account for the bias can be constructed by using:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">var</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><msup><mi>σ</mi><mn>2</mn></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">
  \text{var}\left[\hat{\mathbf\beta}\right] = (\mathbf{X^TX}+\lambda \mathbf{I})^{-1}  \sigma^2.
</annotation></semantics></math></p>
<div id="proof" class="section level3" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Proof</h3>
<p>The criterion to be minimised is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">SSE</mtext><mtext mathvariant="normal">pen</mtext></msub><mo>=</mo><mo stretchy="false" form="postfix">‖</mo><mi>𝐘</mi><mo>−</mo><mrow><mi>𝐗</mi><mi>𝛃</mi></mrow><msubsup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><mo stretchy="false" form="postfix">‖</mo><mi>𝛃</mi><msubsup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn><mn>2</mn></msubsup><mi>.</mi></mrow><annotation encoding="application/x-tex">
   \text{SSE}_\text{pen}=\Vert\mathbf{Y} - \mathbf{X\beta}\Vert_2^2 + \lambda \Vert \boldsymbol{\beta} \Vert_2^2.
 </annotation></semantics></math>
First we re-express SSE in matrix notation:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">SSE</mtext><mtext mathvariant="normal">pen</mtext></msub><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐘</mi><mo>−</mo><mrow><mi>𝐗</mi><mi>𝛃</mi></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐘</mi><mo>−</mo><mrow><mi>𝐗</mi><mi>𝛃</mi></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>λ</mi><msup><mi>𝛃</mi><mi>T</mi></msup><mi>𝛃</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">
   \text{SSE}_\text{pen} = (\mathbf{Y}-\mathbf{X\beta})^T(\mathbf{Y}-\mathbf{X\beta}) + \lambda \boldsymbol{\beta}^T\boldsymbol{\beta}.
 </annotation></semantics></math>
The partial derivative w.r.t. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math> is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>∂</mi><mrow><mi>∂</mi><mi>𝛃</mi></mrow></mfrac><msub><mtext mathvariant="normal">SSE</mtext><mtext mathvariant="normal">pen</mtext></msub><mo>=</mo><mi>−</mi><mn>2</mn><msup><mi>𝐗</mi><mi>T</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐘</mi><mo>−</mo><mrow><mi>𝐗</mi><mi>𝛃</mi></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mn>2</mn><mi>λ</mi><mi>𝛃</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">
   \frac{\partial}{\partial \boldsymbol{\beta}}\text{SSE}_\text{pen} = -2\mathbf{X}^T(\mathbf{Y}-\mathbf{X\beta})+2\lambda\boldsymbol{\beta}.
 </annotation></semantics></math>
Solving <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>∂</mi><mrow><mi>∂</mi><mi>𝛃</mi></mrow></mfrac><msub><mtext mathvariant="normal">SSE</mtext><mtext mathvariant="normal">pen</mtext></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\frac{\partial}{\partial \boldsymbol{\beta}}\text{SSE}_\text{pen}=0</annotation></semantics></math> gives
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐘</mi></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
   \hat{\boldsymbol{\beta}} = (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} \mathbf{X^T Y}.
 </annotation></semantics></math>
(assumption: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\mathbf{X^TX}+\lambda \mathbf{I})</annotation></semantics></math> is of rank <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>. This is always true if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda&gt;0</annotation></semantics></math>)</p>
</div>
</div>
<div id="link-with-svd" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Link with SVD</h2>
<div id="svd-and-inverse" class="section level3" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> SVD and inverse</h3>
<p>Write the SVD of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math> (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&gt;</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">p&gt;n</annotation></semantics></math>) as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐗</mi><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>δ</mi><mi>i</mi></msub><msub><mi>𝐮</mi><mi>i</mi></msub><msubsup><mi>𝐯</mi><mi>i</mi><mi>T</mi></msubsup><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><msub><mi>δ</mi><mi>i</mi></msub><msub><mi>𝐮</mi><mi>i</mi></msub><msubsup><mi>𝐯</mi><mi>i</mi><mi>T</mi></msubsup><mo>=</mo><mi>𝐔</mi><mi>𝚫</mi><msup><mi>𝐕</mi><mi>T</mi></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">
   \mathbf{X} = \sum_{i=1}^n \delta_i \mathbf{u}_i \mathbf{v}_i^T = \sum_{i=1}^p \delta_i \mathbf{u}_i \mathbf{v}_i^T  = \mathbf{U}\boldsymbol{\Delta} \mathbf{V}^T ,
</annotation></semantics></math>
with</p>
<ul>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>δ</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>δ</mi><mrow><mi>n</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>=</mo><mi>⋯</mi><mo>=</mo><msub><mi>δ</mi><mi>p</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\delta_{n+1}=\delta_{n+2}= \cdots = \delta_p=0</annotation></semantics></math></p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚫</mi><annotation encoding="application/x-tex">\boldsymbol{\Delta}</annotation></semantics></math> a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">p\times p</annotation></semantics></math> diagonal matrix of the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>δ</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>δ</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">\delta_1,\ldots, \delta_p</annotation></semantics></math></p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐔</mi><annotation encoding="application/x-tex">\mathbf{U}</annotation></semantics></math> an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">n\times p</annotation></semantics></math> matrix and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐕</mi><annotation encoding="application/x-tex">\mathbf{V}</annotation></semantics></math> a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">p \times p</annotation></semantics></math> matrix. Note that only the first <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> columns of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐔</mi><annotation encoding="application/x-tex">\mathbf{U}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐕</mi><annotation encoding="application/x-tex">\mathbf{V}</annotation></semantics></math> are informative.</p></li>
</ul>
<p>With the SVD of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math> we write
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐗</mi><mi>T</mi></msup><mi>𝐗</mi><mo>=</mo><mi>𝐕</mi><msup><mi>𝚫</mi><mn>2</mn></msup><msup><mi>𝐕</mi><mi>T</mi></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">
   \mathbf{X}^T\mathbf{X} = \mathbf{V}\boldsymbol{\Delta
     }^2\mathbf{V}^T.
 </annotation></semantics></math>
The inverse of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐗</mi><mi>T</mi></msup><mi>𝐗</mi></mrow><annotation encoding="application/x-tex">\mathbf{X}^T\mathbf{X}</annotation></semantics></math> is then given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>T</mi></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo>=</mo><mi>𝐕</mi><msup><mi>𝚫</mi><mrow><mi>−</mi><mn>2</mn></mrow></msup><msup><mi>𝐕</mi><mi>T</mi></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">
   (\mathbf{X}^T\mathbf{X})^{-1} = \mathbf{V}\boldsymbol{\Delta}^{-2}\mathbf{V}^T.
 </annotation></semantics></math>
Since <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚫</mi><annotation encoding="application/x-tex">\boldsymbol{\Delta}</annotation></semantics></math> has <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>δ</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>δ</mi><mrow><mi>n</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>=</mo><mi>⋯</mi><mo>=</mo><msub><mi>δ</mi><mi>p</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\delta_{n+1}=\delta_{n+2}= \cdots = \delta_p=0</annotation></semantics></math>, it is not invertible.</p>
</div>
<div id="svd-of-penalised-matrix-mathbfxtxlambda-mathbfi" class="section level3" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> SVD of penalised matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo>+</mo><mi>λ</mi><mi>𝐈</mi></mrow><annotation encoding="application/x-tex">\mathbf{X^TX}+\lambda \mathbf{I}</annotation></semantics></math></h3>
<p>It can be shown that
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo>=</mo><mi>𝐕</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝚫</mi><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>𝐕</mi><mi>T</mi></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">
  \mathbf{X^TX}+\lambda \mathbf{I} = \mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I}) \mathbf{V}^T ,
</annotation></semantics></math>
i.e. adding a constant to the diagonal elements does not affect the eigenvectors, and all eigenvalues are increased by this constant.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>→</mo><annotation encoding="application/x-tex">\longrightarrow</annotation></semantics></math> zero eigenvalues become <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>.</p>
<p>Hence,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo>=</mo><mi>𝐕</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝚫</mi><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><msup><mi>𝐕</mi><mi>T</mi></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">
  (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} = \mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I})^{-1} \mathbf{V}^T ,
</annotation></semantics></math>
which can be computed even when some eigenvalues in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝚫</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\boldsymbol{\Delta}^2</annotation></semantics></math> are zero.</p>
<p>Note, that for high dimensional data (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&gt;</mo><mo>&gt;</mo><mo>&gt;</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">p&gt;&gt;&gt;n</annotation></semantics></math>) many eigenvalues are zero because <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><annotation encoding="application/x-tex">\mathbf{X^TX}</annotation></semantics></math> is a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">p \times p</annotation></semantics></math> matrix and has rank <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>.</p>
<p>The identity <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo>=</mo><mi>𝐕</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝚫</mi><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>𝐕</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{X^TX}+\lambda \mathbf{I} = \mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I}) \mathbf{V}^T</annotation></semantics></math> is easily checked:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐕</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝚫</mi><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>𝐕</mi><mi>T</mi></msup><mo>=</mo><mi>𝐕</mi><msup><mi>𝚫</mi><mn>2</mn></msup><msup><mi>𝐕</mi><mi>T</mi></msup><mo>+</mo><mi>λ</mi><msup><mrow><mi>𝐕</mi><mi>𝐕</mi></mrow><mi>T</mi></msup><mo>=</mo><mi>𝐕</mi><msup><mi>𝚫</mi><mn>2</mn></msup><msup><mi>𝐕</mi><mi>T</mi></msup><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo>=</mo><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">
  \mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I}) \mathbf{V}^T = \mathbf{V}\boldsymbol{\Delta}^2\mathbf{V}^T + \lambda \mathbf{VV}^T  = \mathbf{V}\boldsymbol{\Delta}^2\mathbf{V}^T + \lambda \mathbf{I} = \mathbf{X^TX}+\lambda \mathbf{I}.
</annotation></semantics></math></p>
</div>
</div>
<div id="properties" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Properties</h2>
<ul>
<li><p>The Ridge estimator is biased! The <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math> are shrunken to zero!
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">]</mo></mrow></mtd><mtd columnalign="center" style="text-align: center"><mo>=</mo></mtd><mtd columnalign="left" style="text-align: left"><msup><mrow><mo stretchy="true" form="prefix">(</mo><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><msup><mi>𝐗</mi><mi>T</mi></msup><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>𝐘</mi><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="center" style="text-align: center"><mo>=</mo></mtd><mtd columnalign="left" style="text-align: left"><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>T</mi></msup><mi>𝐗</mi><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><msup><mi>𝐗</mi><mi>T</mi></msup><mi>𝐗</mi><mi>𝛃</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{eqnarray}
 \text{E}[\hat{\boldsymbol{\beta}}] &amp;=&amp; (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} \mathbf{X}^T \text{E}[\mathbf{Y}]\\
&amp;=&amp; (\mathbf{X}^T\mathbf{X}+\lambda \mathbf{I})^{-1} \mathbf{X}^T \mathbf{X}\boldsymbol{\beta}\\
\end{eqnarray}</annotation></semantics></math></p></li>
<li><p>Note, that the shrinkage is larger in the direction of the smaller eigenvalues.</p></li>
</ul>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">]</mo></mrow></mtd><mtd columnalign="center" style="text-align: center"><mo>=</mo></mtd><mtd columnalign="left" style="text-align: left"><mi>𝐕</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝚫</mi><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><msup><mi>𝐕</mi><mi>T</mi></msup><mi>𝐕</mi><msup><mi>𝚫</mi><mn>2</mn></msup><msup><mi>𝐕</mi><mi>T</mi></msup><mi>𝛃</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="center" style="text-align: center"><mo>=</mo></mtd><mtd columnalign="left" style="text-align: left"><mi>𝐕</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝚫</mi><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><msup><mi>𝚫</mi><mn>2</mn></msup><msup><mi>𝐕</mi><mi>T</mi></msup><mi>𝛃</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="center" style="text-align: center"><mo>=</mo></mtd><mtd columnalign="left" style="text-align: left"><mi>𝐕</mi><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mfrac><msubsup><mi>δ</mi><mn>1</mn><mn>2</mn></msubsup><mrow><msubsup><mi>δ</mi><mn>1</mn><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi></mrow></mfrac></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd><mtd columnalign="center" style="text-align: center"></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mfrac><msubsup><mi>δ</mi><mi>r</mi><mn>2</mn></msubsup><mrow><msubsup><mi>δ</mi><mi>r</mi><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi></mrow></mfrac></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><msup><mi>𝐕</mi><mi>T</mi></msup><mi>𝛃</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{eqnarray}
\text{E}[\hat{\boldsymbol{\beta}}]&amp;=&amp;\mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I})^{-1} \mathbf{V}^T \mathbf{V} \boldsymbol{\Delta}^2 \mathbf{V}^T\boldsymbol{\beta}\\
&amp;=&amp;\mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I})^{-1} \boldsymbol{\Delta}^2 \mathbf{V}^T\boldsymbol{\beta}\\
&amp;=&amp; \mathbf{V}
\left[\begin{array}{ccc}
\frac{\delta_1^2}{\delta_1^2+\lambda}&amp;\ldots&amp;0 \\
&amp;\vdots&amp;\\
0&amp;\ldots&amp;\frac{\delta_r^2}{\delta_r^2+\lambda}
\end{array}\right]
\mathbf{V}^T\boldsymbol{\beta}
\end{eqnarray}</annotation></semantics></math></p>
<ul>
<li><p>the variance of the prediction <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>Y</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>𝐱</mi><mi>T</mi></msup><mover><mi>β</mi><mo accent="true">̂</mo></mover></mrow><annotation encoding="application/x-tex">\hat{{Y}}(\mathbf{x})=\mathbf{x}^T\hat\beta</annotation></semantics></math>,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">var</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mover><mi>Y</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∣</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><msup><mi>𝐱</mi><mi>T</mi></msup><msup><mrow><mo stretchy="true" form="prefix">(</mo><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>𝐱</mi></mrow><annotation encoding="application/x-tex">
 \text{var}\left[\hat{{Y}}(\mathbf{x})\mid \mathbf{x}\right] = \mathbf{x}^T(\mathbf{X^TX}+\lambda \mathbf{I})^{-1}\mathbf{x}
  </annotation></semantics></math>
is smaller than with the least-squares estimator.</p></li>
<li><p>through the bias-variance trade-off it is hoped that better predictions in terms of expected conditional test error can be obtained, for an appropriate choice of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>.</p></li>
</ul>
<p>Recall the expression of the expected conditional test error
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>E</mi><mi>r</mi><mi>r</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="center" style="text-align: center"><mo>=</mo></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>Y</mi><mo accent="true">̂</mo></mover><mo>−</mo><msup><mi>Y</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>∣</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="center" style="text-align: center"><mo>=</mo></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">var</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mover><mi>Y</mi><mo accent="true">̂</mo></mover><mo>∣</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><msup><mtext mathvariant="normal">bias</mtext><mn>2</mn></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mtext mathvariant="normal">var</mtext><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>Y</mi><mo>*</mo></msup><mo>∣</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{eqnarray}
  Err(\mathbf{x}) &amp;=&amp; \text{E}\left[(\hat{Y} - Y^*)^2\mid \mathbf{x}\right]\\
  &amp;=&amp;
  \text{var}\left[\hat{Y}\mid \mathbf{x}\right] + \text{bias}^2(\mathbf{x})+
  \text{var}\left[Y^*\mid \mathbf{x}\right]
\end{eqnarray}</annotation></semantics></math>
where</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>Y</mi><mo accent="true">̂</mo></mover><mo>=</mo><mover><mi>Y</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>𝐱</mi><mi>T</mi></msup><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover></mrow><annotation encoding="application/x-tex">\hat{Y}=\hat{Y}(\mathbf{x})=\mathbf{x}^T\hat{\boldsymbol{\beta}}</annotation></semantics></math> is the prediction at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Y</mi><mo>*</mo></msup><annotation encoding="application/x-tex">Y^*</annotation></semantics></math> is an outcome at predictor <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mover><mi>Y</mi><mo accent="true">̂</mo></mover><mo>∣</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> and </mtext><mspace width="0.333em"></mspace></mrow><msup><mi>μ</mi><mo>*</mo></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>Y</mi><mo>*</mo></msup><mo>∣</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mu(\mathbf{x}) = \text{E}\left[\hat{Y}\mid \mathbf{x}\right] \text{ and } \mu^*(x)=\text{E}\left[Y^*\mid \mathbf{x}\right]</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">bias</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>μ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msup><mi>μ</mi><mo>*</mo></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{bias}(\mathbf{x})=\mu(\mathbf{x})-\mu^*(\mathbf{x})</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">var</mtext><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>Y</mi><mo>*</mo></msup><mo>∣</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\text{var}\left[Y^*\mid \mathbf{x}\right]</annotation></semantics></math> the irreducible error that does not depend on the model. It simply originates from observations that randomly fluctuate around the true mean <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>μ</mi><mo>*</mo></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mu^*(x)</annotation></semantics></math>.</li>
</ul>
</div>
<div id="toxicogenomics-example" class="section level2" number="4.6">
<h2><span class="header-section-number">4.6</span> Toxicogenomics example</h2>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb27-2"><a href="#cb27-2" tabindex="-1"></a>mRidge <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(</span>
<span id="cb27-3"><a href="#cb27-3" tabindex="-1"></a>  <span class="at">x =</span> toxData[,<span class="sc">-</span><span class="dv">1</span>] <span class="sc">%&gt;%</span></span>
<span id="cb27-4"><a href="#cb27-4" tabindex="-1"></a>    as.matrix,</span>
<span id="cb27-5"><a href="#cb27-5" tabindex="-1"></a>  <span class="at">y =</span> toxData <span class="sc">%&gt;%</span></span>
<span id="cb27-6"><a href="#cb27-6" tabindex="-1"></a>    <span class="fu">pull</span>(BA),</span>
<span id="cb27-7"><a href="#cb27-7" tabindex="-1"></a>  <span class="at">alpha =</span> <span class="dv">0</span>) <span class="co"># ridge: alpha = 0</span></span>
<span id="cb27-8"><a href="#cb27-8" tabindex="-1"></a></span>
<span id="cb27-9"><a href="#cb27-9" tabindex="-1"></a><span class="fu">plot</span>(mRidge, <span class="at">xvar=</span><span class="st">&quot;lambda&quot;</span>)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>The R function uses to refer to the penalty parameter. In this course we use <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>, because <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> is often used as eigenvalues.</p>
<p>The graph shows that with increasing penalty parameter, the parameter estimates are shrunken towards zero. The estimates will only reach zero for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mo>→</mo><mi>∞</mi></mrow><annotation encoding="application/x-tex">\lambda \rightarrow \infty</annotation></semantics></math>. The stronger the shrinkage, the larger the bias (towards zero) and the smaller the variance of the parameter estimators (and hence also smaller variance of the predictions).</p>
<p>Another (informal) viewpoint is the following. By shrinking the estimates towards zero, the estimates loose some of their ``degrees of freedom’’ so that the parameters become estimable with only <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>&lt;</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">n&lt;p</annotation></semantics></math> data points. Even with a very small <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda&gt;0</annotation></semantics></math>, the parameters regain their estimability. However, note that the variance of the estimator is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">var</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mrow><msup><mi>𝐗</mi><mi>𝐓</mi></msup><mi>𝐗</mi></mrow><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><msup><mi>σ</mi><mn>2</mn></msup><mo>=</mo><mi>𝐕</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝚫</mi><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mi>𝐈</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><msup><mi>𝐕</mi><mi>T</mi></msup><msup><mi>σ</mi><mn>2</mn></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">
  \text{var}\left[\hat{\mathbf\beta}\right] = (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} \sigma^2 = \mathbf{V}(\boldsymbol{\Delta}^2+\lambda\mathbf{I})^{-1}\mathbf{V}^T\sigma^2.
</annotation></semantics></math>
Hence, a small <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> will result in large variances of the parameter estimators. The larger <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>, the smaller the variances become. In the limit, as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mo>→</mo><mi>∞</mi></mrow><annotation encoding="application/x-tex">\lambda\rightarrow\infty</annotation></semantics></math>, the estimates are converged to zero and show no variability any longer.</p>
</div>
</div>
<div id="lasso-regression" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Lasso Regression</h1>
<ul>
<li><p>The Lasso is another example of penalised regression.</p></li>
<li><p>The lasso estimator of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math> is the solution to minimising the penalised SSE
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">SSE</mtext><mtext mathvariant="normal">pen</mtext></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Y</mi><mi>i</mi></msub><mo>−</mo><msubsup><mi>𝐱</mi><mi>i</mi><mi>T</mi></msubsup><mi>𝛃</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mo stretchy="false" form="postfix">|</mo><msub><mi>β</mi><mi>j</mi></msub><mo stretchy="false" form="postfix">|</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">
 \text{SSE}_\text{pen} = \sum_{i=1}^n (Y_i - \mathbf{x}_i^T\boldsymbol{\beta})^2 + \lambda \sum_{j=1}^p \vert \beta_j\vert.
</annotation></semantics></math></p></li>
</ul>
<p>or, equivalently, minimising</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">SSE</mtext><mo>=</mo><mo stretchy="false" form="postfix">‖</mo><mi>𝐘</mi><mo>−</mo><mrow><mi>𝐗</mi><mi>𝛃</mi></mrow><msubsup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn><mn>2</mn></msubsup><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> subject to </mtext><mspace width="0.333em"></mspace></mrow><mo stretchy="false" form="postfix">‖</mo><mi>𝛃</mi><msub><mo stretchy="false" form="postfix">‖</mo><mn>1</mn></msub><mo>≤</mo><mi>c</mi></mrow><annotation encoding="application/x-tex">
\text{SSE}  = \Vert \mathbf{Y} - \mathbf{X\beta}\Vert_2^2 \text{ subject to } \Vert \mathbf\beta\Vert_1 \leq c
</annotation></semantics></math>
with</p>
<ul>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">‖</mo><mi>𝛃</mi><msub><mo stretchy="false" form="postfix">‖</mo><mn>1</mn></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mo stretchy="false" form="postfix">|</mo><msub><mi>β</mi><mi>j</mi></msub><mo stretchy="false" form="postfix">|</mo></mrow><annotation encoding="application/x-tex">\Vert \mathbf\beta\Vert_1 = \sum\limits_{j=1}^p \vert \beta_j \vert</annotation></semantics></math></p></li>
<li><p>Despite strong similarity between ridge and lasso regression (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding="application/x-tex">L_2</annotation></semantics></math> versus <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding="application/x-tex">L_1</annotation></semantics></math> norm in penalty term), there is no analytical solution of the lasso parameter estimator of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\mathbf\beta</annotation></semantics></math>.</p></li>
<li><p>Fortunately, computational efficient algorithms have been implemented in statistical software</p></li>
<li><p>The Lasso estimator of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math> is biased and generally has a smaller variance then the least-squares estimator.</p></li>
<li><p>Hence, the bias-variance trade-off may here also help in finding better predictions with biased estimators.</p></li>
<li><p>In contrast to ridge regression, however, the lasso estimator can give at most <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>min</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\min(p,n)</annotation></semantics></math> non-zero <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>-estimates.</p></li>
<li><p>Hence, at first sight the lasso is not directly appropriate for high-dimensional settings.</p></li>
<li><p>An important advantage of the lasso is that choosing an appropriate value for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> is a kind a model building or feature selection procedure (see further).</p></li>
</ul>
<div id="graphical-interpretation-of-lasso-vs-ridge" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Graphical interpretation of Lasso vs ridge</h2>
<p>Note that the lasso is a constrained regression problem with</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">‖</mo><mi>𝐘</mi><mo>−</mo><mrow><mi>𝐗</mi><mi>𝛃</mi></mrow><msubsup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn><mn>2</mn></msubsup><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> subject to </mtext><mspace width="0.333em"></mspace></mrow><mo stretchy="false" form="postfix">‖</mo><mi>𝛃</mi><msub><mo stretchy="false" form="postfix">‖</mo><mn>1</mn></msub><mo>≤</mo><mi>c</mi></mrow><annotation encoding="application/x-tex">
\Vert \mathbf{Y} - \mathbf{X\beta}\Vert_2^2 \text{ subject to } \Vert \mathbf\beta\Vert_1 \leq c
</annotation></semantics></math>
and ridge
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">‖</mo><mi>𝐘</mi><mo>−</mo><mrow><mi>𝐗</mi><mi>𝛃</mi></mrow><msubsup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn><mn>2</mn></msubsup><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> subject to </mtext><mspace width="0.333em"></mspace></mrow><mo stretchy="false" form="postfix">‖</mo><mi>𝛃</mi><msubsup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn><mn>2</mn></msubsup><mo>≤</mo><mi>c</mi></mrow><annotation encoding="application/x-tex">
\Vert \mathbf{Y} - \mathbf{X\beta}\Vert_2^2 \text{ subject to } \Vert \mathbf\beta\Vert^2_2 \leq c
</annotation></semantics></math></p>
<p><img src="prediction_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>Note, that</p>
<ul>
<li>parameters for the lasso can never switch sign, they are set at zero! Selection!</li>
<li>ridge regression can lead to parameters that switch sign.</li>
</ul>
</div>
<div id="toxicogenomics-example-1" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Toxicogenomics example</h2>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a>mLasso <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(</span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a>  <span class="at">x =</span> toxData[,<span class="sc">-</span><span class="dv">1</span>] <span class="sc">%&gt;%</span></span>
<span id="cb28-3"><a href="#cb28-3" tabindex="-1"></a>    as.matrix,</span>
<span id="cb28-4"><a href="#cb28-4" tabindex="-1"></a>  <span class="at">y =</span> toxData <span class="sc">%&gt;%</span></span>
<span id="cb28-5"><a href="#cb28-5" tabindex="-1"></a>    <span class="fu">pull</span>(BA),</span>
<span id="cb28-6"><a href="#cb28-6" tabindex="-1"></a><span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb28-7"><a href="#cb28-7" tabindex="-1"></a><span class="fu">plot</span>(mLasso, <span class="at">xvar =</span> <span class="st">&quot;lambda&quot;</span>)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<ul>
<li><p>The graph with the paths of the parameter estimates nicely illustrates the typical behaviour of the lasso estimates as a function of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>: when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> increases the estimates are shrunken towards zero.</p></li>
<li><p>When an estimate hits zero, it remains exactly equal to zero when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math> further increases. A parameter estimate equal to zero, say <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>β</mi><mo accent="true">̂</mo></mover><mi>j</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\hat\beta_j=0</annotation></semantics></math>, implies that the corresponding predictor <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>j</mi></msub><annotation encoding="application/x-tex">x_j</annotation></semantics></math> is no longer included in the model (i.e. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mi>j</mi></msub><msub><mi>x</mi><mi>j</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\beta_jx_j=0</annotation></semantics></math>).</p></li>
<li><p>The model fit is known as a sparse model fit (many zeroes). Hence, choosing a appropriate value for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math> is like choosing the important predictors in the model (feature selection).</p></li>
</ul>
</div>
</div>
<div id="splines-and-the-connection-to-ridge-regression." class="section level1" number="6">
<h1><span class="header-section-number">6</span> Splines and the connection to ridge regression.</h1>
<div id="lidar-dataset" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Lidar dataset</h2>
<ul>
<li><p>LIDAR (light detection and ranging) uses the reflection of laser-emitted light to detect chemical compounds in the atmosphere.</p></li>
<li><p>The LIDAR technique has proven to be an efficient tool for monitoring the distribution of several atmospheric pollutants of importance; see Sigrist (1994).</p></li>
<li><p>The range is the distance traveled before the light is reflected back to its source.</p></li>
<li><p>The logratio is the logarithm of the ratio of received light from two laser sources.</p>
<ul>
<li><p>One source had a frequency equal to the resonance frequency of the compound of interest, which was mercury in this study.</p></li>
<li><p>The other source had a frequency off this resonance frequency.</p></li>
<li><p>The concentration of mercury can be derived from a regression model of the logratio in function of the range for each range x.</p></li>
</ul></li>
</ul>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;SemiPar&quot;</span>)</span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a><span class="fu">data</span>(lidar)</span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a>pLidar <span class="ot">&lt;-</span> lidar <span class="sc">%&gt;%</span></span>
<span id="cb29-4"><a href="#cb29-4" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> range, <span class="at">y =</span> logratio)) <span class="sc">+</span></span>
<span id="cb29-5"><a href="#cb29-5" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb29-6"><a href="#cb29-6" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;range (m)&quot;</span>)</span>
<span id="cb29-7"><a href="#cb29-7" tabindex="-1"></a></span>
<span id="cb29-8"><a href="#cb29-8" tabindex="-1"></a>pLidar <span class="sc">+</span></span>
<span id="cb29-9"><a href="#cb29-9" tabindex="-1"></a>  <span class="fu">geom_smooth</span>()</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<ul>
<li>The data is non-linear</li>
<li>Linear regression will not work!</li>
<li>The data shows a smooth relation between the logratio and the range</li>
</ul>
</div>
<div id="basis-expansion" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Basis expansion</h2>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msub><mi>ϵ</mi><mi>i</mi></msub><mo>,</mo></mrow><annotation encoding="application/x-tex">y_i=f(x_i)+\epsilon_i,</annotation></semantics></math>
with
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msub><mi>θ</mi><mi>k</mi></msub><msub><mi>b</mi><mi>k</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)=\sum\limits_{k=1}^K \theta_k b_k(x)</annotation></semantics></math></p>
<ul>
<li><p>Select set of basis functions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mi>k</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">b_k(x)</annotation></semantics></math></p></li>
<li><p>Select number of basis functions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math></p></li>
<li><p>Examples</p>
<ul>
<li>Polynomial model: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>x</mi><mi>k</mi></msup><annotation encoding="application/x-tex">x^k</annotation></semantics></math></li>
<li>Orthogonal series: Fourier, Legendre polynomials, Wavelets</li>
<li>Polynomial splines: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>,</mo><mi>x</mi><mo>,</mo><msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>−</mo><msub><mi>t</mi><mi>m</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo></msub></mrow><annotation encoding="application/x-tex">1, x, (x-t_m)_+</annotation></semantics></math> with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>K</mi><mo>−</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">m=1, \ldots, K-2</annotation></semantics></math> knots <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>t</mi><mi>m</mi></msub><annotation encoding="application/x-tex">t_m</annotation></semantics></math></li>
<li>…</li>
</ul></li>
</ul>
<div id="trunctated-line-basis" class="section level3" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> Trunctated line basis</h3>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msub><mi>ϵ</mi><mi>i</mi></msub><mo>,</mo></mrow><annotation encoding="application/x-tex">y_i=f(x_i)+\epsilon_i,</annotation></semantics></math></p>
<ul>
<li>One of the most simple basis expansions</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><msub><mi>β</mi><mn>1</mn></msub><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>K</mi><mo>−</mo><mn>2</mn></mrow></munderover><msub><mi>θ</mi><mi>m</mi></msub><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>t</mi><mi>m</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo></msub></mrow><annotation encoding="application/x-tex">f(x_i)=\beta_0+\beta_1x_i+\sum\limits_{m=1}^{K-2}\theta_m(x_i-t_m)_+</annotation></semantics></math> with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>.</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo></msub><annotation encoding="application/x-tex">(.)_+</annotation></semantics></math> the operator that takes the positive part.</li>
<li>Note, that better basis expansions exist, which are orthogonal, computational more stable and/or continuous derivative beyond first order</li>
<li>We will use this basis for didactical purposes</li>
<li>We can use OLS to fit y w.r.t. the basis.</li>
</ul>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a>knots <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">400</span>,<span class="dv">700</span>,<span class="fl">12.5</span>)</span>
<span id="cb30-2"><a href="#cb30-2" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" tabindex="-1"></a>basis <span class="ot">&lt;-</span> <span class="fu">sapply</span>(knots,</span>
<span id="cb30-4"><a href="#cb30-4" tabindex="-1"></a>  <span class="cf">function</span>(k,y) (y<span class="sc">-</span>k)<span class="sc">*</span>(y<span class="sc">&gt;</span>k),</span>
<span id="cb30-5"><a href="#cb30-5" tabindex="-1"></a>  <span class="at">y=</span> lidar <span class="sc">%&gt;%</span> <span class="fu">pull</span>(range)</span>
<span id="cb30-6"><a href="#cb30-6" tabindex="-1"></a>  )</span>
<span id="cb30-7"><a href="#cb30-7" tabindex="-1"></a></span>
<span id="cb30-8"><a href="#cb30-8" tabindex="-1"></a>basisExp <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, <span class="at">range =</span> lidar <span class="sc">%&gt;%</span> <span class="fu">pull</span>(range), basis)</span>
<span id="cb30-9"><a href="#cb30-9" tabindex="-1"></a></span>
<span id="cb30-10"><a href="#cb30-10" tabindex="-1"></a>splineFitLs <span class="ot">&lt;-</span> <span class="fu">lm</span>(logratio <span class="sc">~</span> <span class="sc">-</span><span class="dv">1</span> <span class="sc">+</span> basisExp, lidar)</span>
<span id="cb30-11"><a href="#cb30-11" tabindex="-1"></a></span>
<span id="cb30-12"><a href="#cb30-12" tabindex="-1"></a>pBasis <span class="ot">&lt;-</span> basisExp[,<span class="sc">-</span><span class="dv">1</span>] <span class="sc">%&gt;%</span></span>
<span id="cb30-13"><a href="#cb30-13" tabindex="-1"></a>  data.frame <span class="sc">%&gt;%</span></span>
<span id="cb30-14"><a href="#cb30-14" tabindex="-1"></a>  <span class="fu">gather</span>(<span class="st">&quot;basis&quot;</span>,<span class="st">&quot;values&quot;</span>,<span class="sc">-</span><span class="dv">1</span>) <span class="sc">%&gt;%</span></span>
<span id="cb30-15"><a href="#cb30-15" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> range, <span class="at">y =</span> values, <span class="at">color =</span> basis)) <span class="sc">+</span></span>
<span id="cb30-16"><a href="#cb30-16" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb30-17"><a href="#cb30-17" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">&quot;none&quot;</span>) <span class="sc">+</span></span>
<span id="cb30-18"><a href="#cb30-18" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;basis&quot;</span>)</span>
<span id="cb30-19"><a href="#cb30-19" tabindex="-1"></a></span>
<span id="cb30-20"><a href="#cb30-20" tabindex="-1"></a><span class="fu">grid.arrange</span>(</span>
<span id="cb30-21"><a href="#cb30-21" tabindex="-1"></a>  pLidar <span class="sc">+</span></span>
<span id="cb30-22"><a href="#cb30-22" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> lidar<span class="sc">$</span>range, <span class="at">y =</span> splineFitLs<span class="sc">$</span>fitted), <span class="at">lwd =</span> <span class="dv">2</span>),</span>
<span id="cb30-23"><a href="#cb30-23" tabindex="-1"></a>  pBasis,</span>
<span id="cb30-24"><a href="#cb30-24" tabindex="-1"></a>  <span class="at">ncol=</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<ul>
<li>Note, that the model is overfitting!</li>
<li>The fit is very wiggly and is tuned too much to the data.</li>
<li>The fit has a large variance and low bias.</li>
<li>It will therefore not generalise well to predict the logratio of future observations.</li>
</ul>
<div id="solution-for-overfitting" class="section level4" number="6.2.1.1">
<h4><span class="header-section-number">6.2.1.1</span> Solution for overfitting?</h4>
<ul>
<li><p>We could perform model selection on the basis to select the important basis functions to model the signal. But, this will have the undesired property that the fit will no longer be smooth.</p></li>
<li><p>We can also adopt a ridge penalty!</p></li>
<li><p>However, we do not want to penalise the intercept and the linear term.</p></li>
<li><p>Ridge criterion</p></li>
</ul>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">‖</mo><mi>𝐘</mi><mo>−</mo><mrow><mi>𝐗</mi><mi>𝛃</mi></mrow><msup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn></msup><mo>+</mo><mi>λ</mi><msup><mi>𝛃</mi><mi>T</mi></msup><mi>𝐃</mi><mi>𝛃</mi></mrow><annotation encoding="application/x-tex">\Vert\mathbf{Y}-\mathbf{X\beta}\Vert^2+\lambda\boldsymbol{\beta}^T\mathbf{D}\boldsymbol{\beta}
</annotation></semantics></math></p>
<p>With <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐃</mi><annotation encoding="application/x-tex">\mathbf{D}</annotation></semantics></math> with dimensions (K,K): <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐃</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mn>𝟎</mn><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mn>𝟎</mn><mrow><mn>2</mn><mo>×</mo><mi>K</mi><mo>−</mo><mn>2</mn></mrow></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mn>𝟎</mn><mrow><mi>K</mi><mo>−</mo><mn>2</mn><mo>×</mo><mn>2</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>𝐈</mi><mrow><mi>K</mi><mo>−</mo><mn>2</mn><mo>×</mo><mi>K</mi><mo>−</mo><mn>2</mn></mrow></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{D}=\left[\begin{array}{cc}\mathbf{0}_{2\times2}&amp; \mathbf{0}_{2\times K-2}\\
\mathbf{0}_{K-2\times2}&amp;\mathbf{I}_{K-2\times K-2}\end{array}\right]</annotation></semantics></math></p>
<ul>
<li>Here we will set the penalty at 900.</li>
</ul>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a>D <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="fu">ncol</span>(basisExp))</span>
<span id="cb31-2"><a href="#cb31-2" tabindex="-1"></a>D[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>,<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb31-3"><a href="#cb31-3" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="dv">900</span></span>
<span id="cb31-4"><a href="#cb31-4" tabindex="-1"></a>betaRidge <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(basisExp)<span class="sc">%*%</span>basisExp<span class="sc">+</span>(lambda<span class="sc">*</span>D))<span class="sc">%*%</span><span class="fu">t</span>(basisExp)<span class="sc">%*%</span>lidar<span class="sc">$</span>logratio</span>
<span id="cb31-5"><a href="#cb31-5" tabindex="-1"></a><span class="fu">grid.arrange</span>(</span>
<span id="cb31-6"><a href="#cb31-6" tabindex="-1"></a>  pLidar <span class="sc">+</span></span>
<span id="cb31-7"><a href="#cb31-7" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> lidar<span class="sc">$</span>range, <span class="at">y =</span> <span class="fu">c</span>(basisExp <span class="sc">%*%</span> betaRidge)), <span class="at">lwd =</span> <span class="dv">2</span>),</span>
<span id="cb31-8"><a href="#cb31-8" tabindex="-1"></a>  pBasis,</span>
<span id="cb31-9"><a href="#cb31-9" tabindex="-1"></a>  <span class="at">ncol=</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>How do we choose <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>?</p>
<hr />
</div>
</div>
</div>
</div>
<div id="evaluation-of-prediction-models" class="section level1" number="7">
<h1><span class="header-section-number">7</span> Evaluation of Prediction Models</h1>
<p>Predictions are calculated with the fitted model
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>Y</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>𝐱</mi><mi>T</mi></msup><mover><mi>β</mi><mo accent="true">̂</mo></mover></mrow><annotation encoding="application/x-tex">
   \hat{Y}(\mathbf{x}) = \hat{m}(\mathbf{x})=\mathbf{x}^T\hat{\beta}
 </annotation></semantics></math>
when focussing on prediction, we want the prediction error to be as small as possible.</p>
<p>The <strong>prediction error</strong> for a prediction at covariate pattern <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math> is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>Y</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msup><mi>Y</mi><mo>*</mo></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">
     \hat{Y}(\mathbf{x}) - Y^*,
  </annotation></semantics></math>
where</p>
<ul>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>Y</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>𝐱</mi><mi>T</mi></msup><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover></mrow><annotation encoding="application/x-tex">\hat{Y}(\mathbf{x})=\mathbf{x}^T\hat{\boldsymbol{\beta}}</annotation></semantics></math> is the prediction at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math></p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Y</mi><mo>*</mo></msup><annotation encoding="application/x-tex">Y^*</annotation></semantics></math> is an outcome at covariate pattern <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math></p></li>
</ul>
<p>Prediction is typically used to predict an outcome before it is observed.</p>
<ul>
<li>Hence, the outcome <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Y</mi><mo>*</mo></msup><annotation encoding="application/x-tex">Y^*</annotation></semantics></math> is not observed yet, and</li>
<li>the prediction error cannot be computed.</li>
</ul>
<hr />
<ul>
<li><p>Recall that the prediction model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>Y</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{Y}(\mathbf{x})</annotation></semantics></math> is estimated by using data in the training data set <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐗</mi><mo>,</mo><mi>𝐘</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\mathbf{X},\mathbf{Y})</annotation></semantics></math>, and</p></li>
<li><p>that the outcome <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Y</mi><mo>*</mo></msup><annotation encoding="application/x-tex">Y^*</annotation></semantics></math> is an outcome at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math> which is assumed to be independent of the training data.</p></li>
<li><p>Goal is to use prediction model for predicting a future observation (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Y</mi><mo>*</mo></msup><annotation encoding="application/x-tex">Y^*</annotation></semantics></math>), i.e. an observation that still has to be realised/observed (otherwise prediction seems rather useless).</p></li>
<li><p>Hence, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Y</mi><mo>*</mo></msup><annotation encoding="application/x-tex">Y^*</annotation></semantics></math> can never be part of the training data set.</p></li>
</ul>
<hr />
<p>Here we provide definitions and we show how the prediction performance of a prediction model can be evaluated from data.</p>
<p>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝒯</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐘</mi><mo>,</mo><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">{\mathcal{T}}=(\mathbf{Y},\mathbf{X})</annotation></semantics></math> denote the training data, from which the prediction model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>Y</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>⋅</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{Y}(\cdot)</annotation></semantics></math> is build. This building process typically involves feature selection and parameter estimation.</p>
<p>We will use a more general notation for the prediction model: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mover><mi>Y</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{m}(\mathbf{x})=\hat{Y}(\mathbf{x})</annotation></semantics></math>.</p>
<hr />
<div id="test-or-generalisation-error" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Test or Generalisation Error</h2>
<p>The test or generalisation error for prediction model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>⋅</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{m}(\cdot)</annotation></semantics></math> is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">Err</mtext><mi>𝒯</mi></msub><mo>=</mo><msub><mtext mathvariant="normal">E</mtext><mrow><msup><mi>Y</mi><mo>*</mo></msup><mo>,</mo><msup><mi>X</mi><mo>*</mo></msup></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msup><mi>Y</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>∣</mo><mi>𝒯</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">
    \text{Err}_{\mathcal{T}} = \text{E}_{Y^*,X^*}\left[(\hat{m}(\mathbf{X}^*) - Y^*)^2\mid {\mathcal{T}}\right]
  </annotation></semantics></math>
where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Y</mi><mo>*</mo></msup><mo>,</mo><msup><mi>X</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(Y^*,X^*)</annotation></semantics></math> is independent of the training data.</p>
<hr />
<ul>
<li>Note that the test error is conditional on the training data <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝒯</mi><annotation encoding="application/x-tex">{\mathcal{T}}</annotation></semantics></math>.</li>
<li>Hence, the test error evaluates the performance of the single model build from the observed training data.</li>
<li>This is the ultimate target of the model assessment, because it is exactly this prediction model that will be used in practice and applied to future predictors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝐗</mi><mo>*</mo></msup><annotation encoding="application/x-tex">\mathbf{X}^*</annotation></semantics></math> to predict <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Y</mi><mo>*</mo></msup><annotation encoding="application/x-tex">Y^*</annotation></semantics></math>.</li>
<li>The test error is defined as an average over all such future observations <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Y</mi><mo>*</mo></msup><mo>,</mo><msup><mi>𝐗</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(Y^*,\mathbf{X}^*)</annotation></semantics></math>.</li>
</ul>
<hr />
</div>
<div id="conditional-test-error" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Conditional test error</h2>
<p>Sometimes the conditional test error is used:</p>
<p>The conditional test error in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math> for prediction model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{m}(\mathbf{x})</annotation></semantics></math> is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">Err</mtext><mi>𝒯</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mtext mathvariant="normal">E</mtext><msup><mi>Y</mi><mo>*</mo></msup></msub><mrow><mo stretchy="true" form="prefix">[</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msup><mi>Y</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>∣</mo><mi>𝒯</mi><mo>,</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">
   \text{Err}_{\mathcal{T}}(\mathbf{x}) = \text{E}_{Y^*}\left[(\hat{m}(\mathbf{x}) - Y^*)^2\mid {\mathcal{T}}, \mathbf{x}\right]
 </annotation></semantics></math>
where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Y</mi><mo>*</mo></msup><annotation encoding="application/x-tex">Y^*</annotation></semantics></math> is an outcome at predictor <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math>, independent of the training data.</p>
<p>Hence,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">Err</mtext><mi>𝒯</mi></msub><mo>=</mo><msub><mtext mathvariant="normal">E</mtext><msup><mi>X</mi><mo>*</mo></msup></msub><mrow><mo stretchy="true" form="prefix">[</mo><msub><mtext mathvariant="normal">Err</mtext><mi>𝒯</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
   \text{Err}_{\mathcal{T}} = \text{E}_{X^*}\left[\text{Err}_{\mathcal{T}}(\mathbf{X}^*)\right].
 </annotation></semantics></math></p>
<p>A closely related error is the <strong>insample error</strong>.</p>
<hr />
</div>
<div id="insample-error" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Insample Error</h2>
<p>The insample error for prediction model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{m}(\mathbf{x})</annotation></semantics></math> is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">Err</mtext><mrow><mtext mathvariant="normal">in</mtext><mi>𝒯</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mtext mathvariant="normal">Err</mtext><mi>𝒯</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">
   \text{Err}_{\text{in} \mathcal{T}} = \frac{1}{n}\sum_{i=1}^n \text{Err}_{\mathcal{T}}(\mathbf{x}_i),
 </annotation></semantics></math></p>
<p>i.e. the insample error is the sample average of the conditional test errors evaluated in the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> training dataset predictors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐱</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\mathbf{x}_i</annotation></semantics></math>.</p>
<p>Since <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mtext mathvariant="normal">Err</mtext><mi>𝒯</mi></msub><annotation encoding="application/x-tex">\text{Err}_{\mathcal{T}}</annotation></semantics></math> is an average over all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>, even over those predictors not observed in the training dataset, it is sometimes referred to as the <strong>outsample error</strong>.</p>
<hr />
</div>
<div id="estimation-of-the-insample-error" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> Estimation of the insample error</h2>
<p>We start with introducing the training error rate, which is closely related to the MSE in linear models.</p>
<div id="training-error" class="section level3" number="7.4.1">
<h3><span class="header-section-number">7.4.1</span> Training error</h3>
<p>The training error is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mtext mathvariant="normal">err</mtext><mo accent="true">¯</mo></mover><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Y</mi><mi>i</mi></msub><mo>−</mo><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">
   \overline{\text{err}} = \frac{1}{n}\sum_{i=1}^n (Y_i - \hat{m}(\mathbf{x}_i))^2 ,
 </annotation></semantics></math>
where the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Y</mi><mi>i</mi></msub><mo>,</mo><msub><mi>𝐱</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(Y_i,\mathbf{x}_i)</annotation></semantics></math> from the training dataset which is also used for the calculation of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>m</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{m}</annotation></semantics></math>.</p>
<ul>
<li><p>The training error is an overly optimistic estimate of the test error <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mtext mathvariant="normal">Err</mtext><mi>𝒯</mi></msub><annotation encoding="application/x-tex">\text{Err}_{\mathcal{T}}</annotation></semantics></math>.</p></li>
<li><p>The training error will never increases when the model becomes more complex. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>→</mo><annotation encoding="application/x-tex">\longrightarrow</annotation></semantics></math> cannot be used directly as a model selection criterion.</p></li>
</ul>
<p>Indeed, model parameters are often estimated by minimising the training error (cfr. SSE).</p>
<ul>
<li>Hence the fitted model adapts to the training data, and</li>
<li>training error will be an overly optimistic estimate of the test error <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mtext mathvariant="normal">Err</mtext><mi>𝒯</mi></msub><annotation encoding="application/x-tex">\text{Err}_{\mathcal{T}}</annotation></semantics></math>.</li>
</ul>
<hr />
<p>It can be shown that the training error is related to the insample test error via</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">E</mtext><mi>𝐘</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><msub><mtext mathvariant="normal">Err</mtext><mrow><mtext mathvariant="normal">in</mtext><mi>𝒯</mi></mrow></msub><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><msub><mtext mathvariant="normal">E</mtext><mi>𝐘</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><mover><mtext mathvariant="normal">err</mtext><mo accent="true">¯</mo></mover><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mfrac><mn>2</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mtext mathvariant="normal">cov</mtext><mi>𝐘</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">]</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">
\text{E}_\mathbf{Y}
\left[\text{Err}_{\text{in}{\mathcal{T}}}\right] = \text{E}_\mathbf{Y}\left[\overline{\text{err}}\right] + \frac{2}{n}\sum_{i=1}^n \text{cov}_\mathbf{Y}\left[\hat{m}(\mathbf{x}_i),Y_i\right],
</annotation></semantics></math></p>
<p>Note, that for linear models
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝐗</mi><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo>=</mo><mi>𝐗</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mi>T</mi></msup><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><msup><mi>𝐗</mi><mi>T</mi></msup><mi>𝐘</mi><mo>=</mo><mrow><mi>𝐇</mi><mi>𝐘</mi></mrow></mrow><annotation encoding="application/x-tex"> \hat{m}(\mathbf{x}_i) = \mathbf{X}\hat{\boldsymbol{\beta}}= \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} = \mathbf{HY}
</annotation></semantics></math>
with</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐇</mi><annotation encoding="application/x-tex">\mathbf{H}</annotation></semantics></math> the hat matrix and</li>
<li>all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">Y_i</annotation></semantics></math> are assumed to be independently distributed <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐗</mi><mi>𝛃</mi><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">N(\mathbf{X}\boldsymbol{\beta},\sigma^2)</annotation></semantics></math></li>
</ul>
<p>Hence, for linear models with independent observations</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mtext mathvariant="normal">cov</mtext><mi>𝐘</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow></mtd><mtd columnalign="center" style="text-align: center"><mo>=</mo></mtd><mtd columnalign="left" style="text-align: left"><msub><mtext mathvariant="normal">cov</mtext><mi>𝐘</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><msubsup><mi>𝐇</mi><mi>i</mi><mi>T</mi></msubsup><mi>𝐘</mi><mo>,</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="center" style="text-align: center"><mo>=</mo></mtd><mtd columnalign="left" style="text-align: left"><msub><mtext mathvariant="normal">cov</mtext><mi>𝐘</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>h</mi><mrow><mi>i</mi><mi>i</mi></mrow></msub><msub><mi>Y</mi><mi>i</mi></msub><mo>,</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="center" style="text-align: center"><mo>=</mo></mtd><mtd columnalign="left" style="text-align: left"><msub><mi>h</mi><mrow><mi>i</mi><mi>i</mi></mrow></msub><msub><mtext mathvariant="normal">cov</mtext><mi>𝐘</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>Y</mi><mi>i</mi></msub><mo>,</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="center" style="text-align: center"><mo>=</mo></mtd><mtd columnalign="left" style="text-align: left"><msub><mi>h</mi><mrow><mi>i</mi><mi>i</mi></mrow></msub><msup><mi>σ</mi><mn>2</mn></msup></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{eqnarray}
\text{cov}_\mathbf{Y}\left[\hat{m}(\mathbf{x}_i),Y_i)\right] &amp;=&amp;
\text{cov}_\mathbf{Y}\left[\mathbf{H}_{i}^T\mathbf{Y},Y_i)\right]\\
&amp;=&amp; \text{cov}_\mathbf{Y}\left[h_{ii} Y_i,Y_i\right]\\
&amp;=&amp; h_{ii} \text{cov}_\mathbf{Y}\left[Y_i,Y_i\right]\\
&amp;=&amp; h_{ii} \sigma^2\\
\end{eqnarray}</annotation></semantics></math></p>
<p>And we can thus estimate the insample error by Mallow’s <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>C</mi><mi>p</mi></msub><annotation encoding="application/x-tex">C_p</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>C</mi><mi>p</mi></msub></mtd><mtd columnalign="center" style="text-align: center"><mo>=</mo></mtd><mtd columnalign="left" style="text-align: left"><mover><mtext mathvariant="normal">err</mtext><mo accent="true">¯</mo></mover><mo>+</mo><mfrac><mrow><mn>2</mn><msup><mi>σ</mi><mn>2</mn></msup></mrow><mi>n</mi></mfrac><mtext mathvariant="normal">tr</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐇</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="center" style="text-align: center"><mo>=</mo></mtd><mtd columnalign="left" style="text-align: left"><mover><mtext mathvariant="normal">err</mtext><mo accent="true">¯</mo></mover><mo>+</mo><mfrac><mrow><mn>2</mn><msup><mi>σ</mi><mn>2</mn></msup><mi>p</mi></mrow><mi>n</mi></mfrac></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{eqnarray}
C_p &amp;=&amp; \overline{\text{err}} + \frac{2\sigma^2}{n}\text{tr}(\mathbf{H})\\
&amp;=&amp; \overline{\text{err}} + \frac{2\sigma^2p}{n}
\end{eqnarray}</annotation></semantics></math></p>
<p>with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> the number of predictors.</p>
<ul>
<li>Mallow’s <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>C</mi><mi>p</mi></msub><annotation encoding="application/x-tex">C_p</annotation></semantics></math> is often used for model selection.</li>
<li>Note, that we can also consider it as a kind of penalized least squares:</li>
</ul>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><msub><mi>C</mi><mi>p</mi></msub><mo>=</mo><mo stretchy="false" form="postfix">‖</mo><mi>𝐘</mi><mo>−</mo><mi>𝐗</mi><mi>𝛃</mi><msubsup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mn>2</mn><msup><mi>σ</mi><mn>2</mn></msup><mo stretchy="false" form="postfix">‖</mo><mi>𝛃</mi><msub><mo stretchy="false" form="postfix">‖</mo><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">
n \times C_p = \Vert \mathbf{Y} - \mathbf{X}\boldsymbol{\beta}\Vert_2^2 + 2\sigma^2 \Vert \boldsymbol{\beta} \Vert_0
</annotation></semantics></math>
with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mn>0</mn></msub><annotation encoding="application/x-tex">L_0</annotation></semantics></math> norm <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">‖</mo><mi>𝛃</mi><msub><mo stretchy="false" form="postfix">‖</mo><mn>0</mn></msub><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></msubsup><msubsup><mi>β</mi><mi>p</mi><mn>0</mn></msubsup><mo>=</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">\Vert \boldsymbol{\beta} \Vert_0 = \sum_{j=1}^p \beta_p^0 = p</annotation></semantics></math>.</p>
<hr />
</div>
</div>
<div id="expected-test-error" class="section level2" number="7.5">
<h2><span class="header-section-number">7.5</span> Expected test error</h2>
<p>The test or generalisation error was defined conditionally on the training data. By averaging over the distribution of training datasets, the expected test error arises.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mtext mathvariant="normal">E</mtext><mi>𝒯</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><msub><mtext mathvariant="normal">Err</mtext><mi>𝒯</mi></msub><mo stretchy="true" form="postfix">]</mo></mrow></mtd><mtd columnalign="center" style="text-align: center"><mo>=</mo></mtd><mtd columnalign="left" style="text-align: left"><msub><mtext mathvariant="normal">E</mtext><mi>𝒯</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><msub><mtext mathvariant="normal">E</mtext><mrow><msup><mi>Y</mi><mo>*</mo></msup><mo>,</mo><msup><mi>X</mi><mo>*</mo></msup></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msup><mi>Y</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>∣</mo><mi>𝒯</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="center" style="text-align: center"><mo>=</mo></mtd><mtd columnalign="left" style="text-align: left"><msub><mtext mathvariant="normal">E</mtext><mrow><msup><mi>Y</mi><mo>*</mo></msup><mo>,</mo><msup><mi>X</mi><mo>*</mo></msup><mo>,</mo><mi>𝒯</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msup><mi>Y</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{eqnarray*}
   \text{E}_{\mathcal{T}}\left[\text{Err}_{{\mathcal{T}}}\right]
     &amp;=&amp; \text{E}_{\mathcal{T}}\left[\text{E}_{Y^*,X^*}\left[(\hat{m}(\mathbf{X}^*) - Y^*)^2\mid {\mathcal{T}}\right]\right] \\
     &amp;=&amp; \text{E}_{Y^*,X^*,{\mathcal{T}}}\left[(\hat{m}(\mathbf{X}^*) - Y^*)^2\right].
 \end{eqnarray*}</annotation></semantics></math></p>
<ul>
<li><p>The expected test error may not be of direct interest when the goal is to assess the prediction performance of a single prediction model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>⋅</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{m}(\cdot)</annotation></semantics></math>.</p></li>
<li><p>The expected test error averages the test errors of all models that can be build from all training datasets, and hence this may be less relevant when the interest is in evaluating one particular model that resulted from a single observed training dataset.</p></li>
<li><p>Also note that building a prediction model involves both parameter estimation and feature selection.</p></li>
<li><p>Hence the expected test error also evaluates the feature selection procedure (on average).</p></li>
<li><p>If the expected test error is small, it is an indication that the model building process gives good predictions for future observations <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Y</mi><mo>*</mo></msup><mo>,</mo><msup><mi>𝐗</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(Y^*,\mathbf{X}^*)</annotation></semantics></math> on average.</p></li>
</ul>
<div id="estimating-the-expected-test-error" class="section level3" number="7.5.1">
<h3><span class="header-section-number">7.5.1</span> Estimating the Expected test error</h3>
<p>The expected test error may be estimated by cross validation (CV).</p>
<div id="leave-one-out-cross-validation-loocv" class="section level4" number="7.5.1.1">
<h4><span class="header-section-number">7.5.1.1</span> Leave one out cross validation (LOOCV)}</h4>
<p>The LOOCV estimator of the expected test error (or expected outsample error) is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">CV</mtext><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Y</mi><mi>i</mi></msub><mo>−</mo><msup><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mi>−</mi><mi>i</mi></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">
     \text{CV} = \frac{1}{n} \sum_{i=1}^n \left(Y_i - \hat{m}^{-i}(\mathbf{x}_i)\right)^2 ,
  </annotation></semantics></math>
where</p>
<ul>
<li>the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Y</mi><mi>i</mi></msub><mo>,</mo><msub><mi>𝐱</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(Y_i,\mathbf{x}_i)</annotation></semantics></math> form the training dataset</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mi>−</mi><mi>i</mi></mrow></msup><annotation encoding="application/x-tex">\hat{m}^{-i}</annotation></semantics></math> is the fitted model based on all training data, except observation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mi>−</mi><mi>i</mi></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{m}^{-i}(\mathbf{x}_i)</annotation></semantics></math> is the prediction at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐱</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\mathbf{x}_i</annotation></semantics></math>, which is the observation left out the training data before building model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>.</li>
</ul>
<p>Some rationale as to why LOOCV offers a good estimator of the outsample error:</p>
<ul>
<li><p>the prediction error <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Y</mi><mo>*</mo></msup><mo>−</mo><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">Y^*-\hat{m}(\mathbf{x})</annotation></semantics></math> is mimicked by not using one of the training outcomes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">Y_i</annotation></semantics></math> for the estimation of the model so that this <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">Y_i</annotation></semantics></math> plays the role of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Y</mi><mo>*</mo></msup><annotation encoding="application/x-tex">Y^*</annotation></semantics></math>, and, consequently, the fitted model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mi>−</mi><mi>i</mi></mrow></msup><annotation encoding="application/x-tex">\hat{m}^{-i}</annotation></semantics></math> is independent of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">Y_i</annotation></semantics></math></p></li>
<li><p>the sum in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>V</mi></mrow><annotation encoding="application/x-tex">CV</annotation></semantics></math> is over all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐱</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\mathbf{x}_i</annotation></semantics></math> in the training dataset, but each term <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐱</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\mathbf{x}_i</annotation></semantics></math> was left out once for the calculation of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mi>−</mi><mi>i</mi></mrow></msup><annotation encoding="application/x-tex">\hat{m}^{-i}</annotation></semantics></math>. Hence, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mi>−</mi><mi>i</mi></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{m}^{-i}(\mathbf{x}_i)</annotation></semantics></math> mimics an outsample prediction.</p></li>
<li><p>the sum in CV is over <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> different training datasets (each one with a different observation removed), and hence CV is an estimator of the <em>expected</em> test error.</p></li>
<li><p>For linear models the LOOCV can be readily obtained from the fitted model: i.e.</p></li>
</ul>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">CV</mtext><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mfrac><msubsup><mi>e</mi><mi>i</mi><mn>2</mn></msubsup><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><msub><mi>h</mi><mrow><mi>i</mi><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mfrac></mrow><annotation encoding="application/x-tex">\text{CV} = \frac{1}{n}\sum\limits_{i=1}^n \frac{e_i^2}{(1-h_{ii})^2}</annotation></semantics></math></p>
<p>with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>e</mi><mi>i</mi></msub><annotation encoding="application/x-tex">e_i</annotation></semantics></math> the residuals from the model that is fitted based on all training data.</p>
<hr />
<p>An alternative to LOOCV is the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-fold cross validation procedure. It also gives an estimate of the expected outsample error.</p>
</div>
<div id="k-fold-cross-validation" class="section level4" number="7.5.1.2">
<h4><span class="header-section-number">7.5.1.2</span> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-fold cross validation</h4>
<ul>
<li><p>Randomly divide the training dataset into <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> approximately equal subsets . Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mi>j</mi></msub><annotation encoding="application/x-tex">S_j</annotation></semantics></math> denote the index set of the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>th subset (referred to as a <strong>fold</strong>). Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mi>j</mi></msub><annotation encoding="application/x-tex">n_j</annotation></semantics></math> denote the number of observations in fold <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>.</p></li>
<li><p>The <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-fold cross validation estimator of the expected outsample error is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">CV</mtext><mi>k</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>k</mi></mfrac><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mfrac><mn>1</mn><msub><mi>n</mi><mi>j</mi></msub></mfrac><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><msub><mi>S</mi><mi>j</mi></msub></mrow></munder><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Y</mi><mi>i</mi></msub><mo>−</mo><msup><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mi>−</mi><msub><mi>S</mi><mi>j</mi></msub></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">
   \text{CV}_k = \frac{1}{k}\sum_{j=1}^k \frac{1}{n_j} \sum_{i\in S_j} \left(Y_i - \hat{m}^{-S_j}(\mathbf{x}_i)\right)^2
 </annotation></semantics></math>
where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mi>−</mi><msub><mi>S</mi><mi>j</mi></msub></mrow></msup><annotation encoding="application/x-tex">\hat{m}^{-S_j}</annotation></semantics></math> is the model fitted using all training data, except observations in fold <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> (i.e. observations <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>∈</mo><msub><mi>S</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">i \in S_j</annotation></semantics></math>).</p></li>
</ul>
<hr />
<p>The cross validation estimators of the expected outsample error are nearly unbiased. One argument that helps to understand where the bias comes from is the fact that e.g. in de LOOCV estimator the model is fit on only <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n-1</annotation></semantics></math> observations, whereas we are aiming at estimating the outsample error of a model fit on all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> training observations. Fortunately, the bias is often small and is in practice hardly a concern.</p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-fold CV is computationally more complex.</p>
<p>Since CV and CV<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi></mi><mi>k</mi></msub><annotation encoding="application/x-tex">_k</annotation></semantics></math> are estimators, they also show sampling variability. Standard errors of the CV or CV<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi></mi><mi>k</mi></msub><annotation encoding="application/x-tex">_k</annotation></semantics></math> can be computed. We don’t show the details, but in the example this is illustrated.</p>
</div>
</div>
<div id="bias-variance-trade-off" class="section level3" number="7.5.2">
<h3><span class="header-section-number">7.5.2</span> Bias Variance trade-off</h3>
<p>For the expected conditional test error in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math>, it holds that
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mtext mathvariant="normal">E</mtext><mi>𝒯</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><msub><mtext mathvariant="normal">Err</mtext><mi>𝒯</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mtd><mtd columnalign="center" style="text-align: center"><mo>=</mo></mtd><mtd columnalign="left" style="text-align: left"><msub><mtext mathvariant="normal">E</mtext><mrow><msup><mi>Y</mi><mo>*</mo></msup><mo>,</mo><mi>𝒯</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msup><mi>Y</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>∣</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="center" style="text-align: center"><mo>=</mo></mtd><mtd columnalign="left" style="text-align: left"><msub><mtext mathvariant="normal">var</mtext><mi>𝐘</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><mover><mi>Y</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∣</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msup><mi>μ</mi><mo>*</mo></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>+</mo><msub><mtext mathvariant="normal">var</mtext><msup><mi>Y</mi><mo>*</mo></msup></msub><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>Y</mi><mo>*</mo></msup><mo>∣</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{eqnarray*}
  \text{E}_{\mathcal{T}}\left[\text{Err}_{\mathcal{T}}(\mathbf{x})\right]
    &amp;=&amp; \text{E}_{Y^*,{\mathcal{T}}}\left[(\hat{m}(\mathbf{x})-Y^*)^2 \mid \mathbf{x}\right] \\
    &amp;=&amp;  \text{var}_{\mathbf{Y}}\left[\hat{Y}(\mathbf{x})\mid \mathbf{x}\right] +(\mu(\mathbf{x})-\mu^*(\mathbf{x}))^2+\text{var}_{Y^*}\left[Y^*\mid \mathbf{x}\right]
\end{eqnarray*}</annotation></semantics></math>
where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mtext mathvariant="normal">E</mtext><mi>𝐘</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><mover><mi>Y</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∣</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> and </mtext><mspace width="0.333em"></mspace></mrow><msup><mi>μ</mi><mo>*</mo></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mtext mathvariant="normal">E</mtext><msup><mi>Y</mi><mo>*</mo></msup></msub><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>Y</mi><mo>*</mo></msup><mo>∣</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mu(\mathbf{x}) = \text{E}_{\mathbf{Y}}\left[\hat{Y}(\mathbf{x})\mid \mathbf{x}\right] \text{ and } \mu^*(\mathbf{x})=\text{E}_{Y^*}\left[Y^*\mid \mathbf{x}\right]</annotation></semantics></math>.</p>
<ul>
<li><p><strong>bias</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">bias</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>μ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msup><mi>μ</mi><mo>*</mo></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{bias}(\mathbf{x})=\mu(\mathbf{x})-\mu^*(\mathbf{x})</annotation></semantics></math></p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">var</mtext><msup><mi>Y</mi><mo>*</mo></msup></msub><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>Y</mi><mo>*</mo></msup><mo>∣</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\text{var}_{Y^*}\left[Y^*\mid \mathbf{x}\right]</annotation></semantics></math> does not depend on the model, and is referred to as the <strong>irreducible variance</strong>.</p></li>
</ul>
<hr />
<p>The importance of the bias-variance trade-off can be seen from a model selection perspective. When we agree that a good model is a model that has a small expected conditional test error at some point <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math>, then the bias-variance trade-off shows us that a model may be biased as long as it has a small variance to compensate for the bias. It often happens that a biased model has a substantial smaller variance. When these two are combined, a small expected test error may occur.</p>
<p>Also note that the model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> which forms the basis of the prediction model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{m}(\mathbf{x})</annotation></semantics></math> does NOT need to satisfy <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>μ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">m(\mathbf{x})=\mu(\mathbf{x})</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>μ</mi><mo>*</mo></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">m(\mathbf{x})=\mu^*(\mathbf{x})</annotation></semantics></math>. The model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> is known by the data-analyst (its the basis of the prediction model), whereas <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mu(\mathbf{x})</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>μ</mi><mo>*</mo></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mu^*(\mathbf{x})</annotation></semantics></math> are generally unknown to the data-analyst. We only hope that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> serves well as a prediction model.</p>
<hr />
</div>
<div id="in-practice" class="section level3" number="7.5.3">
<h3><span class="header-section-number">7.5.3</span> In practice</h3>
<p>We use cross validation to estimate the lambda penalty for penalised regression:</p>
<ul>
<li>Ridge Regression</li>
<li>Lasso</li>
<li>Build models, e.g. select the number of PCs for PCA regression</li>
<li>Splines</li>
</ul>
</div>
<div id="toxicogenomics-example-2" class="section level3" number="7.5.4">
<h3><span class="header-section-number">7.5.4</span> Toxicogenomics example</h3>
<div id="lasso" class="section level4" number="7.5.4.1">
<h4><span class="header-section-number">7.5.4.1</span> Lasso</h4>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">15</span>)</span>
<span id="cb32-2"><a href="#cb32-2" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb32-3"><a href="#cb32-3" tabindex="-1"></a>mCvLasso <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(</span>
<span id="cb32-4"><a href="#cb32-4" tabindex="-1"></a>  <span class="at">x =</span> toxData[,<span class="sc">-</span><span class="dv">1</span>] <span class="sc">%&gt;%</span></span>
<span id="cb32-5"><a href="#cb32-5" tabindex="-1"></a>    as.matrix,</span>
<span id="cb32-6"><a href="#cb32-6" tabindex="-1"></a>  <span class="at">y =</span> toxData <span class="sc">%&gt;%</span></span>
<span id="cb32-7"><a href="#cb32-7" tabindex="-1"></a>    <span class="fu">pull</span>(BA),</span>
<span id="cb32-8"><a href="#cb32-8" tabindex="-1"></a>  <span class="at">alpha =</span> <span class="dv">1</span>)  <span class="co"># lasso alpha=1</span></span>
<span id="cb32-9"><a href="#cb32-9" tabindex="-1"></a></span>
<span id="cb32-10"><a href="#cb32-10" tabindex="-1"></a><span class="fu">plot</span>(mCvLasso)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Default CV procedure in is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">k=10</annotation></semantics></math>-fold CV.</p>
<p>The Graphs shows</p>
<ul>
<li>10-fold CV estimates of the extra-sample error as a function of the lasso penalty parameter <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>.</li>
<li>estimate plus and minus once the estimated standard error of the CV estimate (grey bars)</li>
<li>On top the number of non-zero regression parameter estimates are shown.</li>
</ul>
<p>Two vertical reference lines are added to the graph. They correspond to</p>
<ul>
<li>the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log(\lambda)</annotation></semantics></math> that gives the smallest CV estimate of the extra-sample error, and</li>
<li>the largest <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log(\lambda)</annotation></semantics></math> that gives a CV estimate of the extra-sample error that is within one standard error from the smallest error estimate.</li>
<li>The latter choice of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> has no firm theoretical basis, except that it somehow accounts for the imprecision of the error estimate. One could loosely say that this <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math> corresponds to the smallest model (i.e. least number of predictors) that gives an error that is within margin of error of the error of the best model.</li>
</ul>
<hr />
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" tabindex="-1"></a>mLassoOpt <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(</span>
<span id="cb33-2"><a href="#cb33-2" tabindex="-1"></a>  <span class="at">x =</span> toxData[,<span class="sc">-</span><span class="dv">1</span>] <span class="sc">%&gt;%</span></span>
<span id="cb33-3"><a href="#cb33-3" tabindex="-1"></a>    as.matrix,</span>
<span id="cb33-4"><a href="#cb33-4" tabindex="-1"></a>  <span class="at">y =</span> toxData <span class="sc">%&gt;%</span></span>
<span id="cb33-5"><a href="#cb33-5" tabindex="-1"></a>    <span class="fu">pull</span>(BA),</span>
<span id="cb33-6"><a href="#cb33-6" tabindex="-1"></a>    <span class="at">alpha =</span> <span class="dv">1</span>,</span>
<span id="cb33-7"><a href="#cb33-7" tabindex="-1"></a>    <span class="at">lambda =</span> mCvLasso<span class="sc">$</span>lambda.min)</span>
<span id="cb33-8"><a href="#cb33-8" tabindex="-1"></a></span>
<span id="cb33-9"><a href="#cb33-9" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">coef</span>(mLassoOpt))</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["i"],"name":[1],"type":["int"],"align":["right"]},{"label":["j"],"name":[2],"type":["int"],"align":["right"]},{"label":["x"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"1","2":"1","3":"2.135417e-17"},{"1":"7","2":"1","3":"7.723665e-01"},{"1":"105","2":"1","3":"6.745830e-01"},{"1":"147","2":"1","3":"-7.479963e-01"},{"1":"420","2":"1","3":"1.275535e+00"},{"1":"453","2":"1","3":"4.272541e-02"},{"1":"1720","2":"1","3":"-4.548090e-01"},{"1":"1952","2":"1","3":"3.651340e-01"},{"1":"2032","2":"1","3":"4.115143e-16"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>With the optimal <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> (smallest error estimate) the output shows the 9 non-zero estimated regression coefficients (sparse solution).</p>
<hr />
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" tabindex="-1"></a>mLasso1se <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(</span>
<span id="cb34-2"><a href="#cb34-2" tabindex="-1"></a>  <span class="at">x =</span> toxData[,<span class="sc">-</span><span class="dv">1</span>] <span class="sc">%&gt;%</span></span>
<span id="cb34-3"><a href="#cb34-3" tabindex="-1"></a>    as.matrix,</span>
<span id="cb34-4"><a href="#cb34-4" tabindex="-1"></a>    <span class="at">y=</span> toxData <span class="sc">%&gt;%</span></span>
<span id="cb34-5"><a href="#cb34-5" tabindex="-1"></a>      <span class="fu">pull</span>(BA),</span>
<span id="cb34-6"><a href="#cb34-6" tabindex="-1"></a>    <span class="at">alpha =</span> <span class="dv">1</span>,</span>
<span id="cb34-7"><a href="#cb34-7" tabindex="-1"></a>    <span class="at">lambda =</span> mCvLasso<span class="sc">$</span>lambda<span class="fl">.1</span>se)</span>
<span id="cb34-8"><a href="#cb34-8" tabindex="-1"></a></span>
<span id="cb34-9"><a href="#cb34-9" tabindex="-1"></a>mLasso1se <span class="sc">%&gt;%</span></span>
<span id="cb34-10"><a href="#cb34-10" tabindex="-1"></a>  coef <span class="sc">%&gt;%</span></span>
<span id="cb34-11"><a href="#cb34-11" tabindex="-1"></a>  summary</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["i"],"name":[1],"type":["int"],"align":["right"]},{"label":["j"],"name":[2],"type":["int"],"align":["right"]},{"label":["x"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"1","2":"1","3":"3.608536e-18"},{"1":"7","2":"1","3":"6.255918e-01"},{"1":"147","2":"1","3":"-1.767770e-02"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>This shows the solution for the largest <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> within one standard error of the optimal model. Now only 3 non-zero estimates result.</p>
<hr />
</div>
<div id="ridge" class="section level4" number="7.5.4.2">
<h4><span class="header-section-number">7.5.4.2</span> Ridge</h4>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" tabindex="-1"></a>mCvRidge <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(</span>
<span id="cb35-2"><a href="#cb35-2" tabindex="-1"></a>  <span class="at">x =</span> toxData[,<span class="sc">-</span><span class="dv">1</span>] <span class="sc">%&gt;%</span></span>
<span id="cb35-3"><a href="#cb35-3" tabindex="-1"></a>    as.matrix,</span>
<span id="cb35-4"><a href="#cb35-4" tabindex="-1"></a>    <span class="at">y =</span> toxData <span class="sc">%&gt;%</span></span>
<span id="cb35-5"><a href="#cb35-5" tabindex="-1"></a>      <span class="fu">pull</span>(BA),</span>
<span id="cb35-6"><a href="#cb35-6" tabindex="-1"></a>      <span class="at">alpha =</span> <span class="dv">0</span>)  <span class="co"># ridge alpha=0</span></span>
<span id="cb35-7"><a href="#cb35-7" tabindex="-1"></a></span>
<span id="cb35-8"><a href="#cb35-8" tabindex="-1"></a><span class="fu">plot</span>(mCvRidge)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<ul>
<li>Ridge does not seem to have optimal solution.</li>
<li>10-fold CV is also larger than for lasso.</li>
</ul>
<hr />
</div>
<div id="pca-regression" class="section level4" number="7.5.4.3">
<h4><span class="header-section-number">7.5.4.3</span> PCA regression</h4>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1264</span>)</span>
<span id="cb36-2"><a href="#cb36-2" tabindex="-1"></a><span class="fu">library</span>(DAAG)</span>
<span id="cb36-3"><a href="#cb36-3" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" tabindex="-1"></a>tox <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb36-5"><a href="#cb36-5" tabindex="-1"></a>  <span class="at">Y =</span> toxData <span class="sc">%&gt;%</span></span>
<span id="cb36-6"><a href="#cb36-6" tabindex="-1"></a>    <span class="fu">pull</span>(BA),</span>
<span id="cb36-7"><a href="#cb36-7" tabindex="-1"></a>  <span class="at">PC =</span> Zk)</span>
<span id="cb36-8"><a href="#cb36-8" tabindex="-1"></a></span>
<span id="cb36-9"><a href="#cb36-9" tabindex="-1"></a>PC.seq <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">25</span></span>
<span id="cb36-10"><a href="#cb36-10" tabindex="-1"></a>Err <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="dv">25</span>)</span>
<span id="cb36-11"><a href="#cb36-11" tabindex="-1"></a></span>
<span id="cb36-12"><a href="#cb36-12" tabindex="-1"></a>mCvPca <span class="ot">&lt;-</span> <span class="fu">cv.lm</span>(</span>
<span id="cb36-13"><a href="#cb36-13" tabindex="-1"></a>  Y<span class="sc">~</span>PC<span class="fl">.1</span>,</span>
<span id="cb36-14"><a href="#cb36-14" tabindex="-1"></a>  <span class="at">data =</span> tox,</span>
<span id="cb36-15"><a href="#cb36-15" tabindex="-1"></a>  <span class="at">m =</span> <span class="dv">5</span>,</span>
<span id="cb36-16"><a href="#cb36-16" tabindex="-1"></a>  <span class="at">printit =</span> <span class="cn">FALSE</span>)</span>
<span id="cb36-17"><a href="#cb36-17" tabindex="-1"></a></span>
<span id="cb36-18"><a href="#cb36-18" tabindex="-1"></a>Err[<span class="dv">1</span>]<span class="ot">&lt;-</span><span class="fu">attr</span>(mCvPca,<span class="st">&quot;ms&quot;</span>)</span>
<span id="cb36-19"><a href="#cb36-19" tabindex="-1"></a></span>
<span id="cb36-20"><a href="#cb36-20" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">25</span>) {</span>
<span id="cb36-21"><a href="#cb36-21" tabindex="-1"></a>  mCvPca <span class="ot">&lt;-</span> <span class="fu">cv.lm</span>(</span>
<span id="cb36-22"><a href="#cb36-22" tabindex="-1"></a>    <span class="fu">as.formula</span>(</span>
<span id="cb36-23"><a href="#cb36-23" tabindex="-1"></a>      <span class="fu">paste</span>(<span class="st">&quot;Y ~ PC.1 + &quot;</span>,</span>
<span id="cb36-24"><a href="#cb36-24" tabindex="-1"></a>        <span class="fu">paste</span>(<span class="st">&quot;PC.&quot;</span>, <span class="dv">2</span><span class="sc">:</span>i, <span class="at">collapse =</span> <span class="st">&quot;+&quot;</span>, <span class="at">sep=</span><span class="st">&quot;&quot;</span>),</span>
<span id="cb36-25"><a href="#cb36-25" tabindex="-1"></a>        <span class="at">sep=</span><span class="st">&quot;&quot;</span></span>
<span id="cb36-26"><a href="#cb36-26" tabindex="-1"></a>      )</span>
<span id="cb36-27"><a href="#cb36-27" tabindex="-1"></a>    ),</span>
<span id="cb36-28"><a href="#cb36-28" tabindex="-1"></a>    <span class="at">data =</span> tox,</span>
<span id="cb36-29"><a href="#cb36-29" tabindex="-1"></a>    <span class="at">m =</span> <span class="dv">5</span>,</span>
<span id="cb36-30"><a href="#cb36-30" tabindex="-1"></a>    <span class="at">printit =</span> <span class="cn">FALSE</span>)</span>
<span id="cb36-31"><a href="#cb36-31" tabindex="-1"></a>  Err[i]<span class="ot">&lt;-</span><span class="fu">attr</span>(mCvPca,<span class="st">&quot;ms&quot;</span>)</span>
<span id="cb36-32"><a href="#cb36-32" tabindex="-1"></a>}</span></code></pre></div>
<ul>
<li><p>Here we illustrate principal component regression.</p></li>
<li><p>The most important PCs are selected in a forward model selection procedure.</p></li>
<li><p>Within the model selection procedure the models are evaluated with 5-fold CV estimates of the outsample error.</p></li>
<li><p>It is important to realise that a forward model selection procedure will not necessarily result in the best prediction model, particularly because the order of the PCs is generally not related to the importance of the PCs for predicting the outcome.</p></li>
<li><p>A supervised PC would be better.</p></li>
</ul>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" tabindex="-1"></a>pPCreg <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(PC.seq, Err) <span class="sc">%&gt;%</span></span>
<span id="cb37-2"><a href="#cb37-2" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> PC.seq, <span class="at">y =</span> Err)) <span class="sc">+</span></span>
<span id="cb37-3"><a href="#cb37-3" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb37-4"><a href="#cb37-4" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb37-5"><a href="#cb37-5" tabindex="-1"></a>  <span class="fu">geom_hline</span>(</span>
<span id="cb37-6"><a href="#cb37-6" tabindex="-1"></a>    <span class="at">yintercept =</span> <span class="fu">c</span>(</span>
<span id="cb37-7"><a href="#cb37-7" tabindex="-1"></a>      mCvLasso<span class="sc">$</span>cvm[mCvLasso<span class="sc">$</span>lambda<span class="sc">==</span>mCvLasso<span class="sc">$</span>lambda.min],</span>
<span id="cb37-8"><a href="#cb37-8" tabindex="-1"></a>      mCvLasso<span class="sc">$</span>cvm[mCvLasso<span class="sc">$</span>lambda<span class="sc">==</span>mCvLasso<span class="sc">$</span>lambda<span class="fl">.1</span>se]),</span>
<span id="cb37-9"><a href="#cb37-9" tabindex="-1"></a>    <span class="at">col =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb37-10"><a href="#cb37-10" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">1</span>,<span class="dv">26</span>)</span>
<span id="cb37-11"><a href="#cb37-11" tabindex="-1"></a></span>
<span id="cb37-12"><a href="#cb37-12" tabindex="-1"></a><span class="fu">grid.arrange</span>(</span>
<span id="cb37-13"><a href="#cb37-13" tabindex="-1"></a>  pPCreg,</span>
<span id="cb37-14"><a href="#cb37-14" tabindex="-1"></a>  pPCreg <span class="sc">+</span> <span class="fu">ylim</span>(<span class="dv">0</span>,<span class="dv">5</span>),</span>
<span id="cb37-15"><a href="#cb37-15" tabindex="-1"></a>  <span class="at">ncol=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<ul>
<li><p>The graph shows the CV estimate of the outsample error as a function of the number of sparse PCs included in the model.</p></li>
<li><p>A very small error is obtained with the model with only the first PC. The best model with 3 PCs.</p></li>
<li><p>The two vertical reference lines correspond to the error estimates obtained with lasso (optimal <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> and largest <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> within one standard error).</p></li>
<li><p>Thus although there was a priori no guarantee that the first PCs are the most predictive, it seems to be the case here (we were lucky!).</p></li>
<li><p>Moreover, the first PC resulted in a small outsample error.</p></li>
<li><p>Note that the graph does not indicate the variability of the error estimates (no error bars).</p></li>
<li><p>Also note that the graph clearly illustrates the effect of overfitting: including too many PCs causes a large outsample error.</p></li>
</ul>
</div>
</div>
<div id="lidar-example-splines" class="section level3" number="7.5.5">
<h3><span class="header-section-number">7.5.5</span> Lidar Example: splines</h3>
<ul>
<li>We use the mgcv package to fit the spline model to the lidar data.</li>
<li>A better basis is used than the truncated spline basis</li>
<li>Thin plate splines are also linear smoothers, i.e.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>Y</mi><mo accent="true">̂</mo></mover><mo>=</mo><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mi>𝐒</mi><mi>𝐘</mi></mrow></mrow><annotation encoding="application/x-tex">\hat{Y} = \hat{m}(\mathbf{X}) = \mathbf{SY}</annotation></semantics></math></li>
<li>So their variance can be easily calculated.</li>
<li>The ridge/smoothness penalty is chosen by generalized cross validation.</li>
</ul>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" tabindex="-1"></a><span class="fu">library</span>(mgcv)</span>
<span id="cb38-2"><a href="#cb38-2" tabindex="-1"></a>gamfit <span class="ot">&lt;-</span> <span class="fu">gam</span>(logratio <span class="sc">~</span> <span class="fu">s</span>(range), <span class="at">data =</span> lidar)</span>
<span id="cb38-3"><a href="#cb38-3" tabindex="-1"></a>gamfit<span class="sc">$</span>sp</span></code></pre></div>
<pre><code>#&gt;    s(range) 
#&gt; 0.006114634</code></pre>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" tabindex="-1"></a>pLidar <span class="sc">+</span></span>
<span id="cb40-2"><a href="#cb40-2" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> lidar<span class="sc">$</span>range, <span class="at">y =</span> gamfit<span class="sc">$</span>fitted), <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
</div>
</div>
<div id="more-general-error-definitions" class="section level2" number="7.6">
<h2><span class="header-section-number">7.6</span> More general error definitions</h2>
<p>So far we only looked at continuous outcomes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math> and errors defined in terms of the squared loss <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msup><mi>Y</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><annotation encoding="application/x-tex">(\hat{m}(\mathbf{x})-Y^*)^2</annotation></semantics></math>.</p>
<p>More generally, a <strong>loss function</strong> measures an discrepancy between the prediction <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{m}(\mathbf{x})</annotation></semantics></math> and an independent outcome <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Y</mi><mo>*</mo></msup><annotation encoding="application/x-tex">Y^*</annotation></semantics></math> that corresponds to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math>.</p>
<p>Some examples for continuous <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Y</mi><mo>*</mo></msup><mo>,</mo><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="center" style="text-align: center"><mo>=</mo></mtd><mtd columnalign="left" style="text-align: left"><msup><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msup><mi>Y</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mspace width="0.278em"></mspace><mspace width="0.278em"></mspace><mtext mathvariant="normal">(squared error)</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Y</mi><mo>*</mo></msup><mo>,</mo><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="center" style="text-align: center"><mo>=</mo></mtd><mtd columnalign="left" style="text-align: left"><mo stretchy="false" form="postfix">|</mo><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msup><mi>Y</mi><mo>*</mo></msup><mo stretchy="false" form="postfix">|</mo><mspace width="0.278em"></mspace><mspace width="0.278em"></mspace><mtext mathvariant="normal">(absolute error)</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Y</mi><mo>*</mo></msup><mo>,</mo><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="center" style="text-align: center"><mo>=</mo></mtd><mtd columnalign="left" style="text-align: left"><mn>2</mn><msub><mo>∫</mo><mi>𝒴</mi></msub><msub><mi>f</mi><mi>y</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>log</mo><mfrac><mrow><msub><mi>f</mi><mi>y</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><msub><mi>f</mi><mover><mi>m</mi><mo accent="true">̂</mo></mover></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mi>d</mi><mi>y</mi><mspace width="0.278em"></mspace><mspace width="0.278em"></mspace><mtext mathvariant="normal">(deviance)</mtext><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{eqnarray*}
  L(Y^*,\hat{m}(\mathbf{x}))
    &amp;=&amp; (\hat{m}(\mathbf{x})-Y^*)^2 \;\;\text{(squared error)} \\
  L(Y^*,\hat{m}(\mathbf{x}))
    &amp;=&amp; \vert\hat{m}(\mathbf{x})-Y^*\vert \;\;\text{(absolute error)} \\
   L(Y^*,\hat{m}(\mathbf{x}))
    &amp;=&amp; 2 \int_{\mathcal{Y}} f_y(y) \log\frac{f_y(y)}{f_{\hat{m}}(y)} dy \;\;\text{(deviance)}.
\end{eqnarray*}</annotation></semantics></math></p>
<p>In the expression of the deviance</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>f</mi><mi>y</mi></msub><annotation encoding="application/x-tex">f_y</annotation></semantics></math> denotes the density function of a distribution with mean set to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> (cfr. perfect fit), and</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>f</mi><mover><mi>m</mi><mo accent="true">̂</mo></mover></msub><annotation encoding="application/x-tex">f_{\hat{m}}</annotation></semantics></math> is the density function of the same distribution but with mean set to the predicted outcome <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{m}(\mathbf{x})</annotation></semantics></math>.</li>
</ul>
<hr />
<p>With a given loss function, the errors are defined as follows:
- Test or generalisation or outsample error
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">Err</mtext><mi>𝒯</mi></msub><mo>=</mo><msub><mtext mathvariant="normal">E</mtext><mrow><msup><mi>Y</mi><mo>*</mo></msup><mo>,</mo><msup><mi>X</mi><mo>*</mo></msup></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Y</mi><mo>*</mo></msup><mo>,</mo><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">
      \text{Err}_{\mathcal{T}} = \text{E}_{Y^*,X^*}\left[L(Y^*,\hat{m}(\mathbf{X}^*))\right]
    </annotation></semantics></math></p>
<ul>
<li><p>Training error
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mtext mathvariant="normal">err</mtext><mo accent="true">¯</mo></mover><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Y</mi><mi>i</mi></msub><mo>,</mo><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
  \overline{\text{err}} = \frac{1}{n}\sum_{i=1}^n L(Y_i,\hat{m}(\mathbf{x}_i))
</annotation></semantics></math></p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>…</mi><annotation encoding="application/x-tex">\ldots</annotation></semantics></math></p></li>
</ul>
<hr />
<p>When an exponential family distribution is assumed for the outcome distribution, and when the deviance loss is used, the insample error can be estimated by means of the AIC and BIC.</p>
<div id="akaikes-information-criterion-aic" class="section level3" number="7.6.1">
<h3><span class="header-section-number">7.6.1</span> Akaike’s Information Criterion (AIC)</h3>
<p>The AIC for a model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">AIC</mtext><mo>=</mo><mi>−</mi><mn>2</mn><mo>ln</mo><mover><mi>L</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mn>2</mn><mi>p</mi></mrow><annotation encoding="application/x-tex">
\text{AIC} = -2 \ln \hat{L}(m) +2p
</annotation></semantics></math>
where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>L</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{L}(m)</annotation></semantics></math> is the maximised likelihood for model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>.</p>
<p>When assuming normally distributed error terms and homoscedasticity, the AIC becomes
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">AIC</mtext><mo>=</mo><mi>n</mi><mo>ln</mo><mtext mathvariant="normal">SSE</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mn>2</mn><mi>p</mi><mo>=</mo><mi>n</mi><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mover><mtext mathvariant="normal">err</mtext><mo accent="true">¯</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mn>2</mn><mi>p</mi></mrow><annotation encoding="application/x-tex">
\text{AIC} = n\ln \text{SSE}(m) +2p = n\ln(n\overline{\text{err}}(m)) + 2p
</annotation></semantics></math>
with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">SSE</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{SSE}(m)</annotation></semantics></math> the residual sum of squares of model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>.</p>
<p>In linear models with normal error terms, Mallow’s <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>C</mi><mi>p</mi></msub><annotation encoding="application/x-tex">C_p</annotation></semantics></math> criterion (statistic) is a linearised version of AIC and it is an unbiased estimator of the in-sample error.</p>
<hr />
</div>
<div id="bayesian-information-criterion-bic" class="section level3" number="7.6.2">
<h3><span class="header-section-number">7.6.2</span> Bayesian Information Criterion (BIC)}</h3>
<p>The BIC for a model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">BIC</mtext><mo>=</mo><mi>−</mi><mn>2</mn><mo>ln</mo><mover><mi>L</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>p</mi><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\text{BIC} = -2 \ln \hat{L}(m) +p\ln(n)
</annotation></semantics></math>
where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>L</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{L}(m)</annotation></semantics></math> is the maximised likelihood for model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>.</p>
<p>When assuming normally distributed error terms and homoscedasticity, the BIC becomes
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">BIC</mtext><mo>=</mo><mi>n</mi><mo>ln</mo><mtext mathvariant="normal">SSE</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>p</mi><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>n</mi><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mover><mtext mathvariant="normal">err</mtext><mo accent="true">¯</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>p</mi><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\text{BIC} = n\ln \text{SSE}(m) +p\ln(n) = n\ln(n\overline{\text{err}}(m)) + p\ln(n)
</annotation></semantics></math>
with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">SSE</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{SSE}(m)</annotation></semantics></math> the residual sum of squares of model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>.</p>
<p>When large datasets are used, the BIC will favour smaller models than the AIC.</p>
<hr />
</div>
</div>
<div id="training-and-test-sets" class="section level2" number="7.7">
<h2><span class="header-section-number">7.7</span> Training and test sets</h2>
<p>Sometimes, when a large (training) dataset is available, one may decide the split the dataset randomly in a</p>
<ul>
<li><p><strong>training dataset</strong>:
data are used for model fitting and for model building or feature selection (this may require e.g. cross validation)</p></li>
<li><p><strong>test dataset</strong>:
this data are used to evaluate the final model (result of model building). An unbiased estimate of the outsample error (i.e. test or generalisation error) based on this test data is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>m</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">
   \frac{1}{m} \sum_{i=1}^m \left(\hat{m}(\mathbf{x}_i)-Y_i\right)^2,
</annotation></semantics></math>
where</p>
<ul>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Y</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝐱</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>…</mi><mo>,</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Y</mi><mi>m</mi></msub><mo>,</mo><msub><mi>𝐱</mi><mi>m</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(Y_1,\mathbf{x}_1), \ldots, (Y_m,\mathbf{x}_m)</annotation></semantics></math> denote the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> observations in the test dataset</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>m</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{m}</annotation></semantics></math> is estimated from using the training data (this may also be the result from model building, using only the training data).</p></li>
</ul></li>
</ul>
<hr />
<p>Note that the training dataset is used for model building or feature selection. This also requires the evaluation of models. For these evaluations the methods from the previous slides can be used (e.g. cross validation, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-fold CV, Mallow’s <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>C</mi><mi>p</mi></msub><annotation encoding="application/x-tex">C_p</annotation></semantics></math>). The test dataset is only used for the evaluation of the final model (estimated and build from using only the training data). The estimate of the outsample error based on the test dataset is the best possible estimate in the sense that it is unbiased. The observations used for this estimation are independent of the observations in the training data.
However, if the number of data points in the test dataset (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>) is small, the estimate of the outsample error may show large variance and hence is not reliable.</p>
</div>
</div>
<div id="logistic-regression-analysis-for-high-dimensional-data" class="section level1" number="8">
<h1><span class="header-section-number">8</span> Logistic Regression Analysis for High Dimensional Data</h1>
<div id="breast-cancer-example" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Breast Cancer Example</h2>
<ul>
<li><p>Schmidt <em>et al.</em>, 2008, Cancer Research, <strong>68</strong>, 5405-5413</p></li>
<li><p>Gene expression patterns in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>200</mn></mrow><annotation encoding="application/x-tex">n=200</annotation></semantics></math> breast tumors were investigated (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>22283</mn></mrow><annotation encoding="application/x-tex">p=22283</annotation></semantics></math> genes)</p></li>
<li><p>After surgery the tumors were graded by a pathologist (stage 1,2,3)</p></li>
<li><p>Here the objective is to predict stage 3 from the gene expression data (prediction of binary outcome)</p></li>
<li><p>If the prediction model works well, it can be used to predict the stage from a biopsy sample.</p></li>
</ul>
</div>
<div id="data-1" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Data</h2>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" tabindex="-1"></a><span class="co">#BiocManager::install(&quot;genefu&quot;)</span></span>
<span id="cb41-2"><a href="#cb41-2" tabindex="-1"></a><span class="co">#BiocManager::install(&quot;breastCancerMAINZ&quot;)</span></span>
<span id="cb41-3"><a href="#cb41-3" tabindex="-1"></a></span>
<span id="cb41-4"><a href="#cb41-4" tabindex="-1"></a><span class="fu">library</span>(genefu)</span>
<span id="cb41-5"><a href="#cb41-5" tabindex="-1"></a><span class="fu">library</span>(breastCancerMAINZ)</span>
<span id="cb41-6"><a href="#cb41-6" tabindex="-1"></a><span class="fu">data</span>(mainz)</span>
<span id="cb41-7"><a href="#cb41-7" tabindex="-1"></a></span>
<span id="cb41-8"><a href="#cb41-8" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">exprs</span>(mainz)) <span class="co"># gene expressions</span></span>
<span id="cb41-9"><a href="#cb41-9" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb41-10"><a href="#cb41-10" tabindex="-1"></a>H <span class="ot">&lt;-</span> <span class="fu">diag</span>(n)<span class="sc">-</span><span class="dv">1</span><span class="sc">/</span>n<span class="sc">*</span><span class="fu">matrix</span>(<span class="dv">1</span>,<span class="at">ncol=</span>n,<span class="at">nrow=</span>n)</span>
<span id="cb41-11"><a href="#cb41-11" tabindex="-1"></a>X <span class="ot">&lt;-</span> H<span class="sc">%*%</span>X</span>
<span id="cb41-12"><a href="#cb41-12" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(<span class="fu">pData</span>(mainz)<span class="sc">$</span>grade<span class="sc">==</span><span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb41-13"><a href="#cb41-13" tabindex="-1"></a><span class="fu">table</span>(Y)</span></code></pre></div>
<pre><code>#&gt; Y
#&gt;   0   1 
#&gt; 165  35</code></pre>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" tabindex="-1"></a>svdX <span class="ot">&lt;-</span> <span class="fu">svd</span>(X)</span>
<span id="cb43-2"><a href="#cb43-2" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb43-3"><a href="#cb43-3" tabindex="-1"></a>Zk <span class="ot">&lt;-</span> svdX<span class="sc">$</span>u[,<span class="dv">1</span><span class="sc">:</span>k] <span class="sc">%*%</span> <span class="fu">diag</span>(svdX<span class="sc">$</span>d[<span class="dv">1</span><span class="sc">:</span>k])</span>
<span id="cb43-4"><a href="#cb43-4" tabindex="-1"></a><span class="fu">colnames</span>(Zk) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;Z&quot;</span>,<span class="dv">1</span><span class="sc">:</span>k)</span>
<span id="cb43-5"><a href="#cb43-5" tabindex="-1"></a></span>
<span id="cb43-6"><a href="#cb43-6" tabindex="-1"></a>Zk <span class="sc">%&gt;%</span></span>
<span id="cb43-7"><a href="#cb43-7" tabindex="-1"></a>  as.data.frame <span class="sc">%&gt;%</span></span>
<span id="cb43-8"><a href="#cb43-8" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">grade =</span> Y <span class="sc">%&gt;%</span> as.factor) <span class="sc">%&gt;%</span></span>
<span id="cb43-9"><a href="#cb43-9" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span> Z1, <span class="at">y =</span> Z2, <span class="at">color =</span> grade)) <span class="sc">+</span></span>
<span id="cb43-10"><a href="#cb43-10" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<hr />
</div>
<div id="logistic-regression-models" class="section level2" number="8.3">
<h2><span class="header-section-number">8.3</span> Logistic regression models</h2>
<p>Binary outcomes are often analysed with <strong>logistic regression models</strong>.</p>
<p>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math> denote the binary (1/0, case/control, positive/negative) outcome, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math> the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-dimensional predictor.</p>
<p>Logistic regression assumes
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>∣</mo><mi>𝐱</mi><mo>∼</mo><mtext mathvariant="normal">Bernoulli</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>π</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
   Y \mid \mathbf{x} \sim \text{Bernoulli}(\pi(\mathbf{x}))
</annotation></semantics></math>
with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">P</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo>∣</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\pi(\mathbf{x}) = \text{P}\left[Y=1\mid \mathbf{x}\right]</annotation></semantics></math> and
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><mfrac><mrow><mi>π</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mn>1</mn><mo>−</mo><mi>π</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><msup><mi>𝛃</mi><mi>T</mi></msup><mi>𝐱</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">
   \ln \frac{\pi(\mathbf{x})}{1-\pi(\mathbf{x})}=\beta_0 + \boldsymbol{\beta}^T\mathbf{x}.
</annotation></semantics></math></p>
<p>The parameters are typically estimated by maximising the log-likelihood, which is denoted by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛃</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">l(\mathbf{
\beta})</annotation></semantics></math>, i.e.
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo>=</mo><msub><mtext mathvariant="normal">ArgMax</mtext><mi>β</mi></msub><mi>l</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛃</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
   \hat{\boldsymbol{\beta}} = \text{ArgMax}_\beta l(\boldsymbol{\beta}).
</annotation></semantics></math></p>
<ul>
<li><p>Maximum likelihood is only applicable when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>&gt;</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">n&gt;p</annotation></semantics></math>.</p></li>
<li><p>When <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&gt;</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">p&gt;n</annotation></semantics></math> penalised maximum likelihood methods are applicable.</p></li>
</ul>
<hr />
</div>
<div id="penalized-maximum-likelihood" class="section level2" number="8.4">
<h2><span class="header-section-number">8.4</span> Penalized maximum likelihood</h2>
<p>Penalised estimation methods (e.g. lasso and ridge) can als be applied to maximum likelihood, resulting in the <strong>penalised maximum likelihood estimator</strong>.</p>
<p>Lasso:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo>=</mo><msub><mtext mathvariant="normal">ArgMax</mtext><mi>β</mi></msub><mi>l</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛃</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>λ</mi><mo stretchy="false" form="postfix">‖</mo><mi>𝛃</mi><msub><mo stretchy="false" form="postfix">‖</mo><mn>1</mn></msub><mi>.</mi></mrow><annotation encoding="application/x-tex">
  \hat{\boldsymbol{\beta}} = \text{ArgMax}_\beta l(\boldsymbol{\beta}) -\lambda \Vert \boldsymbol{\beta}\Vert_1.
</annotation></semantics></math></p>
<p>Ridge:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo>=</mo><msub><mtext mathvariant="normal">ArgMax</mtext><mi>β</mi></msub><mi>l</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛃</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>λ</mi><mo stretchy="false" form="postfix">‖</mo><mi>𝛃</mi><msubsup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn><mn>2</mn></msubsup><mi>.</mi></mrow><annotation encoding="application/x-tex">
  \hat{\boldsymbol{\beta}} = \text{ArgMax}_\beta l(\boldsymbol{\beta}) -\lambda \Vert \boldsymbol{\beta}\Vert_2^2.
</annotation></semantics></math></p>
<p>Once the parameters are estimated, the model may be used to compute
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>π</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mover><mtext mathvariant="normal">P</mtext><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">[</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo>∣</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
  \hat{\pi}(\mathbf{x}) = \hat{\text{P}}\left[Y=1\mid \mathbf{x}\right].
</annotation></semantics></math>
With these estimated probabilities the prediction rule becomes
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mover><mi>π</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="center" style="text-align: center"><mo>≤</mo><mi>c</mi></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">predict </mtext><mspace width="0.333em"></mspace></mrow><mi>Y</mi><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mover><mi>π</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="center" style="text-align: center"><mo>&gt;</mo><mi>c</mi></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">predict </mtext><mspace width="0.333em"></mspace></mrow><mi>Y</mi><mo>=</mo><mn>1</mn></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{eqnarray*}
  \hat{\pi}(\mathbf{x}) &amp;\leq c&amp; \text{predict } Y=0 \\
  \hat{\pi}(\mathbf{x}) &amp;&gt;c &amp; \text{predict } Y=1
\end{eqnarray*}</annotation></semantics></math>
with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mo>&lt;</mo><mi>c</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">0&lt;c&lt;1</annotation></semantics></math> a threshold that either is fixed (e.g. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mo>=</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">c=1/2</annotation></semantics></math>), depends on prior probabilities, or is empirically determined by optimising e.g. the Area Under the ROC Curve (AUC) or by finding a good compromise between sensitivity and specificity.</p>
<p>Note that logistic regression directly models the <strong>Posterior probability</strong> that an observation belongs to class <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">Y=1</annotation></semantics></math>, given the predictor <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math>.</p>
</div>
<div id="model-evaluation" class="section level2" number="8.5">
<h2><span class="header-section-number">8.5</span> Model evaluation</h2>
<p>Common model evaluation criteria for binary prediction models are:</p>
<ul>
<li><p>sensitivity = true positive rate (TPR)</p></li>
<li><p>specificity = true negative rate (TNR)</p></li>
<li><p>misclassification error</p></li>
<li><p>area under the ROC curve (AUC)</p></li>
</ul>
<p>These criteria can again be estimated via cross validation or via splitting of the data into training and test/validation data.</p>
<div id="sensitivity-of-a-model-pi-with-threshold-c" class="section level3" number="8.5.1">
<h3><span class="header-section-number">8.5.1</span> Sensitivity of a model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>π</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math> with threshold <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math></h3>
<p>Sensitivity is the probability to correctly predict a positive outcome:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">sens</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>π</mi><mo>,</mo><mi>c</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mtext mathvariant="normal">P</mtext><msup><mi>X</mi><mo>*</mo></msup></msub><mrow><mo stretchy="true" form="prefix">[</mo><mover><mi>π</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>&gt;</mo><mi>c</mi><mo>∣</mo><msup><mi>Y</mi><mo>*</mo></msup><mo>=</mo><mn>1</mn><mo>∣</mo><mi>𝒯</mi><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
\text{sens}(\pi,c)=\text{P}_{X^*}\left[\hat\pi(\mathbf{X}^*)&gt;c \mid Y^*=1 \mid {\mathcal{T}}\right].
</annotation></semantics></math></p>
<p>It is also known as the true positive rate (TPR).</p>
</div>
<div id="specificity-of-a-model-pi-with-threshold-c" class="section level3" number="8.5.2">
<h3><span class="header-section-number">8.5.2</span> Specificity of a model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>π</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math> with threshold <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math></h3>
<p>Specificity is the probability to correctly predict a negative outcome:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">spec</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>π</mi><mo>,</mo><mi>c</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mtext mathvariant="normal">P</mtext><msup><mi>X</mi><mo>*</mo></msup></msub><mrow><mo stretchy="true" form="prefix">[</mo><mover><mi>π</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>≤</mo><mi>c</mi><mo>∣</mo><msup><mi>Y</mi><mo>*</mo></msup><mo>=</mo><mn>0</mn><mo>∣</mo><mi>𝒯</mi><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
\text{spec}(\pi,c)=\text{P}_{X^*}\left[\hat\pi(\mathbf{X}^*)\leq c \mid Y^*=0 \mid {\mathcal{T}}\right].
</annotation></semantics></math></p>
<p>It is also known as the true negative rate (TNR).</p>
<hr />
</div>
<div id="misclassification-error-of-a-model-pi-with-threshold-c" class="section level3" number="8.5.3">
<h3><span class="header-section-number">8.5.3</span> Misclassification error of a model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>π</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math> with threshold <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math></h3>
<p>The misclassification error is the probability to incorrectly predict an outcome:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">mce</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>π</mi><mo>,</mo><mi>c</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="center" style="text-align: center"><mo>=</mo></mtd><mtd columnalign="left" style="text-align: left"><msub><mtext mathvariant="normal">P</mtext><mrow><msup><mi>X</mi><mo>*</mo></msup><mo>,</mo><msup><mi>Y</mi><mo>*</mo></msup></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><mover><mi>π</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≤</mo><mi>c</mi><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> and </mtext><mspace width="0.333em"></mspace></mrow><msup><mi>Y</mi><mo>*</mo></msup><mo>=</mo><mn>1</mn><mo>∣</mo><mi>𝒯</mi><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="left" style="text-align: left"><mi>+</mi><msub><mtext mathvariant="normal">P</mtext><mrow><msup><mi>X</mi><mo>*</mo></msup><mo>,</mo><msup><mi>Y</mi><mo>*</mo></msup></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><mover><mi>π</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐗</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>&gt;</mo><mi>c</mi><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> and </mtext><mspace width="0.333em"></mspace></mrow><msup><mi>Y</mi><mo>*</mo></msup><mo>=</mo><mn>0</mn><mo>∣</mo><mi>𝒯</mi><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{eqnarray*}
\text{mce}(\pi,c) &amp;=&amp;\text{P}_{X^*,Y^*}\left[\hat\pi(\mathbf{X})\leq c \text{ and } Y^*=1 \mid {\mathcal{T}}\right] \\
&amp;  &amp; + \text{P}_{X^*,Y^*}\left[\hat\pi(\mathbf{X})&gt; c \text{ and } Y^*=0 \mid {\mathcal{T}}\right].
\end{eqnarray*}</annotation></semantics></math></p>
<p>Note that in the definitions of sensitivity, specificity and the misclassification error, the probabilities refer to the distribution of the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐗</mi><mo>*</mo></msup><mo>,</mo><msup><mi>Y</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\mathbf{X}^*,Y^*)</annotation></semantics></math>, which is independent of the training data, conditional on the training data. This is in line with the test or generalisation error. The misclassification error is actually the test error when a 0/1 loss function is used. Just as before, the sensitivity, specificity and the misclassification error can also be averaged over the distribution of the training data set, which is in line with the expected test error which has been discussed earlier.</p>
<hr />
</div>
<div id="roc-curve-of-a-model-pi" class="section level3" number="8.5.4">
<h3><span class="header-section-number">8.5.4</span> ROC curve of a model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>π</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math></h3>
<p>The Receiver Operating Characteristic (ROC) curve for model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>π</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math> is given by the function</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">ROC</mtext><mo>:</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">]</mo></mrow><mo>→</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">]</mo></mrow><mo>×</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">]</mo></mrow><mo>:</mo><mi>c</mi><mo>↦</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mtext mathvariant="normal">spec</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>π</mi><mo>,</mo><mi>c</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mtext mathvariant="normal">sens</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>π</mi><mo>,</mo><mi>c</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
\text{ROC}: [0,1] \rightarrow [0,1]\times [0,1]: c \mapsto (1-\text{spec}(\pi,c), \text{sens}(\pi,c)).
</annotation></semantics></math></p>
<p>For when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math> moves from 1 to 0, the ROC function defines a curve in the plane <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">]</mo></mrow><mo>×</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">[0,1]\times [0,1]</annotation></semantics></math>, moving from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(0,0)</annotation></semantics></math> for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">c=1</annotation></semantics></math> to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(1,1)</annotation></semantics></math> for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">c=0</annotation></semantics></math>.</p>
<p>The horizontal axis of the ROC curve shows 1-specificity. This is also known as the False Positive Rate (FPR).</p>
<hr />
</div>
<div id="area-under-the-curve-auc-of-a-model-pi" class="section level3" number="8.5.5">
<h3><span class="header-section-number">8.5.5</span> Area under the curve (AUC) of a model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>π</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math></h3>
<p>The area under the curve (AUC) for model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>π</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math> is area under the ROC curve and is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>∫</mo><mn>0</mn><mn>1</mn></msubsup><mtext mathvariant="normal">ROC</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>c</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>c</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">
\int_0^1 \text{ROC}(c) dc.
</annotation></semantics></math></p>
<p>Some notes about the AUC:</p>
<ul>
<li><p>AUC=0.5 results when the ROC curve is the diagonal. This corresponds to flipping a coin, i.e. a complete random prediction.</p></li>
<li><p>AUC=1 results from the perfect ROC curve, which is the ROC curve through the points <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(0,0)</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(0,1)</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(1,1)</annotation></semantics></math>. This ROC curve includes a threshold <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math> such that sensitivity and specificity are equal to one.</p></li>
</ul>
</div>
</div>
<div id="breast-cancer-example-1" class="section level2" number="8.6">
<h2><span class="header-section-number">8.6</span> Breast cancer example</h2>
<div id="data-2" class="section level3" number="8.6.1">
<h3><span class="header-section-number">8.6.1</span> Data</h3>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb44-2"><a href="#cb44-2" tabindex="-1"></a></span>
<span id="cb44-3"><a href="#cb44-3" tabindex="-1"></a><span class="co">#BiocManager::install(&quot;genefu&quot;)</span></span>
<span id="cb44-4"><a href="#cb44-4" tabindex="-1"></a><span class="co">#BiocManager::install(&quot;breastCancerMAINZ&quot;)</span></span>
<span id="cb44-5"><a href="#cb44-5" tabindex="-1"></a></span>
<span id="cb44-6"><a href="#cb44-6" tabindex="-1"></a><span class="fu">library</span>(genefu)</span>
<span id="cb44-7"><a href="#cb44-7" tabindex="-1"></a><span class="fu">library</span>(breastCancerMAINZ)</span>
<span id="cb44-8"><a href="#cb44-8" tabindex="-1"></a><span class="fu">data</span>(mainz)</span>
<span id="cb44-9"><a href="#cb44-9" tabindex="-1"></a></span>
<span id="cb44-10"><a href="#cb44-10" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">exprs</span>(mainz)) <span class="co"># gene expressions</span></span>
<span id="cb44-11"><a href="#cb44-11" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb44-12"><a href="#cb44-12" tabindex="-1"></a>H <span class="ot">&lt;-</span> <span class="fu">diag</span>(n)<span class="sc">-</span><span class="dv">1</span><span class="sc">/</span>n<span class="sc">*</span><span class="fu">matrix</span>(<span class="dv">1</span>,<span class="at">ncol=</span>n,<span class="at">nrow=</span>n)</span>
<span id="cb44-13"><a href="#cb44-13" tabindex="-1"></a>X <span class="ot">&lt;-</span> H<span class="sc">%*%</span>X</span>
<span id="cb44-14"><a href="#cb44-14" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(<span class="fu">pData</span>(mainz)<span class="sc">$</span>grade<span class="sc">==</span><span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb44-15"><a href="#cb44-15" tabindex="-1"></a><span class="fu">table</span>(Y)</span></code></pre></div>
<pre><code>#&gt; Y
#&gt;   0   1 
#&gt; 165  35</code></pre>
<hr />
<p>From the table of the outcomes in Y we read that</p>
<ul>
<li>35 tumors were graded as stage 3 and</li>
<li>165 tumors were graded as stage 1 or 2.</li>
</ul>
<p>In this the stage 3 tumors are referred to as cases or postives and the stage 1 and 2 tumors as controls or negatives.</p>
<hr />
</div>
<div id="training-and-test-dataset" class="section level3" number="8.6.2">
<h3><span class="header-section-number">8.6.2</span> Training and test dataset</h3>
<p>The use of the lasso logistic regression for the prediction of stage 3 breast cancer is illustrated here by</p>
<ul>
<li><p>randomly splitting the dataset into a training dataset (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>80</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">80\%</annotation></semantics></math> of data = 160 tumors) and a test dataset (40 tumors)</p></li>
<li><p>using the training data to select a good <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> value in the lasso logistic regression model (through 10-fold CV)</p></li>
<li><p>evaluating the final model by means of the test dataset (ROC Curve, AUC).</p></li>
</ul>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" tabindex="-1"></a><span class="do">## Used to provide same results as in previous R version</span></span>
<span id="cb46-2"><a href="#cb46-2" tabindex="-1"></a><span class="fu">RNGkind</span>(<span class="at">sample.kind =</span> <span class="st">&quot;Rounding&quot;</span>)</span>
<span id="cb46-3"><a href="#cb46-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">6977326</span>)</span>
<span id="cb46-4"><a href="#cb46-4" tabindex="-1"></a><span class="do">####</span></span>
<span id="cb46-5"><a href="#cb46-5" tabindex="-1"></a></span>
<span id="cb46-6"><a href="#cb46-6" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb46-7"><a href="#cb46-7" tabindex="-1"></a>nTrain <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fl">0.8</span><span class="sc">*</span>n)</span>
<span id="cb46-8"><a href="#cb46-8" tabindex="-1"></a>nTrain</span></code></pre></div>
<pre><code>#&gt; [1] 160</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" tabindex="-1"></a>indTrain <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n,nTrain)</span>
<span id="cb48-2"><a href="#cb48-2" tabindex="-1"></a>XTrain <span class="ot">&lt;-</span> X[indTrain,]</span>
<span id="cb48-3"><a href="#cb48-3" tabindex="-1"></a>YTrain <span class="ot">&lt;-</span> Y[indTrain]</span>
<span id="cb48-4"><a href="#cb48-4" tabindex="-1"></a>XTest <span class="ot">&lt;-</span> X[<span class="sc">-</span>indTrain,]</span>
<span id="cb48-5"><a href="#cb48-5" tabindex="-1"></a>YTest <span class="ot">&lt;-</span> Y[<span class="sc">-</span>indTrain]</span>
<span id="cb48-6"><a href="#cb48-6" tabindex="-1"></a><span class="fu">table</span>(YTest)</span></code></pre></div>
<pre><code>#&gt; YTest
#&gt;  0  1 
#&gt; 32  8</code></pre>
<p>Note that the randomly selected test data has 20% cases of stage 3 tumors.
This is a bit higher than the 17.5% in the complete data.</p>
<p>One could also perform the random splitting among the positives and the negatives separately (stratified splitting).</p>
</div>
<div id="model-fitting-based-on-training-data" class="section level3" number="8.6.3">
<h3><span class="header-section-number">8.6.3</span> Model fitting based on training data</h3>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" tabindex="-1"></a>mLasso <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(</span>
<span id="cb50-2"><a href="#cb50-2" tabindex="-1"></a>  <span class="at">x =</span> XTrain,</span>
<span id="cb50-3"><a href="#cb50-3" tabindex="-1"></a>  <span class="at">y =</span> YTrain,</span>
<span id="cb50-4"><a href="#cb50-4" tabindex="-1"></a>  <span class="at">alpha =</span> <span class="dv">1</span>,</span>
<span id="cb50-5"><a href="#cb50-5" tabindex="-1"></a>  <span class="at">family=</span><span class="st">&quot;binomial&quot;</span>)  <span class="co"># lasso: alpha = 1</span></span>
<span id="cb50-6"><a href="#cb50-6" tabindex="-1"></a></span>
<span id="cb50-7"><a href="#cb50-7" tabindex="-1"></a><span class="fu">plot</span>(mLasso, <span class="at">xvar =</span> <span class="st">&quot;lambda&quot;</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">6</span>,<span class="sc">-</span><span class="fl">1.5</span>))</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<hr />
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" tabindex="-1"></a>mCvLasso <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(</span>
<span id="cb51-2"><a href="#cb51-2" tabindex="-1"></a>  <span class="at">x =</span> XTrain,</span>
<span id="cb51-3"><a href="#cb51-3" tabindex="-1"></a>  <span class="at">y =</span> YTrain,</span>
<span id="cb51-4"><a href="#cb51-4" tabindex="-1"></a>  <span class="at">alpha =</span> <span class="dv">1</span>,</span>
<span id="cb51-5"><a href="#cb51-5" tabindex="-1"></a>  <span class="at">type.measure =</span> <span class="st">&quot;class&quot;</span>,</span>
<span id="cb51-6"><a href="#cb51-6" tabindex="-1"></a>    <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)  <span class="co"># lasso alpha = 1</span></span>
<span id="cb51-7"><a href="#cb51-7" tabindex="-1"></a></span>
<span id="cb51-8"><a href="#cb51-8" tabindex="-1"></a><span class="fu">plot</span>(mCvLasso)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" tabindex="-1"></a>mCvLasso</span></code></pre></div>
<pre><code>#&gt; 
#&gt; Call:  cv.glmnet(x = XTrain, y = YTrain, type.measure = &quot;class&quot;, alpha = 1,      family = &quot;binomial&quot;) 
#&gt; 
#&gt; Measure: Misclassification Error 
#&gt; 
#&gt;     Lambda Index Measure      SE Nonzero
#&gt; min 0.1044    14  0.1437 0.03366      18
#&gt; 1se 0.1911     1  0.1688 0.03492       0</code></pre>
<p>The total misclassification error is used here to select a good value for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" tabindex="-1"></a><span class="co"># BiocManager::install(&quot;plotROC&quot;)</span></span>
<span id="cb54-2"><a href="#cb54-2" tabindex="-1"></a><span class="fu">library</span>(plotROC)</span>
<span id="cb54-3"><a href="#cb54-3" tabindex="-1"></a></span>
<span id="cb54-4"><a href="#cb54-4" tabindex="-1"></a>dfLassoOpt <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb54-5"><a href="#cb54-5" tabindex="-1"></a>  <span class="at">pi =</span> <span class="fu">predict</span>(mCvLasso,</span>
<span id="cb54-6"><a href="#cb54-6" tabindex="-1"></a>    <span class="at">newx =</span> XTest,</span>
<span id="cb54-7"><a href="#cb54-7" tabindex="-1"></a>    <span class="at">s =</span> mCvLasso<span class="sc">$</span>lambda.min,</span>
<span id="cb54-8"><a href="#cb54-8" tabindex="-1"></a>    <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="sc">%&gt;%</span> <span class="fu">c</span>(.),</span>
<span id="cb54-9"><a href="#cb54-9" tabindex="-1"></a>  <span class="at">known.truth =</span> YTest)</span>
<span id="cb54-10"><a href="#cb54-10" tabindex="-1"></a></span>
<span id="cb54-11"><a href="#cb54-11" tabindex="-1"></a>roc <span class="ot">&lt;-</span></span>
<span id="cb54-12"><a href="#cb54-12" tabindex="-1"></a>  dfLassoOpt  <span class="sc">%&gt;%</span></span>
<span id="cb54-13"><a href="#cb54-13" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">d =</span> known.truth, <span class="at">m =</span> pi)) <span class="sc">+</span></span>
<span id="cb54-14"><a href="#cb54-14" tabindex="-1"></a>  <span class="fu">geom_roc</span>(<span class="at">n.cuts =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb54-15"><a href="#cb54-15" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;1-specificity (FPR)&quot;</span>) <span class="sc">+</span></span>
<span id="cb54-16"><a href="#cb54-16" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;sensitivity (TPR)&quot;</span>)</span>
<span id="cb54-17"><a href="#cb54-17" tabindex="-1"></a></span>
<span id="cb54-18"><a href="#cb54-18" tabindex="-1"></a>roc</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" tabindex="-1"></a><span class="fu">calc_auc</span>(roc)</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["PANEL"],"name":[1],"type":["fct"],"align":["left"]},{"label":["group"],"name":[2],"type":["int"],"align":["right"]},{"label":["AUC"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"1","2":"-1","3":"0.8320312","_rn_":"1"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<ul>
<li><p>The ROC curve is shown for the model based on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> with the smallest misclassification error. The model has an AUC of 0.83.</p></li>
<li><p>Based on this ROC curve an appropriate threshold <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math> can be chosen. For example, from the ROC curve we see that it is possible to attain a specificity and a sensitivity of 75%.</p></li>
<li><p>The sensitivities and specificities in the ROC curve are unbiased (independent test dataset) for the prediction model build from the training data. The estimates of sensitivity and specificity, however, are based on only 40 observations.</p></li>
</ul>
<hr />
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" tabindex="-1"></a>mLambdaOpt <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(<span class="at">x =</span> XTrain,</span>
<span id="cb56-2"><a href="#cb56-2" tabindex="-1"></a>  <span class="at">y =</span> YTrain,</span>
<span id="cb56-3"><a href="#cb56-3" tabindex="-1"></a>  <span class="at">alpha =</span> <span class="dv">1</span>,</span>
<span id="cb56-4"><a href="#cb56-4" tabindex="-1"></a>  <span class="at">lambda =</span> mCvLasso<span class="sc">$</span>lambda.min,</span>
<span id="cb56-5"><a href="#cb56-5" tabindex="-1"></a>  <span class="at">family=</span><span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb56-6"><a href="#cb56-6" tabindex="-1"></a></span>
<span id="cb56-7"><a href="#cb56-7" tabindex="-1"></a><span class="fu">qplot</span>(</span>
<span id="cb56-8"><a href="#cb56-8" tabindex="-1"></a>  <span class="fu">summary</span>(<span class="fu">coef</span>(mLambdaOpt))[<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb56-9"><a href="#cb56-9" tabindex="-1"></a>  <span class="fu">summary</span>(<span class="fu">coef</span>(mLambdaOpt))[<span class="sc">-</span><span class="dv">1</span>,<span class="dv">3</span>]) <span class="sc">+</span></span>
<span id="cb56-10"><a href="#cb56-10" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;gene ID&quot;</span>) <span class="sc">+</span></span>
<span id="cb56-11"><a href="#cb56-11" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;beta-hat&quot;</span>) <span class="sc">+</span></span>
<span id="cb56-12"><a href="#cb56-12" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<ul>
<li>The model with the optimal <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> has only 19 non-zero parameter estimates.</li>
<li>Thus only 19 genes are involved in the prediction model.</li>
<li>These 19 parameter estimates are plotting in the graph.
A listing of the model output would show the names of the genes.</li>
</ul>
<hr />
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" tabindex="-1"></a>dfLasso1se <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb57-2"><a href="#cb57-2" tabindex="-1"></a>  <span class="at">pi =</span> <span class="fu">predict</span>(mCvLasso,</span>
<span id="cb57-3"><a href="#cb57-3" tabindex="-1"></a>    <span class="at">newx =</span> XTest,</span>
<span id="cb57-4"><a href="#cb57-4" tabindex="-1"></a>    <span class="at">s =</span> mCvLasso<span class="sc">$</span>lambda<span class="fl">.1</span>se,</span>
<span id="cb57-5"><a href="#cb57-5" tabindex="-1"></a>    <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="sc">%&gt;%</span> <span class="fu">c</span>(.),</span>
<span id="cb57-6"><a href="#cb57-6" tabindex="-1"></a>  <span class="at">known.truth =</span> YTest)</span>
<span id="cb57-7"><a href="#cb57-7" tabindex="-1"></a></span>
<span id="cb57-8"><a href="#cb57-8" tabindex="-1"></a>roc <span class="ot">&lt;-</span></span>
<span id="cb57-9"><a href="#cb57-9" tabindex="-1"></a>  <span class="fu">rbind</span>(</span>
<span id="cb57-10"><a href="#cb57-10" tabindex="-1"></a>    dfLassoOpt <span class="sc">%&gt;%</span></span>
<span id="cb57-11"><a href="#cb57-11" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="at">method =</span> <span class="st">&quot;min&quot;</span>),</span>
<span id="cb57-12"><a href="#cb57-12" tabindex="-1"></a>    dfLasso1se <span class="sc">%&gt;%</span></span>
<span id="cb57-13"><a href="#cb57-13" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="at">method =</span> <span class="st">&quot;1se&quot;</span>)</span>
<span id="cb57-14"><a href="#cb57-14" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb57-15"><a href="#cb57-15" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">d =</span> known.truth, <span class="at">m =</span> pi, <span class="at">color =</span> method)) <span class="sc">+</span></span>
<span id="cb57-16"><a href="#cb57-16" tabindex="-1"></a>  <span class="fu">geom_roc</span>(<span class="at">n.cuts =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb57-17"><a href="#cb57-17" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;1-specificity (FPR)&quot;</span>) <span class="sc">+</span></span>
<span id="cb57-18"><a href="#cb57-18" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;sensitivity (TPR)&quot;</span>)</span>
<span id="cb57-19"><a href="#cb57-19" tabindex="-1"></a></span>
<span id="cb57-20"><a href="#cb57-20" tabindex="-1"></a>roc</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" tabindex="-1"></a><span class="fu">calc_auc</span>(roc)</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["PANEL"],"name":[1],"type":["fct"],"align":["left"]},{"label":["group"],"name":[2],"type":["int"],"align":["right"]},{"label":["method"],"name":[3],"type":["chr"],"align":["left"]},{"label":["AUC"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"1","2":"1","3":"1se","4":"0.5000000","_rn_":"1"},{"1":"1","2":"2","3":"min","4":"0.8320312","_rn_":"2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<ul>
<li><p>When using the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> of the optimal model up to 1 standard deviation, a diagonal ROC curve is obtained and hence AUC is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0.5</mn><annotation encoding="application/x-tex">0.5</annotation></semantics></math>.</p></li>
<li><p>This prediction model is thus equivalent to flipping a coin for making the prediction.</p></li>
<li><p>The reason is that with this choice of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> (strong penalisation) almost all predictors are removed from the model.</p></li>
<li><p>Therefore, do never blindly choose for the ``optimal’’ <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> as defined here, but assess the performance of the model first.</p></li>
</ul>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" tabindex="-1"></a>mLambda1se <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(<span class="at">x =</span> XTrain,</span>
<span id="cb59-2"><a href="#cb59-2" tabindex="-1"></a>  <span class="at">y =</span> YTrain,</span>
<span id="cb59-3"><a href="#cb59-3" tabindex="-1"></a>  <span class="at">alpha =</span> <span class="dv">1</span>,</span>
<span id="cb59-4"><a href="#cb59-4" tabindex="-1"></a>  <span class="at">lambda =</span> mCvLasso<span class="sc">$</span>lambda<span class="fl">.1</span>se,</span>
<span id="cb59-5"><a href="#cb59-5" tabindex="-1"></a>  <span class="at">family=</span><span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb59-6"><a href="#cb59-6" tabindex="-1"></a></span>
<span id="cb59-7"><a href="#cb59-7" tabindex="-1"></a>mLambda1se <span class="sc">%&gt;%</span></span>
<span id="cb59-8"><a href="#cb59-8" tabindex="-1"></a>  coef <span class="sc">%&gt;%</span></span>
<span id="cb59-9"><a href="#cb59-9" tabindex="-1"></a>  summary</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["i"],"name":[1],"type":["int"],"align":["right"]},{"label":["j"],"name":[2],"type":["int"],"align":["right"]},{"label":["x"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"1","2":"1","3":"-1.594512"},{"1":"2","2":"1","3":"0.000000"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<hr />
</div>
</div>
<div id="the-elastic-net" class="section level2" number="8.7">
<h2><span class="header-section-number">8.7</span> The Elastic Net</h2>
<p>The lasso and ridge regression have positive and negative properties.</p>
<ul>
<li><p>Lasso</p>
<ul>
<li><p>positive: sparse solution</p></li>
<li><p>negative: at most <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>min</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>,</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\min(n,p)</annotation></semantics></math> predictors can be selected</p></li>
<li><p>negative: tend to select one predictor among a group of highly correlated predictors</p></li>
</ul></li>
<li><p>Ridge</p>
<ul>
<li>negative: no sparse solution</li>
<li>positive: more than <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>min</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>,</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\min(n,p)</annotation></semantics></math> predictors can be selected</li>
</ul></li>
</ul>
<p>A compromise between lasso and ridge: the <strong>elastic net</strong>:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo>=</mo><msub><mtext mathvariant="normal">ArgMax</mtext><mi>β</mi></msub><mi>l</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛃</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msub><mi>γ</mi><mn>1</mn></msub><mo stretchy="false" form="postfix">‖</mo><mi>𝛃</mi><msub><mo stretchy="false" form="postfix">‖</mo><mn>1</mn></msub><mo>−</mo><msub><mi>γ</mi><mn>2</mn></msub><mo stretchy="false" form="postfix">‖</mo><mi>𝛃</mi><msubsup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn><mn>2</mn></msubsup><mi>.</mi></mrow><annotation encoding="application/x-tex">
  \hat{\boldsymbol{\beta}} = \text{ArgMax}_\beta l(\boldsymbol{\beta}) -\gamma_1 \Vert \boldsymbol\beta\Vert_1 -\gamma_2 \Vert \boldsymbol\beta\Vert_2^2.
</annotation></semantics></math></p>
<p>The elastic gives a sparse solution with potentially more than <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>min</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>,</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\min(n,p)</annotation></semantics></math> predictors.</p>
<hr />
<p>The <code>glmnet</code> R function uses the following parameterisation,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>𝛃</mi><mo accent="true">̂</mo></mover><mo>=</mo><msub><mtext mathvariant="normal">ArgMax</mtext><mi>β</mi></msub><mi>l</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛃</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>λ</mi><mi>α</mi><mo stretchy="false" form="postfix">‖</mo><mi>𝛃</mi><msub><mo stretchy="false" form="postfix">‖</mo><mn>1</mn></msub><mo>−</mo><mi>λ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="false" form="postfix">‖</mo><mi>𝛃</mi><msubsup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn><mn>2</mn></msubsup><mi>.</mi></mrow><annotation encoding="application/x-tex">
  \hat{\boldsymbol{\beta}} = \text{ArgMax}_\beta l(\boldsymbol{\beta}) -\lambda\alpha \Vert \boldsymbol\beta\Vert_1 -\lambda(1-\alpha) \Vert \boldsymbol\beta\Vert_2^2.
</annotation></semantics></math></p>
<ul>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> parameter gives weight to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding="application/x-tex">L_1</annotation></semantics></math> penalty term (hence <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha=1</annotation></semantics></math> gives the lasso, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\alpha=0</annotation></semantics></math> gives ridge).</p></li>
<li><p>a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> parameter to give weight to the penalisation</p></li>
<li><p>Note that the combination of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> gives the same flexibility as the combination of the parameters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>λ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\lambda_1</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>λ</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\lambda_2</annotation></semantics></math>.</p></li>
</ul>
<hr />
<div id="breast-cancer-example-2" class="section level3" number="8.7.1">
<h3><span class="header-section-number">8.7.1</span> Breast cancer example</h3>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" tabindex="-1"></a>mElastic <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(</span>
<span id="cb60-2"><a href="#cb60-2" tabindex="-1"></a>  <span class="at">x =</span> XTrain,</span>
<span id="cb60-3"><a href="#cb60-3" tabindex="-1"></a>  <span class="at">y =</span> YTrain,</span>
<span id="cb60-4"><a href="#cb60-4" tabindex="-1"></a>  <span class="at">alpha =</span> <span class="fl">0.5</span>,</span>
<span id="cb60-5"><a href="#cb60-5" tabindex="-1"></a>  <span class="at">family=</span><span class="st">&quot;binomial&quot;</span>)  <span class="co"># elastic net</span></span>
<span id="cb60-6"><a href="#cb60-6" tabindex="-1"></a></span>
<span id="cb60-7"><a href="#cb60-7" tabindex="-1"></a><span class="fu">plot</span>(mElastic, <span class="at">xvar =</span> <span class="st">&quot;lambda&quot;</span>,<span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="fl">5.5</span>,<span class="sc">-</span><span class="dv">1</span>))</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" tabindex="-1"></a>mCvElastic <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(<span class="at">x =</span> XTrain,</span>
<span id="cb61-2"><a href="#cb61-2" tabindex="-1"></a>  <span class="at">y =</span> YTrain,</span>
<span id="cb61-3"><a href="#cb61-3" tabindex="-1"></a>  <span class="at">alpha =</span> <span class="fl">0.5</span>,</span>
<span id="cb61-4"><a href="#cb61-4" tabindex="-1"></a>  <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>,</span>
<span id="cb61-5"><a href="#cb61-5" tabindex="-1"></a>    <span class="at">type.measure =</span> <span class="st">&quot;class&quot;</span>)  <span class="co"># elastic net</span></span>
<span id="cb61-6"><a href="#cb61-6" tabindex="-1"></a></span>
<span id="cb61-7"><a href="#cb61-7" tabindex="-1"></a><span class="fu">plot</span>(mCvElastic)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" tabindex="-1"></a>mCvElastic</span></code></pre></div>
<pre><code>#&gt; 
#&gt; Call:  cv.glmnet(x = XTrain, y = YTrain, type.measure = &quot;class&quot;, alpha = 0.5,      family = &quot;binomial&quot;) 
#&gt; 
#&gt; Measure: Misclassification Error 
#&gt; 
#&gt;      Lambda Index Measure      SE Nonzero
#&gt; min 0.01859    66  0.1313 0.02708     148
#&gt; 1se 0.21876    13  0.1562 0.03391      26</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" tabindex="-1"></a>dfElast <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb64-2"><a href="#cb64-2" tabindex="-1"></a>  <span class="at">pi =</span> <span class="fu">predict</span>(mElastic,</span>
<span id="cb64-3"><a href="#cb64-3" tabindex="-1"></a>    <span class="at">newx =</span> XTest,</span>
<span id="cb64-4"><a href="#cb64-4" tabindex="-1"></a>    <span class="at">s =</span> mCvElastic<span class="sc">$</span>lambda.min,</span>
<span id="cb64-5"><a href="#cb64-5" tabindex="-1"></a>    <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="sc">%&gt;%</span> <span class="fu">c</span>(.),</span>
<span id="cb64-6"><a href="#cb64-6" tabindex="-1"></a>  <span class="at">known.truth =</span> YTest)</span>
<span id="cb64-7"><a href="#cb64-7" tabindex="-1"></a></span>
<span id="cb64-8"><a href="#cb64-8" tabindex="-1"></a>roc <span class="ot">&lt;-</span> <span class="fu">rbind</span>(</span>
<span id="cb64-9"><a href="#cb64-9" tabindex="-1"></a>  dfLassoOpt <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">method =</span> <span class="st">&quot;lasso&quot;</span>),</span>
<span id="cb64-10"><a href="#cb64-10" tabindex="-1"></a>  dfElast <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">method =</span> <span class="st">&quot;elast. net&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb64-11"><a href="#cb64-11" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">d =</span> known.truth, <span class="at">m =</span> pi, <span class="at">color =</span> method)) <span class="sc">+</span></span>
<span id="cb64-12"><a href="#cb64-12" tabindex="-1"></a>  <span class="fu">geom_roc</span>(<span class="at">n.cuts =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb64-13"><a href="#cb64-13" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;1-specificity (FPR)&quot;</span>) <span class="sc">+</span></span>
<span id="cb64-14"><a href="#cb64-14" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;sensitivity (TPR)&quot;</span>)</span>
<span id="cb64-15"><a href="#cb64-15" tabindex="-1"></a></span>
<span id="cb64-16"><a href="#cb64-16" tabindex="-1"></a>roc</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" tabindex="-1"></a><span class="fu">calc_auc</span>(roc)</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["PANEL"],"name":[1],"type":["fct"],"align":["left"]},{"label":["group"],"name":[2],"type":["int"],"align":["right"]},{"label":["method"],"name":[3],"type":["chr"],"align":["left"]},{"label":["AUC"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"1","2":"1","3":"elast. net","4":"0.8398438","_rn_":"1"},{"1":"1","2":"2","3":"lasso","4":"0.8320312","_rn_":"2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<ul>
<li>More parameters are used than for the lasso, but the performance does not improve.</li>
</ul>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" tabindex="-1"></a>mElasticOpt <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(<span class="at">x =</span> XTrain,</span>
<span id="cb66-2"><a href="#cb66-2" tabindex="-1"></a>  <span class="at">y =</span> YTrain,</span>
<span id="cb66-3"><a href="#cb66-3" tabindex="-1"></a>  <span class="at">alpha =</span> <span class="fl">0.5</span>,</span>
<span id="cb66-4"><a href="#cb66-4" tabindex="-1"></a>  <span class="at">lambda =</span> mCvElastic<span class="sc">$</span>lambda.min,</span>
<span id="cb66-5"><a href="#cb66-5" tabindex="-1"></a>  <span class="at">family=</span><span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb66-6"><a href="#cb66-6" tabindex="-1"></a></span>
<span id="cb66-7"><a href="#cb66-7" tabindex="-1"></a><span class="fu">qplot</span>(</span>
<span id="cb66-8"><a href="#cb66-8" tabindex="-1"></a>  <span class="fu">summary</span>(<span class="fu">coef</span>(mElasticOpt))[<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb66-9"><a href="#cb66-9" tabindex="-1"></a>  <span class="fu">summary</span>(<span class="fu">coef</span>(mElasticOpt))[<span class="sc">-</span><span class="dv">1</span>,<span class="dv">3</span>]) <span class="sc">+</span></span>
<span id="cb66-10"><a href="#cb66-10" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;gene ID&quot;</span>) <span class="sc">+</span></span>
<span id="cb66-11"><a href="#cb66-11" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;beta-hat&quot;</span>) <span class="sc">+</span></span>
<span id="cb66-12"><a href="#cb66-12" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-43-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="acknowledgement" class="section level1 unnumbered">
<h1>Acknowledgement</h1>
<ul>
<li>Olivier Thas for sharing his materials of Analysis of High Dimensional Data 2019-2020, which I used as the starting point for this chapter.</li>
</ul>
</div>
<div id="session-info" class="section level1 unnumbered">
<h1>Session info</h1>
<details>
<summary>
Session info
</summary>
<pre><code>#&gt; [1] &quot;2025-10-20 15:42:49 CEST&quot;</code></pre>
<pre><code>#&gt; ─ Session info ───────────────────────────────────────────────────────────────
#&gt;  setting  value
#&gt;  version  R version 4.4.0 RC (2024-04-16 r86468)
#&gt;  os       macOS 15.6
#&gt;  system   aarch64, darwin20
#&gt;  ui       X11
#&gt;  language (EN)
#&gt;  collate  en_US.UTF-8
#&gt;  ctype    en_US.UTF-8
#&gt;  tz       Europe/Brussels
#&gt;  date     2025-10-20
#&gt;  pandoc   3.4 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/aarch64/ (via rmarkdown)
#&gt; 
#&gt; ─ Packages ───────────────────────────────────────────────────────────────────
#&gt;  package           * version    date (UTC) lib source
#&gt;  AIMS              * 1.36.0     2024-04-30 [1] Bioconductor 3.19 (R 4.4.0)
#&gt;  AnnotationDbi       1.66.0     2024-05-01 [1] Bioconductor 3.19 (R 4.4.0)
#&gt;  Biobase           * 2.64.0     2024-04-30 [1] Bioconductor 3.19 (R 4.4.0)
#&gt;  BiocFileCache       2.12.0     2024-04-30 [1] Bioconductor 3.19 (R 4.4.0)
#&gt;  BiocGenerics      * 0.54.0     2025-04-15 [1] Bioconductor 3.21 (R 4.4.0)
#&gt;  biomaRt           * 2.60.1     2024-06-26 [1] Bioconductor 3.19 (R 4.4.0)
#&gt;  Biostrings          2.72.1     2024-06-02 [1] Bioconductor 3.19 (R 4.4.0)
#&gt;  bit                 4.5.0      2024-09-20 [1] CRAN (R 4.4.1)
#&gt;  bit64               4.5.2      2024-09-22 [1] CRAN (R 4.4.1)
#&gt;  blob                1.2.4      2023-03-17 [1] CRAN (R 4.4.0)
#&gt;  bookdown            0.40       2024-07-02 [1] CRAN (R 4.4.0)
#&gt;  bootstrap           2019.6     2019-06-17 [1] CRAN (R 4.4.0)
#&gt;  breastCancerMAINZ * 1.42.0     2024-05-02 [1] Bioconductor 3.19 (R 4.4.0)
#&gt;  bslib               0.8.0      2024-07-29 [1] CRAN (R 4.4.0)
#&gt;  cachem              1.1.0      2024-05-16 [1] CRAN (R 4.4.0)
#&gt;  class               7.3-22     2023-05-03 [1] CRAN (R 4.4.0)
#&gt;  cli                 3.6.3      2024-06-21 [1] CRAN (R 4.4.0)
#&gt;  cluster             2.1.6      2023-12-01 [1] CRAN (R 4.4.0)
#&gt;  codetools           0.2-20     2024-03-31 [1] CRAN (R 4.4.0)
#&gt;  colorspace          2.1-1      2024-07-26 [1] CRAN (R 4.4.0)
#&gt;  crayon              1.5.3      2024-06-20 [1] CRAN (R 4.4.0)
#&gt;  curl                5.2.3      2024-09-20 [1] CRAN (R 4.4.1)
#&gt;  DAAG              * 1.25.6     2024-05-26 [1] CRAN (R 4.4.0)
#&gt;  data.table          1.17.6     2025-06-17 [1] CRAN (R 4.4.1)
#&gt;  DBI                 1.2.3      2024-06-02 [1] CRAN (R 4.4.0)
#&gt;  dbplyr              2.5.0      2024-03-19 [1] CRAN (R 4.4.0)
#&gt;  deldir              2.0-4      2024-02-28 [1] CRAN (R 4.4.0)
#&gt;  digest              0.6.37     2024-08-19 [1] CRAN (R 4.4.1)
#&gt;  dplyr             * 1.1.4      2023-11-17 [1] CRAN (R 4.4.0)
#&gt;  e1071             * 1.7-16     2024-09-16 [1] CRAN (R 4.4.1)
#&gt;  evaluate            1.0.0      2024-09-17 [1] CRAN (R 4.4.1)
#&gt;  fansi               1.0.6      2023-12-08 [1] CRAN (R 4.4.0)
#&gt;  farver              2.1.2      2024-05-13 [1] CRAN (R 4.4.0)
#&gt;  fastmap             1.2.0      2024-05-15 [1] CRAN (R 4.4.0)
#&gt;  filelock            1.0.3      2023-12-11 [1] CRAN (R 4.4.0)
#&gt;  forcats           * 1.0.0      2023-01-29 [1] CRAN (R 4.4.0)
#&gt;  foreach             1.5.2      2022-02-02 [1] CRAN (R 4.4.0)
#&gt;  future              1.34.0     2024-07-29 [1] CRAN (R 4.4.0)
#&gt;  future.apply        1.11.2     2024-03-28 [1] CRAN (R 4.4.0)
#&gt;  genefu            * 2.36.0     2024-04-30 [1] Bioconductor 3.19 (R 4.4.0)
#&gt;  generics          * 0.1.3      2022-07-05 [1] CRAN (R 4.4.0)
#&gt;  GenomeInfoDb        1.40.1     2024-06-16 [1] Bioconductor 3.19 (R 4.4.0)
#&gt;  GenomeInfoDbData    1.2.12     2024-04-24 [1] Bioconductor
#&gt;  ggforce           * 0.4.2      2024-02-19 [1] CRAN (R 4.4.0)
#&gt;  ggplot2           * 3.5.1      2024-04-23 [1] CRAN (R 4.4.0)
#&gt;  glmnet            * 4.1-8      2023-08-22 [1] CRAN (R 4.4.0)
#&gt;  globals             0.16.3     2024-03-08 [1] CRAN (R 4.4.0)
#&gt;  glue                1.8.0      2024-09-30 [1] CRAN (R 4.4.1)
#&gt;  gridExtra         * 2.3        2017-09-09 [1] CRAN (R 4.4.0)
#&gt;  gtable              0.3.5      2024-04-22 [1] CRAN (R 4.4.0)
#&gt;  highr               0.11       2024-05-26 [1] CRAN (R 4.4.0)
#&gt;  hms                 1.1.3      2023-03-21 [1] CRAN (R 4.4.0)
#&gt;  htmltools           0.5.8.1    2024-04-04 [1] CRAN (R 4.4.0)
#&gt;  httr                1.4.7      2023-08-15 [1] CRAN (R 4.4.0)
#&gt;  httr2               1.0.5      2024-09-26 [1] CRAN (R 4.4.1)
#&gt;  iC10              * 2.0.2      2024-07-19 [1] CRAN (R 4.4.0)
#&gt;  iC10TrainingData    2.0.1      2024-07-16 [1] CRAN (R 4.4.0)
#&gt;  impute              1.78.0     2024-04-30 [1] Bioconductor 3.19 (R 4.4.0)
#&gt;  interp              1.1-6      2024-01-26 [1] CRAN (R 4.4.0)
#&gt;  IRanges             2.38.1     2024-07-03 [1] Bioconductor 3.19 (R 4.4.0)
#&gt;  iterators           1.0.14     2022-02-05 [1] CRAN (R 4.4.0)
#&gt;  jpeg                0.1-10     2022-11-29 [1] CRAN (R 4.4.0)
#&gt;  jquerylib           0.1.4      2021-04-26 [1] CRAN (R 4.4.0)
#&gt;  jsonlite            1.8.9      2024-09-20 [1] CRAN (R 4.4.1)
#&gt;  KEGGREST            1.44.1     2024-06-19 [1] Bioconductor 3.19 (R 4.4.0)
#&gt;  KernSmooth          2.23-24    2024-05-17 [1] CRAN (R 4.4.0)
#&gt;  knitr               1.48       2024-07-07 [1] CRAN (R 4.4.0)
#&gt;  labeling            0.4.3      2023-08-29 [1] CRAN (R 4.4.0)
#&gt;  latex2exp         * 0.9.6      2022-11-28 [1] CRAN (R 4.4.0)
#&gt;  lattice             0.22-6     2024-03-20 [1] CRAN (R 4.4.0)
#&gt;  latticeExtra        0.6-30     2022-07-04 [1] CRAN (R 4.4.0)
#&gt;  lava                1.8.0      2024-03-05 [1] CRAN (R 4.4.0)
#&gt;  lifecycle           1.0.4      2023-11-07 [1] CRAN (R 4.4.0)
#&gt;  limma               3.60.6     2024-10-02 [1] Bioconductor 3.19 (R 4.4.0)
#&gt;  listenv             0.9.1      2024-01-29 [1] CRAN (R 4.4.0)
#&gt;  lubridate         * 1.9.3      2023-09-27 [1] CRAN (R 4.4.0)
#&gt;  magrittr            2.0.3      2022-03-30 [1] CRAN (R 4.4.0)
#&gt;  MASS                7.3-61     2024-06-13 [1] CRAN (R 4.4.0)
#&gt;  Matrix            * 1.7-0      2024-03-22 [1] CRAN (R 4.4.0)
#&gt;  mclust              6.1.1      2024-04-29 [1] CRAN (R 4.4.0)
#&gt;  memoise             2.0.1      2021-11-26 [1] CRAN (R 4.4.0)
#&gt;  mgcv              * 1.9-1      2023-12-21 [1] CRAN (R 4.4.0)
#&gt;  munsell             0.5.1      2024-04-01 [1] CRAN (R 4.4.0)
#&gt;  nlme              * 3.1-166    2024-08-14 [1] CRAN (R 4.4.0)
#&gt;  pamr                1.57       2024-07-01 [1] CRAN (R 4.4.0)
#&gt;  parallelly          1.38.0     2024-07-27 [1] CRAN (R 4.4.0)
#&gt;  pillar              1.9.0      2023-03-22 [1] CRAN (R 4.4.0)
#&gt;  pkgconfig           2.0.3      2019-09-22 [1] CRAN (R 4.4.0)
#&gt;  plotROC           * 2.3.1      2023-10-06 [1] CRAN (R 4.4.0)
#&gt;  plyr                1.8.9      2023-10-02 [1] CRAN (R 4.4.0)
#&gt;  png                 0.1-8      2022-11-29 [1] CRAN (R 4.4.0)
#&gt;  polyclip            1.10-7     2024-07-23 [1] CRAN (R 4.4.0)
#&gt;  prettyunits         1.2.0      2023-09-24 [1] CRAN (R 4.4.0)
#&gt;  prodlim           * 2024.06.25 2024-06-24 [1] CRAN (R 4.4.0)
#&gt;  progress            1.2.3      2023-12-06 [1] CRAN (R 4.4.0)
#&gt;  proxy               0.4-27     2022-06-09 [1] CRAN (R 4.4.0)
#&gt;  purrr             * 1.0.2      2023-08-10 [1] CRAN (R 4.4.0)
#&gt;  R6                  2.5.1      2021-08-19 [1] CRAN (R 4.4.0)
#&gt;  rappdirs            0.3.3      2021-01-31 [1] CRAN (R 4.4.0)
#&gt;  rbibutils           2.2.16     2023-10-25 [1] CRAN (R 4.4.0)
#&gt;  RColorBrewer        1.1-3      2022-04-03 [1] CRAN (R 4.4.0)
#&gt;  Rcpp                1.0.13-1   2024-11-02 [1] CRAN (R 4.4.1)
#&gt;  Rdpack              2.6.1      2024-08-06 [1] CRAN (R 4.4.0)
#&gt;  readr             * 2.1.5      2024-01-10 [1] CRAN (R 4.4.0)
#&gt;  rlang               1.1.4      2024-06-04 [1] CRAN (R 4.4.0)
#&gt;  rmarkdown           2.28       2024-08-17 [1] CRAN (R 4.4.0)
#&gt;  rmeta               3.0        2018-03-20 [1] CRAN (R 4.4.0)
#&gt;  RSQLite             2.3.7      2024-05-27 [1] CRAN (R 4.4.0)
#&gt;  rstudioapi          0.16.0     2024-03-24 [1] CRAN (R 4.4.0)
#&gt;  S4Vectors           0.42.1     2024-07-03 [1] Bioconductor 3.19 (R 4.4.0)
#&gt;  sass                0.4.9      2024-03-15 [1] CRAN (R 4.4.0)
#&gt;  scales              1.3.0      2023-11-28 [1] CRAN (R 4.4.0)
#&gt;  SemiPar           * 1.0-4.2    2018-04-16 [1] CRAN (R 4.4.0)
#&gt;  sessioninfo         1.2.2      2021-12-06 [1] CRAN (R 4.4.0)
#&gt;  shape               1.4.6.1    2024-02-23 [1] CRAN (R 4.4.0)
#&gt;  statmod             1.5.0      2023-01-06 [1] CRAN (R 4.4.0)
#&gt;  stringi             1.8.4      2024-05-06 [1] CRAN (R 4.4.0)
#&gt;  stringr           * 1.5.1      2023-11-14 [1] CRAN (R 4.4.0)
#&gt;  SuppDists           1.1-9.8    2024-09-03 [1] CRAN (R 4.4.1)
#&gt;  survcomp          * 1.54.0     2024-04-30 [1] Bioconductor 3.19 (R 4.4.0)
#&gt;  survival          * 3.7-0      2024-06-05 [1] CRAN (R 4.4.0)
#&gt;  survivalROC         1.0.3.1    2022-12-05 [1] CRAN (R 4.4.0)
#&gt;  tibble            * 3.2.1      2023-03-20 [1] CRAN (R 4.4.0)
#&gt;  tidyr             * 1.3.1      2024-01-24 [1] CRAN (R 4.4.0)
#&gt;  tidyselect          1.2.1      2024-03-11 [1] CRAN (R 4.4.0)
#&gt;  tidyverse         * 2.0.0      2023-02-22 [1] CRAN (R 4.4.0)
#&gt;  timechange          0.3.0      2024-01-18 [1] CRAN (R 4.4.0)
#&gt;  tweenr              2.0.3      2024-02-26 [1] CRAN (R 4.4.0)
#&gt;  tzdb                0.4.0      2023-05-12 [1] CRAN (R 4.4.0)
#&gt;  UCSC.utils          1.0.0      2024-05-06 [1] Bioconductor 3.19 (R 4.4.0)
#&gt;  utf8                1.2.4      2023-10-22 [1] CRAN (R 4.4.0)
#&gt;  vctrs               0.6.5      2023-12-01 [1] CRAN (R 4.4.0)
#&gt;  vroom               1.6.5      2023-12-05 [1] CRAN (R 4.4.0)
#&gt;  withr               3.0.1      2024-07-31 [1] CRAN (R 4.4.0)
#&gt;  xfun                0.47       2024-08-17 [1] CRAN (R 4.4.0)
#&gt;  xml2                1.3.6      2023-12-04 [1] CRAN (R 4.4.0)
#&gt;  XVector             0.44.0     2024-04-30 [1] Bioconductor 3.19 (R 4.4.0)
#&gt;  yaml                2.3.10     2024-07-26 [1] CRAN (R 4.4.0)
#&gt;  zlibbioc            1.50.0     2024-04-30 [1] Bioconductor 3.19 (R 4.4.0)
#&gt; 
#&gt;  [1] /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library
#&gt; 
#&gt; ──────────────────────────────────────────────────────────────────────────────</code></pre>
</details>
</div>

<div id="rmd-source-code">---
title: "3. Prediction with High Dimensional Predictors"
author: "Lieven Clement"
date: "statOmics, Ghent University (https://statomics.github.io)"
output:
  bookdown::pdf_document2:
    toc: true
    number_sections: true
    latex_engine: xelatex
always_allow_html: true
---

```{r, child="_setup.Rmd"}
```

```{r echo=FALSE, message= FALSE}
library(tidyverse)
library(gridExtra)
```

# Introduction

## Prediction with High Dimensional Predictors

General setting:

-   Aim: build a **prediction model** that gives a prediction of an outcome for a given set of predictors.

- We use $X$ to refer to the predictors and $Y$ to refer to the outcome.


- A **training data set** is available, say $(\mathbf{X},\mathbf{Y})$. It contains $n$ observations on outcomes and on $p$ predictors.

- Using the training data, a prediction model is build, say $\hat{m}(\mathbf{X})$. This typically involves **model building (feature selection)** and parameter estimation.


-   During the model building, potential **models need to be evaluated** in terms of their prediction quality.

## Example: Toxicogenomics in early drug development

### Background

- Effect of compound on gene expression.

- Insight in action and toxicity of drug in early phase
- Determine activity with bio-assay: e.g. binding affinity of compound to cell wall receptor (target, IC50).
- Early phase:  20 to 50 compounds
- Based on in vitro results one aims to get insight in how to build better compound (higher on-target activity less toxicity.
- Small variations in molecular structure lead to variations in BA and gene expression.
- Aim: Build model to predict bio-activity based on gene expression in liver cell line.

### Data

- 30 chemical compounds have been screened for toxicity

- Bioassay data on toxicity screening

- Gene expressions in a liver cell line are profiled for each compound (4000 genes)


```{r}
toxData <- read_csv(
  "https://raw.githubusercontent.com/statOmics/HDA2020/data/toxDataCentered.csv",
  col_types = cols()
)
svdX <- svd(toxData[,-1])
```

Data is already centered:

```{r}
toxData %>%
  colMeans %>%
  range
```

```{r}
 toxData %>%
  names %>%
  head
```

- First column contains data on Bioassay.
- The higher the score on Bioassay the more toxic the compound
- Other columns contain data on gene expression X1, ... , X4000

### Data exploration

```{r}
toxData %>%
  ggplot(aes(x="",y=BA)) +
  geom_boxplot(outlier.shape=NA) +
  geom_point(position="jitter")
```

```{r}
svdX <- toxData[,-1] %>%
  svd

k <- 2
Vk <- svdX$v[,1:k]
Uk <- svdX$u[,1:k]
Dk <- diag(svdX$d[1:k])
Zk <- Uk%*%Dk
colnames(Zk) <- paste0("Z",1:k)
colnames(Vk) <- paste0("V",1:k)

Zk %>%
  as.data.frame %>%
  mutate(BA = toxData %>% pull(BA)) %>%
  ggplot(aes(x= Z1, y = Z2, color = BA)) +
  geom_point(size = 3) +
  scale_colour_gradient2(low = "blue",mid="white",high="red") +
  geom_point(size = 3, pch = 21, color = "black")
```

- Scores on the first two principal components (or MDS plot).
- Each point corresponds to a compound.
- Color code refers to the toxicity score (higher score more toxic).
- Clear separation between compounds according to toxicity.

---

- Next logic step in a PCA is to interpret the principal components.
- We thus have to assess the loadings.
- We can add a vector for each gene to get a biplot, but this would require plotting 4000 vectors, which would render the plot unreadable.

Alternative graph to look at the many loadings of the first two PCs.

```{r}
grid.arrange(
  Vk %>%
    as.data.frame %>%
    mutate(geneID = 1:nrow(Vk)) %>%
    ggplot(aes(x = geneID, y = V1)) +
    geom_point(pch=21) +
    geom_hline(yintercept = c(-2,0,2)*sd(Vk[,1]), col = "red") ,
  Vk %>%
    as.data.frame %>%
    mutate(geneID = 1:nrow(Vk)) %>%
    ggplot(aes(x = geneID, y = V2)) +
    geom_point(pch=21) +
    geom_hline(yintercept = c(-2,0,2)*sd(Vk[,2]), col = "red"),
  ncol=2)
```

- It is almost impossible to interpret the PCs because there are 4000 genes contributing to each PC.

- In an attempt to find the most important genes (in the sense that they drive the interpretation of the PCs), the plots show horizontal reference lines: the average of the loadings, and the average ± twice the standard deviation of the loadings. In between the lines we expects about 95% of the loadings (if they were normally distributed).

- The points outside the band come from the genes that have rather large loadings (in absolute value) and hence are important for the interpretation of the PCs.

- Note, that particularly for the first PC, only a few genes show a markedly large loadings that are negative. This means that an upregulation of these genes will lead to low scores on PC1.
- These genes will very likely play an important role in the toxicity mechanism.
- Indeed, low scores on PC1 are in the direction of more toxicity.
- In the next chapter we will introduce a method to obtain sparse PCs.

### Prediction model

```{r}
m1 <- lm(BA ~ -1 + ., toxData)

m1 %>%
  coef %>%
  head(40)

m1 %>%
  coef %>%
  is.na %>%
  sum

summary(m1)$r.squared
```

Problem??

## Brain example

- Courtesy to Solomon Kurz. Statistical rethinking with brms, ggplot2, and the tidyverse version 1.2.0.

https://bookdown.org/content/3890/
https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse

- Data with brain size and body size for seven species

```{r}
brain <-
tibble(species = c("afarensis", "africanus", "habilis", "boisei", "rudolfensis", "ergaster", "sapiens"),
       brain   = c(438, 452, 612, 521, 752, 871, 1350),
       mass    = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5))
```

### Data exploration

```{r}
brain

p <- brain %>%
  ggplot(aes(x =  mass, y = brain, label = species)) +
  geom_point()

p + geom_text(nudge_y = 40)
```

### Models

Six models range in complexity from the simple univariate model

\begin{align*}
\text{brain}_i & \sim \operatorname{Normal} (\mu_i, \sigma) \\
\mu_i & = \beta_0 + \beta_1 \text{mass}_i,
\end{align*}

to the dizzying sixth-degree polynomial model

\begin{align*}
\text{brain}_i & \sim \operatorname{Normal} (\mu_i, \sigma) \\
\mu_i & = \beta_0 + \beta_1 \text{mass}_i + \beta_2 \text{mass}_i^2 + \beta_3 \text{mass}_i^3 + \beta_4 \text{mass}_i^4 + \beta_5 \text{mass}_i^5 + \beta_6 \text{mass}_i^6.
\end{align*}

```{r, message = F, warning = F}
formulas <- sapply(1:6, function(i)
  return(
     paste0("I(mass^",1:i,")") %>% paste(collapse=" + ")
    )
)

formulas <- sapply(
  paste0("brain ~ ", formulas),
  as.formula)

models <- lapply(formulas, lm , data = brain)
```

```{r}
data.frame(
  formula=formulas %>%
    as.character,
  r2 = sapply(
    models,
    function(mod) summary(mod)$r.squared)
  )  %>%
  ggplot(
    aes(x = r2,
      y = formula,
      label = r2 %>%
        round(2) %>%
        as.character)
  ) +
  geom_text()
```

We plot the fit for each model individually and them arrange them together in one plot.

```{r}
plots <- lapply(1:6, function(i)
{
  p +
  geom_smooth(method = "lm", formula = y ~ poly(x,i)) +
  ggtitle(
    paste0(
      "r2 = ",
      round(summary(models[[i]])$r.squared*100,1),
      "%")
    )
})

do.call("grid.arrange",c(plots, ncol = 3))
```

- We clearly see that increasing the model complexity always produces a fit with a smaller SSE.
- The problem of overfitting is very obvious. The more complex polynomial models will not generalise well for prediction!
- We even have a model that fits the data perfectly, but that will make very absurd preditions!

- Too few parameters hurts, too. Fit the underfit intercept-only model.

```{r}
m0 <- lm(brain ~ 1, brain)
summary(m0)

p +
  stat_smooth(method = "lm", formula = y ~ 1) +
  ggtitle(
    paste0(
      "r2 = ",
      round(summary(m0)$r.squared*100,1),
      "%")
    )
```

The underfit model did not learn anything about the relation between mass and brain. It would also do a very poor job for predicting new data.

## Overview

We will make a distinction between continuous and discrete outcomes. In this course we focus on

- Linear regression models for continous outcomes

  - Penalised regression: Lasso and ridge
  - Principal component regression (PCR)

- Logistic regression models for binary outcomes

  - Penalised regression: Lasso and ridge

For all types of model, we will discuss feature selection methods.

# Linear Regression for High Dimensional Data

Consider linear regression model (for double centered data)
\[
  Y_i = \beta_1X_{i1} + \beta_2 X_{i2} + \cdots + \beta_pX_{ip} + \epsilon_i ,
\]
with $\text{E}\left[\epsilon \mid \mathbf{X}\right]=0$ and $\text{var}\left[\epsilon \mid \mathbf{X}\right]=\sigma^2$.

In matrix notation the model becomes
\[
  \mathbf{Y} = \mathbf{X}\mathbf\beta + \mathbf\epsilon.
\]
The least squares estimator of $\mathbf\beta$ is given by
\[
  \hat{\mathbf\beta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} ,
\]
and the variance of $\hat{\mathbf\beta}$ equals
\[
  \text{var}\left[\hat{\mathbf\beta}\right] = (\mathbf{X}^T\mathbf{X})^{-1}\sigma^2.
\]
$\longrightarrow$ the $p \times p$ matrix $(\mathbf{X}^T\mathbf{X})^{-1}$ is crucial

Note, that

- with double centered data it is meant that both the responses are centered (mean of $\mathbf{Y}$ is zero) and that all predictors are centered (columns of $\mathbf{X}$ have zero mean). With double centered data the intercept in a linear regression model is always exactly equal to zero and hence the intercept must not be included in the model.

- we do not assume that the residuals are normally distributed. For prediction purposes this is often not required (normality is particularly important for statistical inference in small samples).

## Linear Regression for multivariate data vs High Dimensional Data

- $\mathbf{X^TX}$ and $(\mathbf{X^TX})^{-1}$ are $p \times p$ matrices

- $\mathbf{X^TX}$ can only be inverted if it has rank $p$

- Rank of a matrix of form $\mathbf{X^TX}$, with $\mathbf{X}$ and $n\times p$ matrix, can never be larger than $\min(n,p)$.

- in most regression problems $n>p$ and rank of $(\mathbf{X^TX})$ equals $p$

- in high dimensional regression problems $p >>> n$ and rank of $(\mathbf{X^TX})$ equals $n<p$

- in the toxicogenomics example $n=30<p=4000$ and $\text{rank}(\mathbf{X^TX})\leq n=30$.
  $\longrightarrow$ $(\mathbf{X^TX})^{-1}$ does not exist, and neither does $\hat{\boldsymbol{\beta}}$.

## Can SVD help?
  - Since the columns of $\mathbf{X}$ are centered, $\mathbf{X^TX} \propto \text{var}\left[\mathbf{X}\right]$.

  - if $\text{rank}(\mathbf{X^TX})=n=30$, the PCA will give 30 components, each being a linear combination of $p=4000$ variables. These 30 PCs contain all information present in the original $\mathbf{X}$ data.

  - if $\text{rank}(\mathbf{X})=n=30$, the SVD of $\mathbf{X}$ is given by
  \[
   \mathbf{X} = \sum_{i=1}^n \delta_i \mathbf{u}_i \mathbf{v}_i^T = \mathbf{U} \boldsymbol{\Delta} \mathbf{V}^T = \mathbf{ZV}^T,
  \]
  with $\mathbf{Z}$ the $n\times n$ matrix with the scores on the $n$ PCs.

  - Still problematic because if we use all PCs $n=p$.


# Principal Component Regression

A principal component regression (PCR) consists of

1. transforming $p=4000$ dimensional $X$-variable to the $n=30$ dimensional $Z$-variable (PC scores). The $n$ PCs are mutually uncorrelated.

2. using the $n$ PC-variables as regressors in a linear regression model

3. performing feature selection to select the most important regressors (PC).

Feature selection is key, because we don't want to have as many regressors as there are observations in the data. This would result in zero residual degrees of freedom. (see later)

---

To keep the exposition general so that we allow for a feature selection to have taken place, I use the notation $\mathbf{U}_S$ to denote a matrix with left-singular column vectors $\mathbf{u}_i$, with $i \in {\mathcal{S}}$ (${\mathcal{S}}$ an index set referring to the PCs to be included in the regression model).

For example, suppose that a feature selection method has resulted in the selection of PCs 1, 3 and 12 for inclusion in the prediction model, then ${\mathcal{S}}=\{1,3,12\}$ and
\[
 \mathbf{U}_S = \begin{pmatrix}
  \mathbf{u}_1 & \mathbf{u}_3 & \mathbf{u}_{12}
 \end{pmatrix}.
\]

---

### Example model based on first 4 PCs

```{r}
k <- 30
Uk <- svdX$u[,1:k]
Dk <- diag(svdX$d[1:k])
Zk <- Uk%*%Dk
Y <- toxData %>%
  pull(BA)

m4 <- lm(Y~Zk[,1:4])
summary(m4)
```

Note:

- the intercept is estimated as zero. (Why?) The model could have been fitted as

```
m4 <- lm(Y~-1+Zk[,1:4])
```

- the PC-predictors are uncorrelated (by construction)

- first PC-predictors are not necessarily the most important predictors

- $p$-values are not very meaningful when prediction is the objective

Methods for feature selection will be discussed later.

# Ridge Regression

## Penalty

 The ridge parameter estimator is defined as the parameter $\mathbf\beta$ that minimises the **penalised least squares criterion**

 \[
 \text{SSE}_\text{pen}=\Vert\mathbf{Y} - \mathbf{X\beta}\Vert_2^2 + \lambda \Vert \boldsymbol{\beta} \Vert_2^2
\]

- $\Vert \boldsymbol{\beta} \Vert_2^2=\sum_{j=1}^p \beta_j^2$ is the **$L_2$ penalty term**

- $\lambda>0$ is the penalty parameter (to be chosen by the user).

Note, that that is equivalent to minimizing
\[
\Vert\mathbf{Y} - \mathbf{X\beta}\Vert_2^2 \text{ subject to } \Vert \boldsymbol{\beta}\Vert^2_2\leq s
\]

Note, that $s$ has a one-to-one correspondence with $\lambda$

## Graphical interpretation

```{r echo = FALSE, warning = FALSE, message = FALSE}
library(ggforce)
library(latex2exp)
library(gridExtra)

p1 <- ggplot() +
  geom_ellipse(aes(x0 = 4, y0 = 11, a = 10, b = 3, angle = pi / 4)) +
  geom_ellipse(aes(x0 = 4, y0 = 11, a = 5, b = 1.5, angle = pi / 4)) +
  xlim(-12.5, 12.5) +
  ylim(-5, 20) +
  geom_point(aes(x = 4, y = 11)) +
  annotate("text", label = TeX("$(\\hat{\\beta}_1^{ols}, \\hat{\\beta}_2^{ols})$"), x = -5, y = 15, size = 6, parse = TRUE) +
  xlab(TeX("$\\beta_1$")) +
  ylab(TeX("$\\beta_2$")) +
  geom_segment(
    aes(x = -5, y = 12.5, xend = 3.7, yend = 11.3),
    arrow = arrow(length = unit(0.25, "cm"))
    ) +
  coord_fixed()

pRidge <- p1 +
  geom_circle(aes(x0 = 0, y0 = 0, r = 3.9) , color = "red") +
  geom_point(aes(x = -1.1, y = 3.75), color = "red") +
  annotate("text", label = TeX("$(\\hat{\\beta}_1^{ridge}, \\hat{\\beta}_2^{ridge})$"), x = -8.1, y = 4.45, size = 6, parse = TRUE, color = "red") +
  ggtitle("Ridge") +
  geom_vline(xintercept = 0, color = "grey") +
  geom_hline(yintercept = 0, color = "grey") +
  theme_minimal()

pRidge
```

## Solution

The solution is given by
\[
  \hat{\boldsymbol{\beta}} = (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} \mathbf{X^T Y}.
\]
It can be shown that $(\mathbf{X^TX}+\lambda \mathbf{I})$ is always of rank $p$ if $\lambda>0$.

Hence, $(\mathbf{X^TX}+\lambda \mathbf{I})$ is invertible and $\hat{\boldsymbol{\beta}}$ exists even if $p>>>n$.

We also find
\[
  \text{var}\left[\hat{\mathbf\beta}\right] = (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} \mathbf{X}^T\mathbf{X} (\mathbf{X^TX}+\lambda \mathbf{I})^{-1}\sigma^2
\]

However, it can be shown that improved intervals that also account for the bias can be constructed by using:

\[
  \text{var}\left[\hat{\mathbf\beta}\right] = (\mathbf{X^TX}+\lambda \mathbf{I})^{-1}  \sigma^2.
\]

### Proof

The criterion to be minimised is
  \[
   \text{SSE}_\text{pen}=\Vert\mathbf{Y} - \mathbf{X\beta}\Vert_2^2 + \lambda \Vert \boldsymbol{\beta} \Vert_2^2.
 \]
 First we re-express SSE in matrix notation:
 \[
   \text{SSE}_\text{pen} = (\mathbf{Y}-\mathbf{X\beta})^T(\mathbf{Y}-\mathbf{X\beta}) + \lambda \boldsymbol{\beta}^T\boldsymbol{\beta}.
 \]
 The partial derivative w.r.t. $\boldsymbol{\beta}$ is
 \[
   \frac{\partial}{\partial \boldsymbol{\beta}}\text{SSE}_\text{pen} = -2\mathbf{X}^T(\mathbf{Y}-\mathbf{X\beta})+2\lambda\boldsymbol{\beta}.
 \]
 Solving $\frac{\partial}{\partial \boldsymbol{\beta}}\text{SSE}_\text{pen}=0$ gives
 \[
   \hat{\boldsymbol{\beta}} = (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} \mathbf{X^T Y}.
 \]
 (assumption: $(\mathbf{X^TX}+\lambda \mathbf{I})$ is of rank $p$. This is always true if $\lambda>0$)

## Link with SVD

### SVD and inverse
Write the SVD of $\mathbf{X}$ ($p>n$) as
\[
   \mathbf{X} = \sum_{i=1}^n \delta_i \mathbf{u}_i \mathbf{v}_i^T = \sum_{i=1}^p \delta_i \mathbf{u}_i \mathbf{v}_i^T  = \mathbf{U}\boldsymbol{\Delta} \mathbf{V}^T ,
\]
with

- $\delta_{n+1}=\delta_{n+2}= \cdots = \delta_p=0$

- $\boldsymbol{\Delta}$ a $p\times p$ diagonal matrix of the $\delta_1,\ldots, \delta_p$

-  $\mathbf{U}$ an $n\times p$ matrix and $\mathbf{V}$ a $p \times p$ matrix. Note that only the first $n$ columns of $\mathbf{U}$ and $\mathbf{V}$ are informative.

With the SVD of $\mathbf{X}$ we write
 \[
   \mathbf{X}^T\mathbf{X} = \mathbf{V}\boldsymbol{\Delta
     }^2\mathbf{V}^T.
 \]
 The inverse of $\mathbf{X}^T\mathbf{X}$ is then given by
 \[
   (\mathbf{X}^T\mathbf{X})^{-1} = \mathbf{V}\boldsymbol{\Delta}^{-2}\mathbf{V}^T.
 \]
 Since $\boldsymbol{\Delta}$ has $\delta_{n+1}=\delta_{n+2}= \cdots = \delta_p=0$, it is not invertible.

### SVD of penalised matrix $\mathbf{X^TX}+\lambda \mathbf{I}$

It can be shown that
\[
  \mathbf{X^TX}+\lambda \mathbf{I} = \mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I}) \mathbf{V}^T ,
\]
i.e. adding a constant to the diagonal elements does not affect the eigenvectors, and all eigenvalues are increased by this constant.
$\longrightarrow$ zero eigenvalues become $\lambda$.

Hence,
\[
  (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} = \mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I})^{-1} \mathbf{V}^T ,
\]
which can be computed even when some eigenvalues in $\boldsymbol{\Delta}^2$ are zero.

Note, that for high dimensional data ($p>>>n$) many eigenvalues are zero because $\mathbf{X^TX}$ is a $p \times p$ matrix and has rank $n$.

The identity $\mathbf{X^TX}+\lambda \mathbf{I} = \mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I}) \mathbf{V}^T$ is easily checked:
\[
  \mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I}) \mathbf{V}^T = \mathbf{V}\boldsymbol{\Delta}^2\mathbf{V}^T + \lambda \mathbf{VV}^T  = \mathbf{V}\boldsymbol{\Delta}^2\mathbf{V}^T + \lambda \mathbf{I} = \mathbf{X^TX}+\lambda \mathbf{I}.
\]


## Properties

- The Ridge estimator is biased! The $\boldsymbol{\beta}$ are shrunken to zero!
\begin{eqnarray}
 \text{E}[\hat{\boldsymbol{\beta}}] &=& (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} \mathbf{X}^T \text{E}[\mathbf{Y}]\\
&=& (\mathbf{X}^T\mathbf{X}+\lambda \mathbf{I})^{-1} \mathbf{X}^T \mathbf{X}\boldsymbol{\beta}\\
\end{eqnarray}

- Note, that the shrinkage is larger in the direction of the smaller eigenvalues.

\begin{eqnarray}
\text{E}[\hat{\boldsymbol{\beta}}]&=&\mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I})^{-1} \mathbf{V}^T \mathbf{V} \boldsymbol{\Delta}^2 \mathbf{V}^T\boldsymbol{\beta}\\
&=&\mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I})^{-1} \boldsymbol{\Delta}^2 \mathbf{V}^T\boldsymbol{\beta}\\
&=& \mathbf{V}
\left[\begin{array}{ccc}
\frac{\delta_1^2}{\delta_1^2+\lambda}&\ldots&0 \\
&\vdots&\\
0&\ldots&\frac{\delta_r^2}{\delta_r^2+\lambda}
\end{array}\right]
\mathbf{V}^T\boldsymbol{\beta}
\end{eqnarray}

-  the variance of the prediction $\hat{{Y}}(\mathbf{x})=\mathbf{x}^T\hat\beta$,
  \[
    \text{var}\left[\hat{{Y}}(\mathbf{x})\mid \mathbf{x}\right] = \mathbf{x}^T(\mathbf{X^TX}+\lambda \mathbf{I})^{-1}\mathbf{x}
  \]
  is smaller than with the least-squares estimator.

-  through the bias-variance trade-off it is hoped that better predictions in terms of expected conditional test error can be obtained, for an appropriate choice of $\lambda$.


Recall the expression of the expected conditional test error
\begin{eqnarray}
  Err(\mathbf{x}) &=& \text{E}\left[(\hat{Y} - Y^*)^2\mid \mathbf{x}\right]\\
  &=&
  \text{var}\left[\hat{Y}\mid \mathbf{x}\right] + \text{bias}^2(\mathbf{x})+
  \text{var}\left[Y^*\mid \mathbf{x}\right]
\end{eqnarray}
where

- $\hat{Y}=\hat{Y}(\mathbf{x})=\mathbf{x}^T\hat{\boldsymbol{\beta}}$ is the prediction at $\mathbf{x}$
- $Y^*$ is an outcome at predictor $\mathbf{x}$
- $\mu(\mathbf{x}) = \text{E}\left[\hat{Y}\mid \mathbf{x}\right] \text{ and } \mu^*(x)=\text{E}\left[Y^*\mid \mathbf{x}\right]$
- $\text{bias}(\mathbf{x})=\mu(\mathbf{x})-\mu^*(\mathbf{x})$
- $\text{var}\left[Y^*\mid \mathbf{x}\right]$ the irreducible error that does not depend on the model. It simply originates from observations that randomly fluctuate around the true mean $\mu^*(x)$.

## Toxicogenomics example

```{r}
library(glmnet)
mRidge <- glmnet(
  x = toxData[,-1] %>%
    as.matrix,
  y = toxData %>%
    pull(BA),
  alpha = 0) # ridge: alpha = 0

plot(mRidge, xvar="lambda")
```


The R function \textsf{glmnet} uses \textsf{lambda} to refer to the penalty parameter. In this course we use $\lambda$, because $\lambda$ is often used as eigenvalues.

The graph shows that with increasing penalty parameter, the parameter estimates are shrunken towards zero. The estimates will only reach zero for $\lambda \rightarrow \infty$. The stronger the shrinkage, the larger the bias (towards zero) and the smaller the variance of the parameter estimators (and hence also smaller variance of the predictions).

Another (informal) viewpoint is the following. By shrinking the estimates towards zero, the estimates loose some of their ``degrees of freedom'' so that the parameters become estimable with only $n<p$ data points. Even with a very small $\lambda>0$, the parameters regain their estimability. However, note that the variance of the estimator is given by
\[
  \text{var}\left[\hat{\mathbf\beta}\right] = (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} \sigma^2 = \mathbf{V}(\boldsymbol{\Delta}^2+\lambda\mathbf{I})^{-1}\mathbf{V}^T\sigma^2.
\]
Hence, a small $\lambda$ will result in large variances of the parameter estimators. The larger $\lambda$, the smaller the variances become. In the limit, as $\lambda\rightarrow\infty$, the estimates are converged to zero and show no variability any longer.

# Lasso Regression

- The Lasso is another example of penalised regression.

- The lasso estimator of $\boldsymbol{\beta}$ is the solution to minimising the penalised SSE
\[
 \text{SSE}_\text{pen} = \sum_{i=1}^n (Y_i - \mathbf{x}_i^T\boldsymbol{\beta})^2 + \lambda \sum_{j=1}^p \vert \beta_j\vert.
\]


or, equivalently, minimising

\[
\text{SSE}  = \Vert \mathbf{Y} - \mathbf{X\beta}\Vert_2^2 \text{ subject to } \Vert \mathbf\beta\Vert_1 \leq c
\]
with

- $\Vert \mathbf\beta\Vert_1 = \sum\limits_{j=1}^p \vert \beta_j \vert$

- Despite strong similarity between ridge and lasso regression ($L_2$ versus $L_1$ norm in penalty term), there is no analytical solution of the lasso parameter estimator of $\mathbf\beta$.

- Fortunately, computational efficient algorithms have been implemented in statistical software

- The Lasso estimator of $\boldsymbol{\beta}$ is biased and generally has a smaller variance then the least-squares estimator.

- Hence, the bias-variance trade-off may here also help in finding better predictions with biased estimators.

- In contrast to ridge regression, however, the lasso estimator can give at most $\min(p,n)$ non-zero $\beta$-estimates.

- Hence, at first sight the lasso is not directly appropriate for high-dimensional settings.

- An important advantage of the lasso is that choosing an appropriate value for $\lambda$ is a kind a model building or feature selection procedure (see further).

## Graphical interpretation of Lasso vs ridge

Note that the lasso is a constrained regression problem with

\[
\Vert \mathbf{Y} - \mathbf{X\beta}\Vert_2^2 \text{ subject to } \Vert \mathbf\beta\Vert_1 \leq c
\]
and ridge
\[
\Vert \mathbf{Y} - \mathbf{X\beta}\Vert_2^2 \text{ subject to } \Vert \mathbf\beta\Vert^2_2 \leq c
\]

```{r echo = FALSE, warning = FALSE, message = FALSE}
pLasso <- p1 +
  geom_segment(aes(x = 0, y = 4.2 , xend = 4.2, yend = 0), color = "red") +
  geom_segment(aes(x = 0, y = 4.2 , xend = - 4.2, yend = 0), color = "red") +
  geom_segment(aes(x = 4.2, y = 0 , xend = 0, yend = -4.2), color = "red") +
  geom_segment(aes(x = 0, y = - 4.2 , xend = - 4.2, yend = 0), color = "red") +
  geom_point(aes(x = 0, y = 4.2), color = "red") +
  annotate("text", label = TeX("$(\\hat{\\beta}_1^{lasso}, \\hat{\\beta}_2^{lasso})$"), x = 7, y = 4.2, size = 6, parse = TRUE, color = "red") +
  ggtitle("Lasso") +
  geom_vline(xintercept = 0, color = "grey") +
  geom_hline(yintercept = 0, color = "grey") +
  theme_minimal()

grid.arrange(pLasso, pRidge, ncol = 2)
```

Note, that

- parameters for the lasso can never switch sign, they are set at zero! Selection!
- ridge regression can lead to parameters that switch sign.

## Toxicogenomics example

```{r}
mLasso <- glmnet(
  x = toxData[,-1] %>%
    as.matrix,
  y = toxData %>%
    pull(BA),
alpha = 1)
plot(mLasso, xvar = "lambda")
```

- The graph with the paths of the parameter estimates nicely illustrates the typical behaviour of the lasso estimates as a function of $\lambda$: when $\lambda$ increases the estimates are shrunken towards zero.

- When an estimate hits zero, it remains exactly equal to zero when $\gamma$ further increases. A parameter estimate equal to zero, say $\hat\beta_j=0$, implies that the corresponding predictor $x_j$ is no longer included in the model (i.e. $\beta_jx_j=0$).

- The model fit is known as a sparse model fit (many zeroes). Hence, choosing a appropriate value for $\gamma$ is like choosing the important predictors in the model (feature selection).


# Splines and the connection to ridge regression.

## Lidar dataset

- LIDAR (light detection and ranging) uses the reflection of laser-emitted light to detect chemical compounds in the atmosphere.
- The LIDAR technique has proven to be an efficient tool for monitoring the distribution of several atmospheric pollutants of importance; see Sigrist (1994).
- The range is the distance traveled before the light is reflected back to its source.
- The logratio is the logarithm of the ratio of received light from two laser sources.

  - One source had a frequency equal to the resonance frequency of the compound of interest, which was mercury in this study.
  - The other source had a frequency off this resonance frequency.

  - The concentration of mercury can be derived from a regression model of the logratio in function of  the range for each range x.

```{r}
library("SemiPar")
data(lidar)
pLidar <- lidar %>%
  ggplot(aes(x = range, y = logratio)) +
  geom_point() +
  xlab("range (m)")

pLidar +
  geom_smooth()
```

- The data is non-linear
- Linear regression will not work!
- The data shows a smooth relation between the logratio and the range

## Basis expansion

\[y_i=f(x_i)+\epsilon_i,\]
with
\[f(x)=\sum\limits_{k=1}^K \theta_k b_k(x)\]

-  Select set of basis functions $b_k(x)$
-  Select number of basis functions $K$
-  Examples

    -  Polynomial model: $x^k$
    -  Orthogonal series: Fourier, Legendre polynomials, Wavelets
    -  Polynomial splines: $1, x, (x-t_m)_+$ with $m=1, \ldots, K-2$ knots $t_m$
    -  ...

### Trunctated line basis

\[y_i=f(x_i)+\epsilon_i,\]

-  One of the most simple basis expansions
-  $f(x_i)=\beta_0+\beta_1x_i+\sum\limits_{m=1}^{K-2}\theta_m(x_i-t_m)_+$ with $(.)_+$ the operator that takes the positive part.
-  Note, that better basis expansions exist, which are orthogonal, computational more stable and/or continuous derivative beyond first order
-  We will use this basis for didactical purposes
- We can use OLS to fit y w.r.t. the basis.

```{r}
knots <- seq(400,700,12.5)

basis <- sapply(knots,
  function(k,y) (y-k)*(y>k),
  y= lidar %>% pull(range)
  )

basisExp <- cbind(1, range = lidar %>% pull(range), basis)

splineFitLs <- lm(logratio ~ -1 + basisExp, lidar)

pBasis <- basisExp[,-1] %>%
  data.frame %>%
  gather("basis","values",-1) %>%
  ggplot(aes(x = range, y = values, color = basis)) +
  geom_line() +
  theme(legend.position="none") +
  ylab("basis")

grid.arrange(
  pLidar +
    geom_line(aes(x = lidar$range, y = splineFitLs$fitted), lwd = 2),
  pBasis,
  ncol=1)
```

- Note, that the model is overfitting!
- The fit is very wiggly and is tuned too much to the data.
- The fit has a large variance and low bias.
- It will therefore not generalise well to predict the logratio of future observations.

#### Solution for overfitting?

- We could perform model selection on the basis to select the important basis functions to model the signal. But, this will have the undesired property that the fit will no longer be smooth.

- We can also adopt a ridge penalty!
- However, we do not want to penalise the intercept and the linear term.
- Ridge criterion

\[\Vert\mathbf{Y}-\mathbf{X\beta}\Vert^2+\lambda\boldsymbol{\beta}^T\mathbf{D}\boldsymbol{\beta}
\]

With $\mathbf{D}$ with dimensions (K,K): $\mathbf{D}=\left[\begin{array}{cc}\mathbf{0}_{2\times2}& \mathbf{0}_{2\times K-2}\\
\mathbf{0}_{K-2\times2}&\mathbf{I}_{K-2\times K-2}\end{array}\right]$

- Here we will set the penalty at 900.

```{r}
D <- diag(ncol(basisExp))
D[1:2,1:2] <- 0
lambda <- 900
betaRidge <- solve(t(basisExp)%*%basisExp+(lambda*D))%*%t(basisExp)%*%lidar$logratio
grid.arrange(
  pLidar +
    geom_line(aes(x = lidar$range, y = c(basisExp %*% betaRidge)), lwd = 2),
  pBasis,
  ncol=1)
```

How do we choose $\lambda$?

---

# Evaluation of Prediction Models


Predictions are calculated with the fitted model
 \[
   \hat{Y}(\mathbf{x}) = \hat{m}(\mathbf{x})=\mathbf{x}^T\hat{\beta}
 \]
 when focussing on prediction, we want the prediction error to be as small as possible.

The **prediction error** for a prediction at covariate pattern $\mathbf{x}$ is given by
  \[
     \hat{Y}(\mathbf{x}) - Y^*,
  \]
where

- $\hat{Y}(\mathbf{x})=\mathbf{x}^T\hat{\boldsymbol{\beta}}$ is the prediction at $\mathbf{x}$

-  $Y^*$ is an outcome at covariate pattern $\mathbf{x}$

Prediction is typically used to predict an outcome before it is observed.

- Hence, the outcome $Y^*$ is not observed yet, and
- the prediction error cannot be computed.

---

- Recall that the prediction model $\hat{Y}(\mathbf{x})$ is estimated by using data in the training data set $(\mathbf{X},\mathbf{Y})$, and
- that the outcome $Y^*$ is an outcome at $\mathbf{x}$ which is assumed to be independent of the training data.

- Goal is to use prediction model for predicting a future observation ($Y^*$), i.e. an observation that still has to be realised/observed (otherwise prediction seems rather useless).

- Hence, $Y^*$ can never be part of the training data set.

---

Here we provide definitions and we show how the prediction performance of a prediction model can be evaluated from data.

Let ${\mathcal{T}}=(\mathbf{Y},\mathbf{X})$ denote the training data, from which the prediction model $\hat{Y}(\cdot)$ is build. This building process typically involves feature selection and parameter estimation.

 We will use a more general notation for the prediction model: $\hat{m}(\mathbf{x})=\hat{Y}(\mathbf{x})$.

---

## Test or Generalisation Error

 The test or generalisation error for prediction model $\hat{m}(\cdot)$ is given by
  \[
    \text{Err}_{\mathcal{T}} = \text{E}_{Y^*,X^*}\left[(\hat{m}(\mathbf{X}^*) - Y^*)^2\mid {\mathcal{T}}\right]
  \]
  where $(Y^*,X^*)$ is independent of the training data.

---

- Note that the test error is conditional on the training data ${\mathcal{T}}$.
- Hence, the test error evaluates the performance of the single model build from the observed training data.
- This is the ultimate target of the model assessment, because it is exactly this prediction model that will be used in practice and applied to future predictors $\mathbf{X}^*$ to predict $Y^*$.
- The test error is defined as an average over all such future observations $(Y^*,\mathbf{X}^*)$.

---

## Conditional test error

Sometimes the conditional test error is used:

The conditional test error in $\mathbf{x}$ for prediction model $\hat{m}(\mathbf{x})$ is given by
 \[
   \text{Err}_{\mathcal{T}}(\mathbf{x}) = \text{E}_{Y^*}\left[(\hat{m}(\mathbf{x}) - Y^*)^2\mid {\mathcal{T}}, \mathbf{x}\right]
 \]
 where $Y^*$ is an outcome at predictor $\mathbf{x}$, independent of the training data.

 Hence,
 \[
   \text{Err}_{\mathcal{T}} = \text{E}_{X^*}\left[\text{Err}_{\mathcal{T}}(\mathbf{X}^*)\right].
 \]

A closely related error is the **insample error**.

---

## Insample Error

The insample error for prediction model $\hat{m}(\mathbf{x})$ is given by
 \[
   \text{Err}_{\text{in} \mathcal{T}} = \frac{1}{n}\sum_{i=1}^n \text{Err}_{\mathcal{T}}(\mathbf{x}_i),
 \]

i.e. the insample error is the sample average of the conditional test errors evaluated in the $n$ training dataset predictors $\mathbf{x}_i$.

Since $\text{Err}_{\mathcal{T}}$ is an average over all $\mathbf{X}$, even over those predictors not observed in the training dataset, it is sometimes referred to as the **outsample error**.

---

## Estimation of the insample error

We start with introducing the training error rate, which is closely related to the MSE in linear models.

### Training error

 The training error is given by
 \[
   \overline{\text{err}} = \frac{1}{n}\sum_{i=1}^n (Y_i - \hat{m}(\mathbf{x}_i))^2 ,
 \]
 where the $(Y_i,\mathbf{x}_i)$ from the training dataset which is also used for the calculation of $\hat{m}$.

- The training error is an overly optimistic estimate of the test error $\text{Err}_{\mathcal{T}}$.

- The training error will never increases when the model becomes more complex. $\longrightarrow$ cannot be used directly as a model selection criterion.

Indeed, model parameters are often estimated by minimising the training error (cfr. SSE).

- Hence the fitted model adapts to the training data, and
- training error will be an overly optimistic estimate of the test error $\text{Err}_{\mathcal{T}}$.

---

It can be shown that the training error is related to the insample test error via

\[
\text{E}_\mathbf{Y}
\left[\text{Err}_{\text{in}{\mathcal{T}}}\right] = \text{E}_\mathbf{Y}\left[\overline{\text{err}}\right] + \frac{2}{n}\sum_{i=1}^n \text{cov}_\mathbf{Y}\left[\hat{m}(\mathbf{x}_i),Y_i\right],
\]

Note, that for linear models
\[ \hat{m}(\mathbf{x}_i) = \mathbf{X}\hat{\boldsymbol{\beta}}= \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} = \mathbf{HY}
\]
with

- $\mathbf{H}$ the hat matrix and
- all $Y_i$ are assumed to be independently distributed  $N(\mathbf{X}\boldsymbol{\beta},\sigma^2)$

Hence, for linear models with independent observations

\begin{eqnarray}
\text{cov}_\mathbf{Y}\left[\hat{m}(\mathbf{x}_i),Y_i)\right] &=&
\text{cov}_\mathbf{Y}\left[\mathbf{H}_{i}^T\mathbf{Y},Y_i)\right]\\
&=& \text{cov}_\mathbf{Y}\left[h_{ii} Y_i,Y_i\right]\\
&=& h_{ii} \text{cov}_\mathbf{Y}\left[Y_i,Y_i\right]\\
&=& h_{ii} \sigma^2\\
\end{eqnarray}

And we can thus estimate the insample error by Mallow's $C_p$

\begin{eqnarray}
C_p &=& \overline{\text{err}} + \frac{2\sigma^2}{n}\text{tr}(\mathbf{H})\\
&=& \overline{\text{err}} + \frac{2\sigma^2p}{n}
\end{eqnarray}

with $p$ the number of predictors.

- Mallow's $C_p$ is often used for model selection.
- Note, that we can also consider it as a kind of penalized least squares:

\[
n \times C_p = \Vert \mathbf{Y} - \mathbf{X}\boldsymbol{\beta}\Vert_2^2 + 2\sigma^2 \Vert \boldsymbol{\beta} \Vert_0
\]
with $L_0$ norm $\Vert \boldsymbol{\beta} \Vert_0 = \sum_{j=1}^p \beta_p^0 = p$.

---

## Expected test error

The test or generalisation error was defined conditionally on the training data. By averaging over the distribution of training datasets, the expected test error arises.

\begin{eqnarray*}
   \text{E}_{\mathcal{T}}\left[\text{Err}_{{\mathcal{T}}}\right]
     &=& \text{E}_{\mathcal{T}}\left[\text{E}_{Y^*,X^*}\left[(\hat{m}(\mathbf{X}^*) - Y^*)^2\mid {\mathcal{T}}\right]\right] \\
     &=& \text{E}_{Y^*,X^*,{\mathcal{T}}}\left[(\hat{m}(\mathbf{X}^*) - Y^*)^2\right].
 \end{eqnarray*}

 - The expected test error may not be of direct interest when the goal is to assess the prediction performance of a single prediction model $\hat{m}(\cdot)$.

 - The expected test error averages the test errors of all models that can be build from all training datasets, and hence this may be less relevant when the interest is in evaluating one particular model that resulted from a single observed training dataset.

 - Also note that building a prediction model involves both parameter estimation and feature selection.

 - Hence the expected test error also evaluates the feature selection procedure (on average).

 - If the expected test error is small, it is an indication that the model building process gives good predictions for future observations $(Y^*,\mathbf{X}^*)$ on average.

### Estimating the Expected test error

The expected test error may be estimated by cross validation (CV).

#### Leave one out cross validation (LOOCV)}

The LOOCV estimator of the expected test error (or expected outsample error) is given by
  \[
     \text{CV} = \frac{1}{n} \sum_{i=1}^n \left(Y_i - \hat{m}^{-i}(\mathbf{x}_i)\right)^2 ,
  \]
where

- the $(Y_i,\mathbf{x}_i)$ form the training dataset
-   $\hat{m}^{-i}$ is the fitted model based on all training data, except observation $i$
-   $\hat{m}^{-i}(\mathbf{x}_i)$ is the prediction at $\mathbf{x}_i$, which is the observation left out the training data before building model $m$.

Some rationale as to why LOOCV offers a good estimator of the outsample error:

- the prediction error $Y^*-\hat{m}(\mathbf{x})$ is mimicked by not using one of the training outcomes $Y_i$ for the estimation of the model so that this $Y_i$ plays the role of $Y^*$, and, consequently, the fitted model $\hat{m}^{-i}$ is independent of $Y_i$

 - the sum in $CV$ is over all $\mathbf{x}_i$ in the training dataset, but each term $\mathbf{x}_i$ was left out once for the calculation of $\hat{m}^{-i}$. Hence, $\hat{m}^{-i}(\mathbf{x}_i)$ mimics an outsample prediction.

 - the sum in CV is over $n$ different training datasets (each one with a different observation removed), and hence CV is an estimator of the *expected* test error.

 - For linear models the LOOCV can be readily obtained from the fitted model: i.e.

 \[\text{CV} = \frac{1}{n}\sum\limits_{i=1}^n \frac{e_i^2}{(1-h_{ii})^2}\]

 with $e_i$ the residuals from the model that is fitted based on all training data.

---

An alternative to LOOCV is the $k$-fold cross validation procedure. It also gives an estimate of the expected outsample error.

#### $k$-fold cross validation

-  Randomly divide the training dataset into $k$ approximately equal subsets . Let $S_j$ denote the index set of the $j$th subset (referred to as a **fold**). Let $n_j$ denote the number of observations in fold $j$.

- The $k$-fold cross validation estimator of the expected outsample error is given by
 \[
     \text{CV}_k = \frac{1}{k}\sum_{j=1}^k \frac{1}{n_j} \sum_{i\in S_j} \left(Y_i - \hat{m}^{-S_j}(\mathbf{x}_i)\right)^2
 \]
 where $\hat{m}^{-S_j}$ is the model fitted using all training data, except observations in fold $j$ (i.e. observations $i \in S_j$).

---

The cross validation estimators of the expected outsample error are nearly unbiased. One argument that helps to understand where the bias comes from is the fact that e.g. in de LOOCV estimator the model is fit on only $n-1$ observations, whereas we are aiming at estimating the outsample error of a model fit on all $n$ training observations. Fortunately, the bias is often small and is in practice hardly a concern.

$k$-fold CV is computationally more complex.

Since CV and CV$_k$ are estimators, they also show sampling variability. Standard errors of the CV or CV$_k$ can be computed. We don't show the details, but in the example this is illustrated.

### Bias Variance trade-off

For the expected conditional test error in $\mathbf{x}$, it holds that
\begin{eqnarray*}
  \text{E}_{\mathcal{T}}\left[\text{Err}_{\mathcal{T}}(\mathbf{x})\right]
    &=& \text{E}_{Y^*,{\mathcal{T}}}\left[(\hat{m}(\mathbf{x})-Y^*)^2 \mid \mathbf{x}\right] \\
    &=&  \text{var}_{\mathbf{Y}}\left[\hat{Y}(\mathbf{x})\mid \mathbf{x}\right] +(\mu(\mathbf{x})-\mu^*(\mathbf{x}))^2+\text{var}_{Y^*}\left[Y^*\mid \mathbf{x}\right]
\end{eqnarray*}
where $\mu(\mathbf{x}) = \text{E}_{\mathbf{Y}}\left[\hat{Y}(\mathbf{x})\mid \mathbf{x}\right] \text{ and } \mu^*(\mathbf{x})=\text{E}_{Y^*}\left[Y^*\mid \mathbf{x}\right]$.

- **bias**: $\text{bias}(\mathbf{x})=\mu(\mathbf{x})-\mu^*(\mathbf{x})$

- $\text{var}_{Y^*}\left[Y^*\mid \mathbf{x}\right]$ does not depend on the model, and is referred to as the **irreducible variance**.

---

The importance of the bias-variance trade-off can be seen from a model selection perspective. When we agree that a good model is a model that has a small expected conditional test error at some point $\mathbf{x}$, then the bias-variance trade-off shows us that a model may be biased as long as it has a small variance to compensate for the bias.  It often happens that a biased model has a substantial smaller variance. When these two are combined, a small expected test error may occur.

Also note that the model $m$ which forms the basis of the prediction model $\hat{m}(\mathbf{x})$ does NOT need to satisfy $m(\mathbf{x})=\mu(\mathbf{x})$ or $m(\mathbf{x})=\mu^*(\mathbf{x})$. The model $m$ is known by the data-analyst (its the basis of the prediction model), whereas $\mu(\mathbf{x})$ and $\mu^*(\mathbf{x})$ are generally unknown to the data-analyst. We only hope that $m$ serves well as a prediction model.

---

### In practice

We use cross validation to estimate the lambda penalty for penalised regression:

- Ridge Regression
- Lasso
- Build models, e.g. select the number of PCs for PCA regression
- Splines

### Toxicogenomics example

#### Lasso

```{r}
set.seed(15)
library(glmnet)
mCvLasso <- cv.glmnet(
  x = toxData[,-1] %>%
    as.matrix,
  y = toxData %>%
    pull(BA),
  alpha = 1)  # lasso alpha=1

plot(mCvLasso)
```

Default CV procedure in \textsf{cv.glmnet} is $k=10$-fold CV.

The Graphs shows

- 10-fold CV estimates of the extra-sample error as a function of the lasso penalty parameter $\lambda$.
- estimate plus and minus once the estimated standard error of the CV estimate (grey bars)
- On top the number of non-zero regression parameter estimates are shown.

Two vertical reference lines are added to the graph. They correspond to

- the $\log(\lambda)$ that gives the smallest CV estimate of the extra-sample error, and
- the largest $\log(\lambda)$ that gives a CV estimate of the extra-sample error that is within one standard error from the smallest error estimate.
- The latter choice of $\lambda$ has no firm theoretical basis, except that it somehow accounts for the imprecision of the error estimate. One could loosely say that this $\gamma$ corresponds to the smallest model (i.e. least number of predictors) that gives an error that is within margin of error of the error of the best model.

---

```{r}
mLassoOpt <- glmnet(
  x = toxData[,-1] %>%
    as.matrix,
  y = toxData %>%
    pull(BA),
    alpha = 1,
    lambda = mCvLasso$lambda.min)

summary(coef(mLassoOpt))
```


With the optimal $\lambda$ (smallest error estimate) the output shows the `r  nrow(summary(coef(mLassoOpt)))` non-zero estimated regression coefficients (sparse solution).

---

```{r}
mLasso1se <- glmnet(
  x = toxData[,-1] %>%
    as.matrix,
    y= toxData %>%
      pull(BA),
    alpha = 1,
    lambda = mCvLasso$lambda.1se)

mLasso1se %>%
  coef %>%
  summary
```

This shows the solution for the largest $\lambda$ within one standard error of the optimal model. Now only `r  nrow(summary(coef(mLasso1se)))` non-zero estimates result.

---

#### Ridge

```{r}
mCvRidge <- cv.glmnet(
  x = toxData[,-1] %>%
    as.matrix,
    y = toxData %>%
      pull(BA),
      alpha = 0)  # ridge alpha=0

plot(mCvRidge)
```

- Ridge does not seem to have optimal solution.
- 10-fold CV is also larger than for lasso.

---

#### PCA regression

```{r fig.keep = "none", warning = FALSE}
set.seed(1264)
library(DAAG)

tox <- data.frame(
  Y = toxData %>%
    pull(BA),
  PC = Zk)

PC.seq <- 1:25
Err <- numeric(25)

mCvPca <- cv.lm(
  Y~PC.1,
  data = tox,
  m = 5,
  printit = FALSE)

Err[1]<-attr(mCvPca,"ms")

for(i in 2:25) {
  mCvPca <- cv.lm(
    as.formula(
      paste("Y ~ PC.1 + ",
        paste("PC.", 2:i, collapse = "+", sep=""),
        sep=""
      )
    ),
    data = tox,
    m = 5,
    printit = FALSE)
  Err[i]<-attr(mCvPca,"ms")
}
```

- Here we illustrate principal component regression.

- The most important PCs are selected in a forward model selection procedure.

- Within the model selection procedure the models are evaluated with 5-fold CV estimates of the outsample error.

- It is important to realise that a forward model selection procedure will not necessarily result in the best prediction model, particularly because the order of the PCs is generally not related to the importance of the PCs for predicting the outcome.

- A supervised PC would be better.

```{r}
pPCreg <- data.frame(PC.seq, Err) %>%
  ggplot(aes(x = PC.seq, y = Err)) +
  geom_line() +
  geom_point() +
  geom_hline(
    yintercept = c(
      mCvLasso$cvm[mCvLasso$lambda==mCvLasso$lambda.min],
      mCvLasso$cvm[mCvLasso$lambda==mCvLasso$lambda.1se]),
    col = "red") +
  xlim(1,26)

grid.arrange(
  pPCreg,
  pPCreg + ylim(0,5),
  ncol=2)
```

- The graph shows the CV estimate of the outsample error as a function of the number of sparse PCs included in the model.

- A very small error is obtained with the model with only the first PC. The best model with 3 PCs.

- The two vertical reference lines correspond to the error estimates obtained with lasso (optimal $\lambda$ and largest $\lambda$ within one standard error).

- Thus although there was a priori no guarantee that the first PCs are the most predictive, it seems to be the case here (we were lucky!).

- Moreover, the first PC resulted in a small outsample error.

- Note that the graph does not indicate the variability of the error estimates (no error bars).

- Also note that the graph clearly illustrates the effect of overfitting: including too many PCs causes a large outsample error.

### Lidar Example: splines

- We use the mgcv package to fit the spline model to the lidar data.
- A better basis is used than the truncated spline basis
- Thin plate splines are also linear smoothers, i.e.
$\hat{Y} = \hat{m}(\mathbf{X}) = \mathbf{SY}$
- So their variance can be easily calculated.
- The ridge/smoothness penalty is chosen by generalized cross validation.

```{r}
library(mgcv)
gamfit <- gam(logratio ~ s(range), data = lidar)
gamfit$sp

pLidar +
  geom_line(aes(x = lidar$range, y = gamfit$fitted), lwd = 2)
```

## More general error definitions

So far we only looked at continuous outcomes $Y$ and errors defined in terms of the squared loss $(\hat{m}(\mathbf{x})-Y^*)^2$.

More generally, a **loss function** measures an discrepancy between the prediction $\hat{m}(\mathbf{x})$ and an independent outcome $Y^*$ that corresponds to $\mathbf{x}$.


Some examples for continuous $Y$:
\begin{eqnarray*}
  L(Y^*,\hat{m}(\mathbf{x}))
    &=& (\hat{m}(\mathbf{x})-Y^*)^2 \;\;\text{(squared error)} \\
  L(Y^*,\hat{m}(\mathbf{x}))
    &=& \vert\hat{m}(\mathbf{x})-Y^*\vert \;\;\text{(absolute error)} \\
   L(Y^*,\hat{m}(\mathbf{x}))
    &=& 2 \int_{\mathcal{Y}} f_y(y) \log\frac{f_y(y)}{f_{\hat{m}}(y)} dy \;\;\text{(deviance)}.
\end{eqnarray*}


In the expression of the deviance

- $f_y$ denotes the density function of a distribution with mean set to $y$ (cfr. perfect fit), and
- $f_{\hat{m}}$ is the density function of the same distribution but with mean set to the predicted outcome $\hat{m}(\mathbf{x})$.

---

With a given loss function, the errors are defined as follows:
- Test or generalisation or outsample error
    \[
      \text{Err}_{\mathcal{T}} = \text{E}_{Y^*,X^*}\left[L(Y^*,\hat{m}(\mathbf{X}^*))\right]
    \]

- Training error
  \[
    \overline{\text{err}} = \frac{1}{n}\sum_{i=1}^n L(Y_i,\hat{m}(\mathbf{x}_i))
  \]

- $\ldots$

---

When an exponential family distribution is assumed for the outcome distribution, and when the deviance loss is used, the insample error can be estimated by means of the AIC and BIC.

### Akaike's Information Criterion (AIC)

The AIC for a model $m$ is given by
\[
\text{AIC} = -2 \ln \hat{L}(m) +2p
\]
where $\hat{L}(m)$ is the maximised likelihood for model $m$.

When assuming normally distributed error terms and homoscedasticity, the AIC becomes
\[
\text{AIC} = n\ln \text{SSE}(m) +2p = n\ln(n\overline{\text{err}}(m)) + 2p
\]
with $\text{SSE}(m)$ the residual sum of squares of model $m$.

In linear models with normal error terms, Mallow's $C_p$ criterion (statistic) is a linearised version of AIC and it is an unbiased estimator of the in-sample error.

---

### Bayesian Information Criterion (BIC)}

The BIC for a model $m$ is given by
\[
\text{BIC} = -2 \ln \hat{L}(m) +p\ln(n)
\]
where $\hat{L}(m)$ is the maximised likelihood for model $m$.

When assuming normally distributed error terms and homoscedasticity, the BIC becomes
\[
\text{BIC} = n\ln \text{SSE}(m) +p\ln(n) = n\ln(n\overline{\text{err}}(m)) + p\ln(n)
\]
with $\text{SSE}(m)$ the residual sum of squares of model $m$.

When large datasets are used, the BIC will favour smaller models than the AIC.

---

## Training and test sets

Sometimes, when a large (training) dataset is available, one may decide the split the dataset randomly in a

- **training dataset**:
   data are used for model fitting and for model building or feature selection (this may require e.g. cross validation)

- **test dataset**:
   this data are used to evaluate the final model (result of model building). An unbiased estimate of the outsample error (i.e. test or generalisation error) based on this test data is
  \[
     \frac{1}{m} \sum_{i=1}^m \left(\hat{m}(\mathbf{x}_i)-Y_i\right)^2,
  \]
  where
    - $(Y_1,\mathbf{x}_1), \ldots, (Y_m,\mathbf{x}_m)$ denote the $m$ observations in the test dataset

    - $\hat{m}$ is estimated from using the training data (this may also be the result from model building, using only the training data).

---

Note that the training dataset is used for model building or feature selection. This also requires the evaluation of models. For these evaluations the methods from the previous slides can be used (e.g. cross validation, $k$-fold CV, Mallow's $C_p$). The test dataset is only used for the  evaluation of the final model (estimated and build from using only the training data). The estimate of the outsample error based on the test dataset is the best possible estimate in the sense that it is unbiased. The observations used for this estimation are independent of the observations in the training data.
However, if the number of data points in the test dataset ($m$) is small, the estimate of the outsample error may show large variance and hence is not reliable.

# Logistic Regression Analysis for High Dimensional Data

## Breast Cancer Example

- Schmidt *et al.*, 2008, Cancer Research, **68**, 5405-5413

- Gene expression patterns in $n=200$ breast tumors were investigated ($p=22283$ genes)

- After surgery the tumors were graded by a pathologist (stage 1,2,3)

- Here the objective is to predict stage 3 from the gene expression data (prediction of binary outcome)

- If the prediction model works well, it can be used to predict the stage from a biopsy sample.

## Data

```{r, message=FALSE, warning=FALSE}
#BiocManager::install("genefu")
#BiocManager::install("breastCancerMAINZ")

library(genefu)
library(breastCancerMAINZ)
data(mainz)

X <- t(exprs(mainz)) # gene expressions
n <- nrow(X)
H <- diag(n)-1/n*matrix(1,ncol=n,nrow=n)
X <- H%*%X
Y <- ifelse(pData(mainz)$grade==3,1,0)
table(Y)

svdX <- svd(X)
k <- 2
Zk <- svdX$u[,1:k] %*% diag(svdX$d[1:k])
colnames(Zk) <- paste0("Z",1:k)

Zk %>%
  as.data.frame %>%
  mutate(grade = Y %>% as.factor) %>%
  ggplot(aes(x= Z1, y = Z2, color = grade)) +
  geom_point(size = 3)
```

---

## Logistic regression models

Binary outcomes are often analysed with **logistic regression models**.

Let $Y$ denote the binary (1/0, case/control, positive/negative) outcome, and $\mathbf{x}$ the $p$-dimensional predictor.

Logistic regression  assumes
\[
   Y \mid \mathbf{x} \sim \text{Bernoulli}(\pi(\mathbf{x}))
\]
with $\pi(\mathbf{x}) = \text{P}\left[Y=1\mid \mathbf{x}\right]$ and
\[
   \ln \frac{\pi(\mathbf{x})}{1-\pi(\mathbf{x})}=\beta_0 + \boldsymbol{\beta}^T\mathbf{x}.
\]

The parameters are typically estimated by maximising the log-likelihood, which is denoted by $l(\mathbf{
\beta})$, i.e.
\[
   \hat{\boldsymbol{\beta}} = \text{ArgMax}_\beta l(\boldsymbol{\beta}).
\]

- Maximum likelihood is only applicable when $n>p$.

- When $p>n$ penalised maximum likelihood methods are applicable.

---

## Penalized maximum likelihood

Penalised estimation methods (e.g. lasso and ridge) can als be applied to maximum likelihood, resulting in the **penalised maximum likelihood estimator**.

Lasso:
\[
  \hat{\boldsymbol{\beta}} = \text{ArgMax}_\beta l(\boldsymbol{\beta}) -\lambda \Vert \boldsymbol{\beta}\Vert_1.
\]

Ridge:
\[
  \hat{\boldsymbol{\beta}} = \text{ArgMax}_\beta l(\boldsymbol{\beta}) -\lambda \Vert \boldsymbol{\beta}\Vert_2^2.
\]

Once the parameters are estimated, the model may be used to compute
\[
  \hat{\pi}(\mathbf{x}) = \hat{\text{P}}\left[Y=1\mid \mathbf{x}\right].
\]
With these estimated probabilities the prediction rule becomes
\begin{eqnarray*}
  \hat{\pi}(\mathbf{x}) &\leq c& \text{predict } Y=0 \\
  \hat{\pi}(\mathbf{x}) &>c & \text{predict } Y=1
\end{eqnarray*}
with $0<c<1$ a threshold that either is fixed (e.g. $c=1/2$), depends on prior probabilities, or is empirically determined by optimising e.g. the Area Under the ROC Curve (AUC) or by finding a good compromise between sensitivity and specificity.

Note that logistic regression directly models the **Posterior probability** that an observation belongs to class $Y=1$, given the predictor $\mathbf{x}$.

## Model evaluation

Common model evaluation criteria for binary prediction models are:

- sensitivity = true positive rate (TPR)

- specificity = true negative rate (TNR)

- misclassification error

- area under the ROC curve (AUC)

These criteria can again be estimated via cross validation or via splitting of the data into training and test/validation data.

### Sensitivity of a model $\pi$ with threshold $c$

Sensitivity is the probability to correctly predict a positive outcome:
\[
\text{sens}(\pi,c)=\text{P}_{X^*}\left[\hat\pi(\mathbf{X}^*)>c \mid Y^*=1 \mid {\mathcal{T}}\right].
\]

It is also known as the true positive rate (TPR).

### Specificity of a model $\pi$ with threshold $c$

Specificity is the probability to correctly predict a negative outcome:
\[
\text{spec}(\pi,c)=\text{P}_{X^*}\left[\hat\pi(\mathbf{X}^*)\leq c \mid Y^*=0 \mid {\mathcal{T}}\right].
\]

It is also known as the true negative rate (TNR).

---

### Misclassification error of a model $\pi$ with threshold $c$

The misclassification error is the probability to incorrectly predict an outcome:
\begin{eqnarray*}
\text{mce}(\pi,c) &=&\text{P}_{X^*,Y^*}\left[\hat\pi(\mathbf{X})\leq c \text{ and } Y^*=1 \mid {\mathcal{T}}\right] \\
&  & + \text{P}_{X^*,Y^*}\left[\hat\pi(\mathbf{X})> c \text{ and } Y^*=0 \mid {\mathcal{T}}\right].
\end{eqnarray*}

Note that in the definitions of sensitivity, specificity and the misclassification error, the probabilities refer to the distribution of the  $(\mathbf{X}^*,Y^*)$, which is independent of the training data, conditional on the training data. This is in line with the test or generalisation error. The misclassification error is actually the test error when a 0/1 loss function is used. Just as before, the sensitivity, specificity and the misclassification error can also be averaged over the distribution of the training data set, which is in line with the expected test error which has been discussed earlier.

---

### ROC curve of a model $\pi$

The Receiver Operating Characteristic (ROC) curve for model $\pi$ is given by the function

\[
\text{ROC}: [0,1] \rightarrow [0,1]\times [0,1]: c \mapsto (1-\text{spec}(\pi,c), \text{sens}(\pi,c)).
\]

For when $c$ moves from 1 to 0, the ROC function defines a curve in the plane $[0,1]\times [0,1]$, moving from $(0,0)$ for $c=1$ to $(1,1)$ for $c=0$.

The horizontal axis of the ROC curve shows 1-specificity. This is also known as the False Positive Rate (FPR).

---

### Area under the curve (AUC) of a model $\pi$

The area under the curve (AUC) for model $\pi$ is area under the ROC curve and is given by
\[
\int_0^1 \text{ROC}(c) dc.
\]

Some notes about the AUC:

- AUC=0.5 results when the ROC curve is the diagonal. This corresponds to flipping a coin, i.e. a complete random prediction.

- AUC=1 results from the perfect ROC curve, which is the ROC curve through the points $(0,0)$, $(0,1)$ and $(1,1)$. This ROC curve includes a threshold $c$ such that sensitivity and specificity are equal to one.

## Breast cancer example

### Data

```{r, message=FALSE, warning=FALSE}
library(glmnet)

#BiocManager::install("genefu")
#BiocManager::install("breastCancerMAINZ")

library(genefu)
library(breastCancerMAINZ)
data(mainz)

X <- t(exprs(mainz)) # gene expressions
n <- nrow(X)
H <- diag(n)-1/n*matrix(1,ncol=n,nrow=n)
X <- H%*%X
Y <- ifelse(pData(mainz)$grade==3,1,0)
table(Y)
```

---

From the table of the outcomes in Y we read that

- `r sum(Y==1)` tumors were graded as stage 3 and
- `r sum(Y==0)` tumors were graded as stage 1 or 2.

In this the stage 3 tumors are referred to as cases or postives and the stage 1 and 2 tumors as controls or negatives.

---

### Training and test dataset

The use of the lasso logistic regression for the prediction of stage 3 breast cancer is illustrated here by

- randomly splitting the dataset into a training dataset ($80\%$ of data = 160 tumors) and a test dataset (40 tumors)

- using the training data to select a good $\lambda$ value in the lasso logistic regression model (through 10-fold CV)

- evaluating the final model by means of the test dataset (ROC Curve, AUC).


```{r}

## Used to provide same results as in previous R version
RNGkind(sample.kind = "Rounding")
set.seed(6977326)
####

n <- nrow(X)
nTrain <- round(0.8*n)
nTrain

indTrain <- sample(1:n,nTrain)
XTrain <- X[indTrain,]
YTrain <- Y[indTrain]
XTest <- X[-indTrain,]
YTest <- Y[-indTrain]
table(YTest)
```

Note that the randomly selected test data has `r mean(YTest==1)*100`% cases of stage 3 tumors.
This is a bit higher than the `r mean(Y==1)*100`%  in the complete data.

One could also perform the random splitting among the positives and the negatives separately (stratified splitting).

### Model fitting based on training data

```{r}
mLasso <- glmnet(
  x = XTrain,
  y = YTrain,
  alpha = 1,
  family="binomial")  # lasso: alpha = 1

plot(mLasso, xvar = "lambda", xlim = c(-6,-1.5))
```

---

```{r}
mCvLasso <- cv.glmnet(
  x = XTrain,
  y = YTrain,
  alpha = 1,
  type.measure = "class",
	family = "binomial")  # lasso alpha = 1

plot(mCvLasso)
mCvLasso
```

The total misclassification error is used here to select a good value for $\lambda$.

```{r}
# BiocManager::install("plotROC")
library(plotROC)

dfLassoOpt <- data.frame(
  pi = predict(mCvLasso,
    newx = XTest,
    s = mCvLasso$lambda.min,
    type = "response") %>% c(.),
  known.truth = YTest)

roc <-
  dfLassoOpt  %>%
  ggplot(aes(d = known.truth, m = pi)) +
  geom_roc(n.cuts = 0) +
  xlab("1-specificity (FPR)") +
  ylab("sensitivity (TPR)")

roc

calc_auc(roc)
```

- The ROC curve is shown for the model based on $\lambda$ with the smallest misclassification error. The model has an AUC of `r calc_auc(roc) %>% pull(AUC) %>% round(2)`.

- Based on this ROC curve an appropriate threshold $c$ can be chosen. For example, from the ROC curve we see that it is possible to attain a specificity and a sensitivity of 75\%.

- The sensitivities and specificities in the ROC curve are unbiased (independent test dataset) for the prediction model build from the training data. The estimates of sensitivity and specificity, however, are based on only 40 observations.

---

```{r}
mLambdaOpt <- glmnet(x = XTrain,
  y = YTrain,
  alpha = 1,
  lambda = mCvLasso$lambda.min,
  family="binomial")

qplot(
  summary(coef(mLambdaOpt))[-1,1],
  summary(coef(mLambdaOpt))[-1,3]) +
  xlab("gene ID") +
  ylab("beta-hat") +
  geom_hline(yintercept = 0, color = "red")
```

- The model with the optimal $\lambda$ has only `r mLambdaOpt %>% coef %>% summary %>% nrow` non-zero parameter estimates.
- Thus only `r mLambdaOpt %>% coef %>% summary %>% nrow` genes are involved in the prediction model.
- These `r mLambdaOpt %>% coef %>% summary %>% nrow` parameter estimates are plotting in the graph.
A listing of the model output would show the names of the genes.

---

```{r}

dfLasso1se <- data.frame(
  pi = predict(mCvLasso,
    newx = XTest,
    s = mCvLasso$lambda.1se,
    type = "response") %>% c(.),
  known.truth = YTest)

roc <-
  rbind(
    dfLassoOpt %>%
      mutate(method = "min"),
    dfLasso1se %>%
      mutate(method = "1se")
  ) %>%
  ggplot(aes(d = known.truth, m = pi, color = method)) +
  geom_roc(n.cuts = 0) +
  xlab("1-specificity (FPR)") +
  ylab("sensitivity (TPR)")

roc

calc_auc(roc)
```

- When using the $\lambda$ of the optimal model up to 1 standard deviation, a diagonal ROC curve is obtained and hence AUC is $0.5$.

- This prediction model is thus equivalent to flipping a coin for making the prediction.

- The reason is that with this choice of $\lambda$ (strong penalisation) almost all predictors are removed from the model.

- Therefore, do never blindly choose for the ``optimal'' $\lambda$ as defined here, but assess the performance of the model first.

```{r}
mLambda1se <- glmnet(x = XTrain,
  y = YTrain,
  alpha = 1,
  lambda = mCvLasso$lambda.1se,
  family="binomial")

mLambda1se %>%
  coef %>%
  summary
```

---

## The Elastic Net

The lasso and ridge regression have positive and negative properties.

- Lasso

   - positive: sparse solution

   - negative: at most $\min(n,p)$ predictors can be selected

   - negative: tend to select one predictor among a group of highly correlated predictors


- Ridge

    - negative: no sparse solution
    - positive: more than $\min(n,p)$ predictors can be selected

A compromise between lasso and ridge: the **elastic net**:
\[
  \hat{\boldsymbol{\beta}} = \text{ArgMax}_\beta l(\boldsymbol{\beta}) -\gamma_1 \Vert \boldsymbol\beta\Vert_1 -\gamma_2 \Vert \boldsymbol\beta\Vert_2^2.
\]

The elastic gives a sparse solution with potentially more than $\min(n,p)$ predictors.

---

The `glmnet` R function uses the following parameterisation,
\[
  \hat{\boldsymbol{\beta}} = \text{ArgMax}_\beta l(\boldsymbol{\beta}) -\lambda\alpha \Vert \boldsymbol\beta\Vert_1 -\lambda(1-\alpha) \Vert \boldsymbol\beta\Vert_2^2.
\]

- $\alpha$ parameter gives weight to $L_1$ penalty term (hence $\alpha=1$ gives the lasso, and $\alpha=0$ gives ridge).

- a $\lambda$ parameter to give weight to the penalisation

- Note that the combination of $\lambda$ and $\alpha$ gives the same flexibility as the combination of the parameters $\lambda_1$ and $\lambda_2$.

---

### Breast cancer example

```{r}
mElastic <- glmnet(
  x = XTrain,
  y = YTrain,
  alpha = 0.5,
  family="binomial")  # elastic net

plot(mElastic, xvar = "lambda",xlim=c(-5.5,-1))
```

```{r}
mCvElastic <- cv.glmnet(x = XTrain,
  y = YTrain,
  alpha = 0.5,
  family = "binomial",
	type.measure = "class")  # elastic net

plot(mCvElastic)
mCvElastic
```

```{r}
dfElast <- data.frame(
  pi = predict(mElastic,
    newx = XTest,
    s = mCvElastic$lambda.min,
    type = "response") %>% c(.),
  known.truth = YTest)

roc <- rbind(
  dfLassoOpt %>% mutate(method = "lasso"),
  dfElast %>% mutate(method = "elast. net")) %>%
  ggplot(aes(d = known.truth, m = pi, color = method)) +
  geom_roc(n.cuts = 0) +
  xlab("1-specificity (FPR)") +
  ylab("sensitivity (TPR)")

roc

calc_auc(roc)
```

- More parameters are used than for the lasso, but the performance does not improve.

```{r}
mElasticOpt <- glmnet(x = XTrain,
  y = YTrain,
  alpha = 0.5,
  lambda = mCvElastic$lambda.min,
  family="binomial")

qplot(
  summary(coef(mElasticOpt))[-1,1],
  summary(coef(mElasticOpt))[-1,3]) +
  xlab("gene ID") +
  ylab("beta-hat") +
  geom_hline(yintercept = 0, color = "red")
```

# Acknowledgement {-}

- Olivier Thas for sharing his materials of Analysis of High Dimensional Data 2019-2020, which I used as the starting point for this chapter.

```{r, child="_session-info.Rmd"}
```
</div>
<div class="footer">
    <hr>
    This work is licensed under the <a href= "https://creativecommons.org/licenses/by-nc-sa/4.0">
    CC BY-NC-SA 4.0</a> licence.
</div>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeSourceEmbed("prediction.Rmd");
  window.initializeCodeFolding("show" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>


</body>
</html>
